{
  "4427277e-c272-4ff3-95e6-f236e1053d95": {
    "id": "4427277e-c272-4ff3-95e6-f236e1053d95",
    "filename": "978-981-99-8079-6_14.pdf",
    "status": "ready",
    "metadata": {
      "title": "Multimodal Isotropic Neural Architecture with Patch Embedding",
      "authors": [
        "Hubert Truchan"
      ],
      "abstract": null,
      "keywords": [
        "Multimodal Classification",
        "Isotropic Architecture",
        "Patch Embedding",
        "Time Series"
      ],
      "publication_date": null,
      "doi": null
    },
    "sections": [
      {
        "id": "section_0",
        "title": "Abstract",
        "content": "Abstract. Patch embedding has been a signi\ufb01cant advancement in\nTransformer-based models, particularly the Vision Transformer (ViT),\nas it enables handling larger image sizes and mitigating the quadratic\nruntime of self-attention layers in Transformers. Moreover, it allows\nfor capturing global dependencies and relationships between patches,\nenhancing e\ufb00ective image understanding and analysis. However, it is\nimportant to acknowledge that Convolutional Neural Networks (CNNs)\ncontinue to excel in scenarios with limited data availability. Their e\ufb03-\nciency in terms of memory usage and latency makes them particularly\nsuitable for deployment on edge devices. Expanding upon this, we pro-\npose Minape, a novel multimodal isotropic convolutional neural archi-\ntecture that incorporates patch embedding to both time series and\nimage data for classi\ufb01cation purposes. By employing isotropic models,\nMinape addresses the challenges posed by varying data sizes and com-\nplexities of the data. It groups samples based on modality type, creating\ntwo-dimensional representations that undergo linear embedding before\nbeing processed by a scalable isotropic convolutional network architec-\nture. The outputs of these pathways are merged and fed to a temporal\nclassi\ufb01er. Experimental results demonstrate that Minape signi\ufb01cantly\noutperforms existing approaches in terms of accuracy while requiring\nfewer than 1M parameters and occupying less than 12 MB in size. This\nperformance was observed on multimodal benchmark datasets and the\nauthors\u2019 newly collected multi-dimensional multimodal dataset, Mude-\nstreda, obtained from real industrial processing devices1(1Link to code\nand dataset: https://github.com/hubtru/Minape).\nKeywords: Multimodal Classi\ufb01cation \u00b7 Isotropic Architecture \u00b7 Patch\nEmbedding \u00b7 Time Series",
        "page_start": 1,
        "page_end": 1,
        "chunk_ids": []
      },
      {
        "id": "section_1",
        "title": "1\nIntroduction",
        "content": "1\nIntroduction\nHumans perceive the world through multiple senses, such as vision and hearing,\nleading to a multimodal understanding. This combination of data from di\ufb00er-\nent modalities o\ufb00ers augmented and complementary information, enabling more\nrobust inference. In recent years, deep learning approaches have made remarkable\nprogress in leveraging data from various modalities, resulting in improved perfor-\nmance across classical problems, including action recognition [14] and semantic\nc\n\u20dd The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024\nB. Luo et al. (Eds.): ICONIP 2023, LNCS 14447, pp. 173\u2013187, 2024.\nhttps://doi.org/10.1007/978-981-99-8079-6_14\n\n\n174\nH. Truchan et al.\nsegmentation [1]. Despite these advancements, e\ufb00ectively integrating informa-\ntion from multiple modalities remains a fundamental challenge in multimodal\nlearning. Several research e\ufb00orts have focused on designing fusion paradigms\nto fuse multimodal data [12,33]. However, these approaches often require man-\nual design and are speci\ufb01c to certain tasks and modalities, primarily focusing\non image and text as the common modalities [31]. Yet, in applications such as\nintelligent production and healthcare, time series and audio data serve as the\nmajor source of information. For instance, device state recognition is a vital\nproblem in intelligent production and healthcare systems, demanding precise\nmonitoring due to the dynamic nature of the device\u2019s physical and environmen-\ntal properties. Maintenance reports show that these devices exhibit symptoms\nof changing conditions and damage before experiencing complete failure during\noperation. These symptoms, including distinctive sounds, can serve as indicators\nfor identifying the state of the device. Traditional methods like single modality\napproaches [22] or self-supervised [18] are impractical in these scenarios due to\nthe complexity and variability of data. Such applications often su\ufb00er from noise\nand con\ufb02icts between modalities, which can signi\ufb01cantly impact prediction accu-\nracy. Moreover, the limited availability of training samples further complicates\nthe development of e\ufb03cient models. An ideal algorithm should address these\nchallenges by being robust to noise, selectively leveraging strong modalities, and\ne\ufb00ectively capturing complementary information among modalities.\nIn this paper, we present (Minape), a novel multimodal isotropic neural\narchitecture with patch embedding that integrates time series and image data\nas input. Minape is based on convolutional neural networks (CNNs) and uses a\npatch-based representation to learn local features from the data. One key advan-\ntage of Minape is its isotropic nature, which allows it to handle inputs of varying\nsizes and aspect ratios and to be trained with fewer labelled examples compared\nto existing methods. Minape can be employed in numerous applications, includ-\ning healthcare systems, intelligent production, and real-time monitoring of device\nstates. Through extensive experiments, we demonstrate that Minape achieves\nsigni\ufb01cantly higher accuracy compared to the state-of-the-art approaches on\nboth public multimodal classi\ufb01cation datasets and our newly introduced multi-\nmodal device state dataset (Mudestreda) collected from industrial processing\ndevices. In addition, Minape requires signi\ufb01cantly less memory to store its much\nsmaller model and can be trained on small and medium size multimodal datasets.\nOverall, the contributions of this paper are as follows:\n\u2013 The Minape framework, a novel multimodal isotropic convolutional architec-\nture with patch embedding that requires less than 12 MB size and less than\n1M Parameters, which yields the inference output over 90 images/s (results\nreported for Nvidia 1080TI) for audiovisual data,\n\u2013 An alternate solution to transformer-based fusion models that can be e\ufb00ec-\ntively trained on the small and medium size multimodal datasets with less\nthan 1k instances and capable of operating on edge devices and being deployed\nin real standalone systems,\n\n\nMultimodal Isotropic Neural Architecture with Patch Embedding\n175\nFig. 1. A sample instance from Mudestreda that shows the importance of leveraging\nmultimodal data for device state prediction.\n\u2013 Minape yields signi\ufb01cantly higher accuracy compared to the state-of-the-art\napproaches. Figure 1 intuitively demonstrates that this augmentation leads to\na better predictive performance by an example from Mudestreda, emphasizing\nits real-world application readiness,\n\u2013 Minape exhibits scalability concerning model pathways and depth dimension.\nIt permits \ufb02exible model scaling based on data size and task complexity,\n\u2013 Finally, we share our collected data, Mudestreda, which is publicly available\nand aims for multimodal device state recognition.",
        "page_start": 1,
        "page_end": 3,
        "chunk_ids": []
      },
      {
        "id": "section_2",
        "title": "2\nRelated Work",
        "content": "2\nRelated Work\n2.1\nMultimodal Fusion\nThe focus of multimodal fusion has been primarily on exploring di\ufb00erent archi-\ntectures and techniques for e\ufb00ectively integrating di\ufb00erent data modalities to\nimprove the performance of the model. In particular, an Action Segmentation\nmodel (ASPnet) based on ASFormer disentangles hidden features into modality-\nshared components and projects them into independent FC layers [1]. Similarly,\na Transformer-based approach [13] creates a single shared representation space\nusing multiple types of image-paired data. [24] also utilized a Transformer-based\narchitecture to leverage audiovisual context in action localization. Other works\nintroduced diverse multimodal fusion strategies involving semantic fusion [31],\ntemporal sequence-based fusion [36], and cross-attention mechanism [18]. Our\napproach simpli\ufb01es the fusion process by using isotropic convolutional neu-\nral architectures, making it computationally e\ufb03cient and suitable for smaller\ndatasets.\n\n\n176\nH. Truchan et al.\n2.2\nMultimodal Model Optimisation\nFocusing on model optimization, the primary limitation in the existing liter-\nature relates to the resource-demanding nature of the algorithms. In order to\nmanage the long-range temporal dependencies in the data, an attention bottle-\nneck was used, but that solution resulted in high memory demand [1]. Similarly, a\njoint embedding solution reduced the modalities [13], yet the model is resource-\nintensive with high memory usage. Another approach proposed an e\ufb03cient fusion\nstrategy via prompt-based techniques [17] but still required more than 12 GB of\nmemory for training. Other methods also developed innovative ways to optimize\ntheir networks, for instance, feature anticipation [36], modality-wise L2 normal-\nization [32], progressive reduction of tokens [37], but they all required signi\ufb01cant\ncomputational resources, including large memory and high model size.\nAdditionally, recent works applied SWIN-transformer [22], TimeSformer [18],\nVGG-M [27], 3D-Resnet-18 [26] and all reported models with parameters exceed-\ning tens of millions, far above the size of our proposed model. Similarly, recent\nresearch showcased innovative techniques with pyramid cross-fusion Trans-\nformer [35], contrastive-based alignment training [14], modality dropout train-\ning [12], single channel version of ViT [14], knowledge distillation [2], cross-modal\nprototypical loss [33] and injecting trainable parameters into a frozen ViT [19],\nbut these methods were computationally expensive and required large-scale mul-\ntimodal datasets for pre-training. Furthermore, recent works presented history-\naware [5] and weakly-supervised parsing [20], but these are not feasible for small-\nto medium-sized multimodal datasets. In contrast, our work proposes a novel,\nlightweight isotropic convolutional architecture that performs well on limited\ndatasets, requiring less than 1M parameters and less than 12 MB size which is\n24M parameter and 88 MB less than the lightest available model.\n3\nMinape: A Multimodal Isotropic Neural Framework\nWe consider a target process that generates a sequence of M-modal multi-\ndimensional data points, X(1), X(2), . . . , X(l), consisting of time series and\nimages and expressed as the set of tensors T1,1, T1,2, ..., TL,M, where X \u2208 IRD\nand the mth tensor is Dh\u00d7b\u00d7d\nl,m\ndimensional in IRD, where h \u00d7 b \u00d7 d describes\ntensor dimension, as height, bright and depth respectively, M is the number of\nauxiliary sensors, and l is the number of data points. Tensors are grouped into\ng groups according to their modality, where Ti,G1, i = 1..L, G1 \u2208 M is the set of\nthe tensors belonging to modality type one of the size g1.\nOur objective is to train a classi\ufb01er f : IRD \u2192 y where y is the class of\ndata point X(i). This is measured by sparse categorical cross-entropy in our\nexperiments. We assume the samples are drawn from an ergodic and stationary\nprocess. Multiple sensors are grouped in a cluster to reduce the Wasserstein\ndistance between multimodal feature distributions [34].\n\n\nMultimodal Isotropic Neural Architecture with Patch Embedding\n177\nFig. 2. The Minape general architecture.\nTo bene\ufb01t from the synergy of the available M multimodal tensors and\nenhance the manifolds mapping abilities, we propose deep concatenation of the\nlatent spaces of the linearly embedded two-dimensional representation of input\nsamples from f1:g classi\ufb01ers to train a classi\ufb01er f \u2032 : IRDu \u2192 y, where Du rep-\nresents the dimension of the deep concatenation layer, and g is the number of\nmodalities of the same type.\n3.1\nThe Minape Architecture Overview\nMinape (Fig. 2) is a two-pathway convolutional-based model with visual and\ntime-series feature extractions to obtain a joint representation of the multimodal\ndata. The shared multimodal representation is then passed to the recurrent neu-\nral layer to capture the long-term dependencies of the data sequence. The data\npoints comprise M tensors, which are initially grouped into g groups based on\ntheir modality type and input into representative fi, i = (1 . . . g) subnetworks\nthat share parameters across tensors in the same modality group. Then, the\nlearned representations of modalities ui, i = (1 . . . g) are merged using the con-\ncatenation layer, resulting in vector u:\nu = [f1(H1), . . . fi(Hi) . . . fg(Hg)], fi : IRDHi \u2192 IRDui(i = 1 . . . g), u \u2208 IRDu.\n(1)\nThe merged representation, u, serves as the input of the TempMixer block, repre-\nsenting the temporal convolutional network to model the temporal dependencies.\nThis is followed by fully connected and softmax layers:\nu\u2032 = ftemp(u), ftemp : IRDu \u2192 IRD\u2032\nu,\nv = W \u00b7 u\u2032 + b, P(y(i)) =\nevi\n\ufffdy\nj=1 evj for i = 1, 2, . . . , y.\n(2)\n\n\n178\nH. Truchan et al.\n3.2\nUnimodal Feature Representation\nThe unimodal pathway takes as input data points consisting of gv tensors Tj,\nj = 1 . . . gv, of one modality, and stacks them together along their third dimen-\nsion to create the tensor Tv with Dhv\u00d7bv\u00d7dv dimensions. To assure high-precision\nrepresentation, the tensors are \ufb01rst linearly patched, which decreases their inter-\nnal resolution, and then fed into a sequence of isotropic channel-point convolu-\ntional blocks. The patching procedure and linear embedding are seamlessly inte-\ngrated into the network [30] using a single convolution layer with dgv input chan-\nnels, kernel size k, stride p, and cgv output channels. This step enables the e\ufb00ec-\ntive representation and results in the Qv tensor with the depth of cv = gv \u00d7 dgv:\nQv = BN(\u03c3{Conv(Xi,j, stride = p, kernelSize = k)}) : Dhgv /p\u00d7bgv /p\u00d7cv. (3)\nThe downsampled input is then processed by the Gaussian Error Linear Unit (\u03c3)\nactivation function, which applies nonlinearity by weighting the input by their\npercentile and can be considered as a smoothed version of the ReLU activation\nfunction. The fast version of the GELU [15] is used, which exhibits a su\ufb03cient\ntrade-o\ufb00 between the latency and the degree of approximation:\nHv = BN(Qv \u00b7 1\n2[1 + erf( Qv\n\u221a\n2]).\n(4)\nThe activation is followed by a batch normalization (BN) layer, which normalizes\neach channel across mini-batch samples, helping to decrease the susceptibility to\nvariations throughout the data. The channel-point convolutional block consists\nof the repeated sequence of grouped convolutions and pointwise convolutions\nwith the skip connection that fosters information propagation.\nThe outcome of the convolution process of each group is combined inde-\npendently as the dot product of the input and the \ufb01lters sliding vertically and\nhorizontally across the input \ufb01eld and a sum of the bias term. The grouped\nconvolution with groups equal to the number of channels (cv) is used. Thus, it\nallows for a channel-wise separable (depth-wise separable) convolution:\nH\n\u2032\nv = BN(\u03c3{DepthConv(Hv), groups = cv}) + Hv.\n(5)\nThe depthwise convolution is followed by pointwise convolution, de\ufb01ned as the\nconvolution layer with a kernel size of 1 \u00d7 1 and a number of \ufb01lters equal to the\nnumber of channels output by the patch embedding block that allows for linear\ncombinations across channels:\nHv+1 = BN(\u03c3{PointConv(H\n\u2032\nv), kenel = 1 \u00d7 1}).\n(6)\nThe last layer is a two-dimensional global average pooling layer that performs\ndownsampling by calculating the average of the vertical and horizontal dimen-\nsions of the input volume.\n\n\nMultimodal Isotropic Neural Architecture with Patch Embedding\n179\nFig. 3. The structure of the visual modality pathway using patch embeddings.\nVisual Domain Pathway: In our architecture, the \ufb01rst pathway groups the\ntensors in the visual domain into Gv and normalizes their spatial dimensions to\nmeet the subnetwork dimension requirements. These tensors are stacked together\nalong their third dimension to create the tensor Tv with the depth of dgv:\n\u2200i = 1 . . . L, \u2200j \u2208 Gv normv(Ti,j) : Dhj\u00d7bj\u00d7dj \u2192 Dhgv \u00d7bgv \u00d7dgv .\n(7)\nFollowing the above procedure, as depicted in Fig. 3, we patch and linearly embed\nthe input tensor Tv using a single convolution layer. The outcome of Eq. 3 is then\nnormalized using the fast version of GELU, followed by batch normalization as\ndescribed in Eq. 4. The resulting tensor Hv serves as input to the sequence of\nisotropic depth-wise and point-wise convolutions that perform the operations in\nEqs. 5 and 6, respectively. The outcome of the visual domain pathway is the\nvector uv. The adjustable depth of the isotropic architecture ensures scalability\naccording to the task complexity.\nTime Series Pathway: The second pathway (Fig. 4) groups time series tensors\ninto Gs, transforms them into two-dimensional embeddings, normalizes and spa-\ntially aligns them to ensure temporal correlations, applies patch embedding, and\nextracts signal representations with the channel-point convolutions block. The\ntime series sequences of tensor Tj are \ufb01rst resampled to ngs, and the dimension\nhj is trimmed or padded to the hgs length:\n\u2200i = 1 . . . L, \u2200j \u2208 Gs norms(Ti,j) : Dhj\u00d7bj\u00d7dj \u2192 Dhgs\u00d7bj.\n(8)\nThe two-dimensional embedding is de\ufb01ned as the logarithmically scaled\namplitudes of the Mel spectrogram, which performs a windowing transforma-\ntion of the signal to create a local frequency analysis.\nFirst, the window used for the STFT calculation with a size of nstft \u2264\nngs is de\ufb01ned. Due to zero padding and varying amplitudes of Ts, the Hann\nwindowing function is used to normalize the signals. The Mel \ufb01lter bank is\nde\ufb01ned as mel(sr, nstft, nmels), where sr is the sampling rate of the incoming\nsignal, nstft is the length of the STFT window, and nmels is the number of Mel\nbands to generate. The output is a two-dimensional array IRnmels\u00d7(1+nstft/2).\n\n\n180\nH. Truchan et al.\nFig. 4. The structure of the time series modality pathway.\nTable 1. Summary of the characteristics of multimodal datasets.\nDataset\nTask\nModalities Types of Modalities\nInstances Classes\nVisuo-Haptic [4] Object recognition\n4\nImage, Pressure, Texture,\nProximity\n305\n63\nMeX [29]\nActivity recognition\n4\nImage, Acceleration, Proximity,\nPressure\n710\n7\nBAUM-1a [10]\nEmotion recognition\n2\nImage, Audio\n273\n9\nemoFBVPs [25]\nEmotion recognition\n2\nImage, Audio\n1380\n23\nHA4M [7]\nAssembly task\n6\nRGB images, Depth maps, IR\nimages, RGB-to-Depth-Alignments,\nSkeleton data\n2604\n12\nEnergy [11]\nConsumption classi\ufb01cation 5\nphotoplethysmography (PPG),\nelectrocardiography (ECG),\nAccelerometer, a fraction of oxygen\nin expired breath (VO2),\nGyroscope\n1192\n3\nMudestreda\nDevice State recognition\n4\nImage, Dynamometer data\n512\n3\nThe \ufb01nal two-dimensional embedding is obtained using the following formula:\n2Demb = log10{mel(STFT(Inputseq, stride, hann(nstft)))},\n(9)\nwhere hann(nstft) refers to the Hanning window function applied with a\nwindow length of nstft for the short-time Fourier transformation (stft). The\nstride size determines the sliding window step used for STFT, and the Mel\n\ufb01lterbank downsamples the signal to obtain the Mel spectrogram. The obtained\ntwo-dimensional representations of tensors grouped in Gs are stacked, resulting\nin the tensor Ts with ds = \ufffd\nj\u2208Gs bj channels:\n\u2200j \u2208 Gs, \u2200k = 1 . . . bj Ts = 2Demb[T:,j(:, k)] : Dhgs\u00d7bgs\u00d7ds.\n(10)\nNext, the spectrograms are patched and linearly embedded using Eq. 3, followed\nby the channel-point convolutional block in Eqs. 5 and 6.\n\n\nMultimodal Isotropic Neural Architecture with Patch Embedding\n181",
        "page_start": 3,
        "page_end": 9,
        "chunk_ids": []
      },
      {
        "id": "section_3",
        "title": "4\nExperiments",
        "content": "4\nExperiments\n4.1\nExperimental Setup\nWe test the performance of the Minape framework on several open-source mul-\ntimodal benchmarks (Table 1)1. All of them contain visual and time series data:\nVisuo-Haptic object recognition for robots contains time series (pressure, tex-\nture, proximity) and images, MeX human activity recognition contains time\nseries data (acceleration, proximity, pressure) and images, BAUM-1a face mimic\nrecognition and emoFBVPs physiological signals recognition contain images and\naudio signals, HA4M assembly task recognition contains images (RGB, depth\nmaps, infra-red images, RGB-to-Depth alignments) and time series data (skele-\nton data), and Energy consumption estimation contains images (photoplethys-\nmography) and time series data (electrocardiography, accelerometer, fraction of\noxygen in expired breath, gyroscope).\nWe present two versions of multimodal fusion neural network architecture:\none that uses isotropic neural architecture with patch embedding Minape, and\nthe second which uses patch embedding with e\ufb03cientnetv2-m for intermediate\nfeature representation (Minape-S). The TempMixer block consists of four one-\ndimensional convolution layers with 64 \ufb01lters each. The model is trained accord-\ning to an improved training procedure presented in timm [28] that combines best\npractices for training, such as novel optimization and data augmentation2. We\nhave enriched this procedure with the AdamW [23] optimizer and AutoAugment\n[8]. The results of Minape and Minape-S are reported for depth size = 16, 3\u00d7103\niterations, learning rate = 0.001, kernel size = 13, embedding dimension = 1,\nglobalAveragePooling layer, and the GELU activation function.\nIn addition, we varied the batch size, number of epochs, steps per epoch, the\npooling layer (globalMaxPooling2dLayer, globalAveragePooling), and the acti-\nvation function (GELU, ReLu). We compare Minape with two well-established\nunimodal algorithms, two recent multimodal versions, and transformers-based\narchitectures. The selected algorithms allow for testing a broad range of fusion",
        "page_start": 9,
        "page_end": 9,
        "chunk_ids": []
      },
      {
        "id": "section_4",
        "title": "methods",
        "content": "methods, e.g., multiplicative combination, embracement, averaging, concatena-\ntion, and fusion methods:\n\u2013 The Temporal Convolutional Network (TCN)3 leverages dilated causal con-\nvolutions and residual blocks for e\ufb03ciently processing long input sequences\nand learning complex temporal patterns [3].\n\u2013 E\ufb03cientNetv2-m is a convolution-based image learning method that com-\nbines training-aware neural architecture search and scaling, which adaptively\nadjusts regularization along with image size.\n\u2013 Multiplicative Multimodal network (Mulmix)4 uses multiplicative modality\nmixture combination by \ufb01rst additively creating mixture candidates and then\n1 All experiments were performed on a single Nvidia GTX1080Ti 12 GB GPU.\n2 https://github.com/martinsbruveris/tensor\ufb02ow-image-models.\n3 https://github.com/locuslab/TCN.\n4 https://github.com/skywaLKer518/MultiplicativeMultimodal.\n\n\n182\nH. Truchan et al.\nTable 2. The average accuracy percentage of our proposed framework ( Minape-S and\nMinape with primary learner) compared to other state-of-the-art methods on seven\nmultimodal benchmarks. The results are calculated on \ufb01ve trials and the ranks are\nreported.\nDataset\nTCN\u2217\nE\ufb03cientNetv2-m\u2217\u2217 Mulmix\nEmbraceNet ViT\nMinape-S\nMinape\nVisuo-Haptic\n71.9 \u00b1 2.0 (5) 70.2 \u00b1 3.8 (6)\n67.6 \u00b1 2.2 (7) 73.3 \u00b1 3.1 (4) 78.1 \u00b1 2.7 (3) 80.7 \u00b1 1.7 (2) 85.4 \u00b1 2.3 (1)\nMeX\n72.9 \u00b1 3.7 (7) 85.0 \u00b1 4.3 (5)\n81.5 \u00b1 3.5 (6) 90.8 \u00b1 3.1 (2) 87.7 \u00b1 3.2 (3)\n86.2 \u00b1 4.8 (4) 93.4 \u00b1 1.9 (1)\nBAUM-1a\n31.3 \u00b1 5.5 (7) 41.8 \u00b1 5.1 (6)\n44.0 \u00b1 3.5 (5) 48.2 \u00b1 4.5 (3) 45.6 \u00b1 2.9 (4)\n51.5 \u00b1 2.5 (2) 57.7 \u00b1 3.2 (1)\nemoFBVPs\n81.8 \u00b1 7.2 (7) 83.2 \u00b1 8.0 (6)\n86.6 \u00b1 6.0 (4) 83.4 \u00b1 5.8 (5) 89.3 \u00b1 1.8 (2)\n88.1 \u00b1 3.3 (3) 92.7 \u00b1 3.1 (1)\nHA4M\n57.5 \u00b1 9.0 (7) 61.3 \u00b1 6.5 (6)\n65.1 \u00b1 4.8 (5) 68.9 \u00b1 6.2 (4) 74.8 \u00b1 1.4 (2)\n72.7 \u00b1 3.7 (3) 76.5 \u00b1 2.9 (1)\nEnergy\n65.2 \u00b1 6.1 (7) 67.8 \u00b1 6.6 (6)\n72.3 \u00b1 5.7 (5) 74.1 \u00b1 3.9 (4) 77.3 \u00b1 2.7 (3)\n78.9 \u00b1 2.1 (2) 81.6 \u00b1 1.8 (1)\nMudestreda\n71.4 \u00b1 3.5 (7) 79.9 \u00b1 2.8 (6)\n87.1 \u00b1 2.5 (5) 91.3 \u00b1 2.0 (4) 93.7 \u00b1 2.3 (3)\n94.7 \u00b1 1.5 (2) 98.2 \u00b1 1.0 (1)\nAverage rank 6.7\n5.9\n5.3\n3.7\n2.9\n2.6\n1.0\n\u2217 - trained only on time series, \u2217\u2217 - trained only on the visual modality.\nselecting useful modality mixtures with multiplicative combination proce-\ndure [21].\n\u2013 EmbraceNet5 is a deep learning method that leverages cross-modal corre-\nlations during the training phases [6]. It uses the embracement process to\nprobabilistically select subsets of information from each modality to model\nthe inter-modal correlations.\n\u2013 Visual Transformer (ViT)6 uses the Transformer layer with self-attention\napplied to image patch sequences [9].\n4.2\nExperimental Results\nFor increased reliability, we performed \ufb01ve independent trials using ten-fold\ncross-validation by partitioning each dataset into training (80%), validation\n(10%), and testing (10%) subsets. The outcomes of this comprehensive com-\nparison are detailed in Table 2. For each method, the result of the best hyperpa-\nrameter setting is reported. All results are in terms of accuracy percentage, and\nthe mean and standard deviation of ten repeats are reported.\nThe tested datasets have varied sensor sequence lengths ranging from 276 in\nVisuo-Haptic to 1.5 \u00d7 105 in Mudestreda. We change the depth of the Minape\narchitectures from four with a width of 128 in Visuo-Haptic to 16 with a width\nof 256 in BAUM-1a. We observe that wider networks and successively down-\nsampled convolutional network designs yield better results, even when trained\nfor fewer epochs. Moreover, changing the kernel size from 5 in Mex to 13 in Mud-\nestreda indicates that larger kernels result in better performance. The kernels\nin the depthwise layer de\ufb01ne the size of the receptive \ufb01eld. Thus, large ker-\nnels mix arbitrarily distant spatial locations, allowing them to capture spatial\ndependencies more comprehensively.\nOur experiments indicate that the best results belong to patch embedding\nand isotropic feature extraction architectures. In all tested datasets, Minape\n5 https://github.com/idearibosome/embracenet.\n6 https://github.com/keras-team/keras-io/blob/master/examples/vision/\nvit_small_ds.py.\n\n\nMultimodal Isotropic Neural Architecture with Patch Embedding\n183\nFig. 5. (a) Example of force signals (Fx, Fy, Fz) from tool nr. 1 (T1) for 30 milling\nphases. (b) Comparison of multimodal models on Mudestreda. Various modalities are\nrepresented with i: image, t: time series, and m: multimodal. Minape\u2020 is a model with\nno patches. Minape (m) is 12 MB and SL-ViT (m) is 863 MB.\nachieves signi\ufb01cantly better results compared to other methods of comparison.\nFurthermore, we should note that the corresponding number of subnetworks does\nnot grow exponentially with the number of multimodalities, making the solution\nscalable, even to high-dimensional spaces. Both the patch embedding and the\nlarge kernel size preserve locality well, suggesting that the spatial representation\nis sensitive to the relative embedding dimensions and \ufb01lter size and generates\nequivalent results even without downsampling the representation in subsequent\nlayers. Given that Minape yields better results than Minape-S and the fact that\ntheir di\ufb00erence lies only in the convolutional layer architecture, it can be con-\ncluded that the performance is more related to the kernel dimension and internal\nnetwork resolution than to the depth of the classi\ufb01er.\n4.3\nMudestreda: A Multimodal Device State Recognition Use Case\nIn addition to the existing benchmarks, we study a real-case application of the\nmilling process and propose a new multimodal industrial device state recogni-\ntion dataset called Mudestreda. Mudestreda comprises 512 four-dimensional\nmultimodal observations consisting of three force signal sequences and one RGB\nimage of the shaft milling tool over \ufb01ve weeks from the Production Centrum.\nThe three dimensions of time series modality are forces recorded in three axes,\nFx, Fy, and Fz, with a frequency of 10 kHz. After each milling phase, a picture\nof the tool is taken, and the \ufb02ank wear is measured to assign each observation to\nthe respective class based on the de\ufb01ned metric: class-1 [0, 71)\u03bcm (sharp), class-2\n[71, 110)\u03bcm (used), and class-3 [110, +\u221e)\u03bcm (dulled). Figure 5a shows an exam-\nple of the collected signal, where a strong correlation between the tool wear and\nthe force amplitudes can be observed. It indicates the smallest amplitudes for\nthe sharp tool increase with tool wear.\n\n\n184\nH. Truchan et al.\nTable 3. Performance comparison of various models and modalities (image (i), time-\nseries (t), both modalities (m)) on the Mudestreda dataset in terms of accuracy and area\nunder the curve (AUC). Model\u2020 results belong to a model without patch embedding.\nThe inference time on the test set and the number of model parameters are reported\nas ms/img and #Params, respectively. The base model without a primary learner is\nindicated in bold.\nModel\nModality\nFusion-Type\nms/img\nSize(MB)\n#Params\nAUC\nAccuracy\nViT\ni\n-\n27\n421\n36.3M\n0.89\n87.5 \u00b1 3.1\nt\n-\n28\n422\n36.4M\n0.88\n83.3 \u00b1 2.7\nm\nConcat\n55\n844\n72.8M\n0.98\n93.7 \u00b1 2.3\nSL-ViT\ni\n-\n29\n428\n36.5M\n0.84\n72.9 \u00b1 2.2\nt\n-\n36\n434\n36.4M\n0.97\n92.7 \u00b1 1.8\nm\nConcat\n62\n863\n73.7M\n0.95\n91.6 \u00b1 1.5\nMinape\ni\n-\n5\n6\n0.4M\n0.95\n89.3 \u00b1 1.9\nt\n-\n6\n6\n0.4M\n0.97\n93.4 \u00b1 1.6\nm\nConcat\n11\n12\n0.9M\n0.98\n96.2 \u00b1 1.2\nMinape \u2020\ni\n-\n1,118\n3\n0.2M\n0.83\n63.6 \u00b1 2.6\nt\n-\n1,122\n3\n0.2M\n0.89\n83.5 \u00b1 1.9\nm\nConcat\n1,239\n6\n0.4M\n0.97\n93.7 \u00b1 2.2\nTable 3 provides a detailed analysis of the impact of di\ufb00erent modalities for\nMinape and ViT as the second-best comparison method in Table 2. In addition\nto ViT, we report the results for SL-ViT7, which is a modi\ufb01ed version of the\nViT with Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA)\naddressing the problem of locality inductive bias and allowing training a small-\nsize datasets [16]. The SL-ViT parameters are set as ViT with \ufb02ags spt = true,\nlsa = true.\nTable 3 shows that considering the combination of various modalities signi\ufb01-\ncantly enhances model accuracy across all examined models. Patches in Minape\ne\ufb00ectively improve the model\u2019s accuracy while reducing inference times. How-\never, transformer models (ViT, SL-ViT), despite their comparable accuracy, have\nlonger inference times and higher memory demands, which limit their applica-\nbility. The Minape model stands out with the highest accuracy, lowest memory\nusage, and shortest latency, making it a top choice for practical applications,\ngiven its balanced performance metrics. Figure 5b illustrates that Minape signif-\nicantly outperforms other models in terms of inference latency and model size\n(MB). Minape exhibits a model parameter size reduction of more than 80 times\ncompared to the multimodal ViT.\nWe scrutinized the sensitivity of Minape on the Mudestreda dataset, as\nshown in Table 4. The results demonstrate that the base model, when utilizing a\nconcatenation fusion strategy, signi\ufb01cantly outperforms other fusion types. The\nkey advantage of concatenation fusion is its simplicity, as it does not introduce\nadditional hyperparameters or necessitate complex computations. This not only\n7 https://github.com/keras-team/keras-io/blob/master/examples/vision/\nvit_small_ds.py.\n\n\nMultimodal Isotropic Neural Architecture with Patch Embedding\n185\nTable 4. The results for sensitivity analysis of Minape on the Mudestreda dataset.\nThe best model is indicated in bold.\nMinape\nFusion-Type\nPrime Learner\nms/img\nSize(MB)\n#Param(M)\nAccuracy\n\u0394(%)\nFusion\nAdd\n\u00d7\n10\n6\n1.3\n87.3 \u00b1 2.8\n\u22128.9\nMultiply\n\u00d7\n12\n6\n1.3\n91.6 \u00b1 1.7\n\u22124.6\nAverage\n\u00d7\n11\n6\n1.3\n87.5 \u00b1 2.5\n\u22128.7\nLearner\nConcat\nGRU\n10\n13\n1.9\n96.3 \u00b1 1.2\n+0.1\nConcat\nLSTM\n10\n16\n2.1\n94.3 \u00b1 1.6\n\u22121.9\nConcat\nTCN\n11\n39\n4.0\n98.2 \u00b1 1.0\n+2\nAttention \u2713\nConcat\n\u00d7\n15\n8\n2.0\n95.1 \u00b1 1.4\n\u22121.1\nAttention \u2713\nConcat\nTCN\n16\n17\n2.8\n94.9 \u00b1 1.8\n\u22121.3\nensures e\ufb03cient memory utilization but also meets the computational require-\nments of the system. Among our evaluated primary learners, which include GRU,\nLSTM, and TCN, the TCN model enhanced the baseline performance by 2%\nwithout any impact on the inference time. Our study also underscores the rel-\nevance of the attention mechanism in this work\ufb02ow. Further experiments show\nthat the attention layer does not improve the model performance8.",
        "page_start": 9,
        "page_end": 13,
        "chunk_ids": []
      },
      {
        "id": "section_5",
        "title": "5\nConclusion",
        "content": "5\nConclusion\nWe proposed Minape, a novel multimodal isotropic neural architecture with\npatch embedding, to improve multimodal learning for time series and image data.\nWe observed that the patch representation and wider networks that successively\ndown-sample the convolutional network size yield better results than deeper\nmodels and even ViT. It uses less memory and has faster inference allowing\nthe use of the model on stand-by devices. The empirical results demonstrate\nthat the modality fusion with the patch embedding yields higher accuracy than\nstate-of-the-art and baseline methods on six multimodal test benchmarks and\nMudestreda, our newly introduced real-case multimodal device state recognition\ndataset.\nReferences\n1. van Amsterdam, B., Kadkhodamohammadi, A., Luengo, I., Stoyanov, D.: Aspnet:\naction segmentation with shared-private representation of multiple data sources.\nIn: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 2384\u20132393 (2023)\n2. Aslam, M.H., Zeeshan, M.O., Pedersoli, M., Koerich, A.L., Bacon, S., Granger, E.:\nPrivileged knowledge distillation for dimensional emotion recognition in the wild.\nIn: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 3337\u20133346 (2023)\n8 Further ablation studies on the impact of hyperparameters can be found at https://\ngithub.com/hubtru/Minape.\n\n\n186\nH. Truchan et al.\n3. Bai, S., Kolter, J.Z., Koltun, V.: An empirical evaluation of generic convolutional\nand recurrent networks for sequence modeling. arXiv:1803.01271 (2018)\n4. Bonner, L.E.R., Buhl, D.D., Kristensen, K., Navarro-Guerrero, N.: Au dataset for\nvisuo-haptic object recognition for robots. arXiv preprint arXiv:2112.13761 (2021)\n5. Chen, S., Guhur, P.L., Schmid, C., Laptev, I.: History aware multimodal trans-\nformer for vision-and-language navigation. Adv. Neural Inform. Process. Syst.\n(NeurIPS) 34, 5834\u20135847 (2021)\n6. Choi, J.H., Lee, J.S.: Embracenet: a robust deep learning architecture for multi-\nmodal classi\ufb01cation. Inform. Fusion 51, 259\u2013270 (2019)\n7. Cicirelli, G., et al.: The ha4m dataset: multi-modal monitoring of an assembly task\nfor human action recognition in manufacturing. Sci. Data 9(1), 745 (2022)\n8. Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning\naugmentation policies from data. arXiv preprint arXiv:1805.09501 (2018)\n9. Dosovitskiy, A., et al.: An image is worth 16x16 words: transformers for image\nrecognition at scale. In: International Conference on Learning Representation\n(ICLR) (2021)\n10. Eroglu Erdem, C., Turan, C., Aydin, Z.: Baum-2: a multilingual audio-visual a\ufb00ec-\ntive face database. Multimed. Tools Appl. 74(18), 7429\u20137459 (2015)\n11. Gashi, S., Min, C., Montanari, A., Santini, S., Kawsar, F.: A multidevice and mul-\ntimodal dataset for human energy expenditure estimation using wearable devices.\nSci. Data 9(1), 537 (2022)\n12. Geng, T., Wang, T., Duan, J., Cong, R., Zheng, F.: Dense-localizing audio-visual\nevents in untrimmed videos: A large-scale benchmark and baseline. In: IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 22942\u2013\n22951 (2023)\n13. Girdhar, R., et al.: Imagebind: one embedding space to bind them all. In:\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 15180\u201315190 (2023)\n14. Gong, X., et al.: MMG-ego4D: multimodal generalization in egocentric action\nrecognition. In: IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (CVPR), pp. 6481\u20136491 (2023)\n15. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 (2016)\n16. Lee, S.H., Lee, S., Song, B.C.: Vision transformer for small-size datasets. arXiv\npreprint arXiv:2112.13492 (2021)\n17. Li, Y., Quan, R., Zhu, L., Yang, Y.: E\ufb03cient multimodal fusion via interactive\nprompting. In: IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pp. 2604\u20132613 (2023)\n18. Lialin, V., Rawls, S., Chan, D., Ghosh, S., Rumshisky, A., Hamza, W.: Scalable and\naccurate self-supervised multimodal representation learning without aligned video\nand text data. In: IEEE/CVF Winter Conference on Applications of Computer\nVision (WACV), pp. 390\u2013400 (2023)\n19. Lin, Y.B., Sung, Y.L., Lei, J., Bansal, M., Bertasius, G.: Vision transformers are\nparameter-e\ufb03cient audio-visual learners. In: IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 2299\u20132309 (2023)\n20. Lin, Y.B., Tseng, H.Y., Lee, H.Y., Lin, Y.Y., Yang, M.H.: Exploring cross-video\nand cross-modality signals for weakly-supervised audio-visual video parsing. Adv.\nNeural Inform. Process. Syst. (NeurIPS) 34, 11449\u201311461 (2021)\n21. Liu, K., Li, Y., Xu, N., Natarajan, P.: Learn to combine modalities in multimodal\ndeep learning. arXiv preprint arXiv:1805.11730 (2018)\n\n\nMultimodal Isotropic Neural Architecture with Patch Embedding\n187\n22. Liu, X., Lu, H., Yuan, J., Li, X.: Cat: causal audio transformer for audio classi\ufb01ca-\ntion. In: IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pp. 1\u20135 (2023)\n23. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International\nConference on Learning Representation (ICLR) (2018)\n24. Ramazanova, M., Escorcia, V., Caba, F., Zhao, C., Ghanem, B.: Owl (observe,\nwatch, listen): Audiovisual temporal context for localizing actions in egocentric\nvideos. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 4879\u20134889 (2023)\n25. Ranganathan, H., Chakraborty, S., Panchanathan, S.: Multimodal emotion recog-\nnition using deep learning architectures. In: IEEE winter conference on Applica-\ntions of Computer Vision (WACV), pp. 1\u20139 (2016)\n26. Ryan, F., Jiang, H., Shukla, A., Rehg, J.M., Ithapu, V.K.: Egocentric auditory\nattention localization in conversations. In: IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 14663\u201314674 (2023)\n27. Senocak, A., Kim, J., Oh, T.H., Li, D., Kweon, I.S.: Event-speci\ufb01c audio-visual\nfusion layers: a simple and new perspective on video understanding. In: IEEE/CVF\nWinter Conference on Applications of Computer Vision (WACV), pp. 2237\u20132247\n(2023)\n28. Wightman, R., Touvron, H., J\u00e9gou, H.: Resnet strikes back: an improved training\nprocedure in timm. arXiv preprint arXiv:2110.00476 (2021)\n29. Wijekoon, A., Wiratunga, N., Cooper, K.: Mex: multi-modal exercises dataset for\nhuman activity recognition. arXiv preprint arXiv:1908.08992 (2019)\n30. Wu, H., et al.: Cvt: introducing convolutions to vision transformers. In: IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 22\u201331\n(2021)\n31. Xiao, Y., Ma, Y., Li, S., Zhou, H., Liao, R., Li, X.: Semanticac: semantics-assisted\nframework for audio classi\ufb01cation. In: IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pp. 1\u20135 (2023)\n32. Xu, R., Feng, R., Zhang, S.X., Hu, D.: Mmcosine: multi-modal cosine loss towards\nbalanced audio-visual \ufb01ne-grained learning. In: IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pp. 1\u20135 (2023)\n33. Xue, Z., Marculescu, R.: Dynamic multimodal fusion. In: IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pp. 2574\u20132583 (2023)\n34. Zhang, X., Tang, X., Zong, L., Liu, X., Mu, J.: Deep multimodal clustering with\ncross reconstruction. In: Paci\ufb01c-Asia Conference on Knowledge Discovery and Data\nMining (PAKDD), pp. 305\u2013317 (2020)\n35. Zhang, Z., et al.: Abaw5 challenge: a facial a\ufb00ect recognition approach utilizing\ntransformer encoder and audiovisual fusion. In: IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pp. 5724\u20135733 (2023)\n36. Zhong, Z., Schneider, D., Voit, M., Stiefelhagen, R., Beyerer, J.: Anticipative fea-\nture fusion transformer for multi-modal action anticipation. In: IEEE/CVF Winter\nConference on Applications of Computer Vision (WACV), pp. 6068\u20136077 (2023)\n37. Zhu, W., Omar, M.: Multiscale audio spectrogram transformer for e\ufb03cient audio\nclassi\ufb01cation. In: IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 1\u20135 (2023)",
        "page_start": 13,
        "page_end": 15,
        "chunk_ids": []
      }
    ],
    "total_pages": 15,
    "processed_at": "2025-11-17T01:51:51.780178",
    "created_at": "2025-11-17T01:51:51.780178"
  }
}