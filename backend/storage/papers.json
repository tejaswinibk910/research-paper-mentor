{
  "4427277e-c272-4ff3-95e6-f236e1053d95": {
    "id": "4427277e-c272-4ff3-95e6-f236e1053d95",
    "filename": "978-981-99-8079-6_14.pdf",
    "status": "ready",
    "metadata": {
      "title": "Multimodal Isotropic Neural Architecture with Patch Embedding",
      "authors": [
        "Hubert Truchan"
      ],
      "abstract": null,
      "keywords": [
        "Multimodal Classification",
        "Isotropic Architecture",
        "Patch Embedding",
        "Time Series"
      ],
      "publication_date": null,
      "doi": null
    },
    "sections": [
      {
        "id": "section_0",
        "title": "Abstract",
        "content": "Abstract. Patch embedding has been a signi\ufb01cant advancement in\nTransformer-based models, particularly the Vision Transformer (ViT),\nas it enables handling larger image sizes and mitigating the quadratic\nruntime of self-attention layers in Transformers. Moreover, it allows\nfor capturing global dependencies and relationships between patches,\nenhancing e\ufb00ective image understanding and analysis. However, it is\nimportant to acknowledge that Convolutional Neural Networks (CNNs)\ncontinue to excel in scenarios with limited data availability. Their e\ufb03-\nciency in terms of memory usage and latency makes them particularly\nsuitable for deployment on edge devices. Expanding upon this, we pro-\npose Minape, a novel multimodal isotropic convolutional neural archi-\ntecture that incorporates patch embedding to both time series and\nimage data for classi\ufb01cation purposes. By employing isotropic models,\nMinape addresses the challenges posed by varying data sizes and com-\nplexities of the data. It groups samples based on modality type, creating\ntwo-dimensional representations that undergo linear embedding before\nbeing processed by a scalable isotropic convolutional network architec-\nture. The outputs of these pathways are merged and fed to a temporal\nclassi\ufb01er. Experimental results demonstrate that Minape signi\ufb01cantly\noutperforms existing approaches in terms of accuracy while requiring\nfewer than 1M parameters and occupying less than 12 MB in size. This\nperformance was observed on multimodal benchmark datasets and the\nauthors\u2019 newly collected multi-dimensional multimodal dataset, Mude-\nstreda, obtained from real industrial processing devices1(1Link to code\nand dataset: https://github.com/hubtru/Minape).\nKeywords: Multimodal Classi\ufb01cation \u00b7 Isotropic Architecture \u00b7 Patch\nEmbedding \u00b7 Time Series",
        "page_start": 1,
        "page_end": 1,
        "chunk_ids": []
      },
      {
        "id": "section_1",
        "title": "1\nIntroduction",
        "content": "1\nIntroduction\nHumans perceive the world through multiple senses, such as vision and hearing,\nleading to a multimodal understanding. This combination of data from di\ufb00er-\nent modalities o\ufb00ers augmented and complementary information, enabling more\nrobust inference. In recent years, deep learning approaches have made remarkable\nprogress in leveraging data from various modalities, resulting in improved perfor-\nmance across classical problems, including action recognition [14] and semantic\nc\n\u20dd The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2024\nB. Luo et al. (Eds.): ICONIP 2023, LNCS 14447, pp. 173\u2013187, 2024.\nhttps://doi.org/10.1007/978-981-99-8079-6_14\n\n\n174\nH. Truchan et al.\nsegmentation [1]. Despite these advancements, e\ufb00ectively integrating informa-\ntion from multiple modalities remains a fundamental challenge in multimodal\nlearning. Several research e\ufb00orts have focused on designing fusion paradigms\nto fuse multimodal data [12,33]. However, these approaches often require man-\nual design and are speci\ufb01c to certain tasks and modalities, primarily focusing\non image and text as the common modalities [31]. Yet, in applications such as\nintelligent production and healthcare, time series and audio data serve as the\nmajor source of information. For instance, device state recognition is a vital\nproblem in intelligent production and healthcare systems, demanding precise\nmonitoring due to the dynamic nature of the device\u2019s physical and environmen-\ntal properties. Maintenance reports show that these devices exhibit symptoms\nof changing conditions and damage before experiencing complete failure during\noperation. These symptoms, including distinctive sounds, can serve as indicators\nfor identifying the state of the device. Traditional methods like single modality\napproaches [22] or self-supervised [18] are impractical in these scenarios due to\nthe complexity and variability of data. Such applications often su\ufb00er from noise\nand con\ufb02icts between modalities, which can signi\ufb01cantly impact prediction accu-\nracy. Moreover, the limited availability of training samples further complicates\nthe development of e\ufb03cient models. An ideal algorithm should address these\nchallenges by being robust to noise, selectively leveraging strong modalities, and\ne\ufb00ectively capturing complementary information among modalities.\nIn this paper, we present (Minape), a novel multimodal isotropic neural\narchitecture with patch embedding that integrates time series and image data\nas input. Minape is based on convolutional neural networks (CNNs) and uses a\npatch-based representation to learn local features from the data. One key advan-\ntage of Minape is its isotropic nature, which allows it to handle inputs of varying\nsizes and aspect ratios and to be trained with fewer labelled examples compared\nto existing methods. Minape can be employed in numerous applications, includ-\ning healthcare systems, intelligent production, and real-time monitoring of device\nstates. Through extensive experiments, we demonstrate that Minape achieves\nsigni\ufb01cantly higher accuracy compared to the state-of-the-art approaches on\nboth public multimodal classi\ufb01cation datasets and our newly introduced multi-\nmodal device state dataset (Mudestreda) collected from industrial processing\ndevices. In addition, Minape requires signi\ufb01cantly less memory to store its much\nsmaller model and can be trained on small and medium size multimodal datasets.\nOverall, the contributions of this paper are as follows:\n\u2013 The Minape framework, a novel multimodal isotropic convolutional architec-\nture with patch embedding that requires less than 12 MB size and less than\n1M Parameters, which yields the inference output over 90 images/s (results\nreported for Nvidia 1080TI) for audiovisual data,\n\u2013 An alternate solution to transformer-based fusion models that can be e\ufb00ec-\ntively trained on the small and medium size multimodal datasets with less\nthan 1k instances and capable of operating on edge devices and being deployed\nin real standalone systems,\n\n\nMultimodal Isotropic Neural Architecture with Patch Embedding\n175\nFig. 1. A sample instance from Mudestreda that shows the importance of leveraging\nmultimodal data for device state prediction.\n\u2013 Minape yields signi\ufb01cantly higher accuracy compared to the state-of-the-art\napproaches. Figure 1 intuitively demonstrates that this augmentation leads to\na better predictive performance by an example from Mudestreda, emphasizing\nits real-world application readiness,\n\u2013 Minape exhibits scalability concerning model pathways and depth dimension.\nIt permits \ufb02exible model scaling based on data size and task complexity,\n\u2013 Finally, we share our collected data, Mudestreda, which is publicly available\nand aims for multimodal device state recognition.",
        "page_start": 1,
        "page_end": 3,
        "chunk_ids": []
      },
      {
        "id": "section_2",
        "title": "2\nRelated Work",
        "content": "2\nRelated Work\n2.1\nMultimodal Fusion\nThe focus of multimodal fusion has been primarily on exploring di\ufb00erent archi-\ntectures and techniques for e\ufb00ectively integrating di\ufb00erent data modalities to\nimprove the performance of the model. In particular, an Action Segmentation\nmodel (ASPnet) based on ASFormer disentangles hidden features into modality-\nshared components and projects them into independent FC layers [1]. Similarly,\na Transformer-based approach [13] creates a single shared representation space\nusing multiple types of image-paired data. [24] also utilized a Transformer-based\narchitecture to leverage audiovisual context in action localization. Other works\nintroduced diverse multimodal fusion strategies involving semantic fusion [31],\ntemporal sequence-based fusion [36], and cross-attention mechanism [18]. Our\napproach simpli\ufb01es the fusion process by using isotropic convolutional neu-\nral architectures, making it computationally e\ufb03cient and suitable for smaller\ndatasets.\n\n\n176\nH. Truchan et al.\n2.2\nMultimodal Model Optimisation\nFocusing on model optimization, the primary limitation in the existing liter-\nature relates to the resource-demanding nature of the algorithms. In order to\nmanage the long-range temporal dependencies in the data, an attention bottle-\nneck was used, but that solution resulted in high memory demand [1]. Similarly, a\njoint embedding solution reduced the modalities [13], yet the model is resource-\nintensive with high memory usage. Another approach proposed an e\ufb03cient fusion\nstrategy via prompt-based techniques [17] but still required more than 12 GB of\nmemory for training. Other methods also developed innovative ways to optimize\ntheir networks, for instance, feature anticipation [36], modality-wise L2 normal-\nization [32], progressive reduction of tokens [37], but they all required signi\ufb01cant\ncomputational resources, including large memory and high model size.\nAdditionally, recent works applied SWIN-transformer [22], TimeSformer [18],\nVGG-M [27], 3D-Resnet-18 [26] and all reported models with parameters exceed-\ning tens of millions, far above the size of our proposed model. Similarly, recent\nresearch showcased innovative techniques with pyramid cross-fusion Trans-\nformer [35], contrastive-based alignment training [14], modality dropout train-\ning [12], single channel version of ViT [14], knowledge distillation [2], cross-modal\nprototypical loss [33] and injecting trainable parameters into a frozen ViT [19],\nbut these methods were computationally expensive and required large-scale mul-\ntimodal datasets for pre-training. Furthermore, recent works presented history-\naware [5] and weakly-supervised parsing [20], but these are not feasible for small-\nto medium-sized multimodal datasets. In contrast, our work proposes a novel,\nlightweight isotropic convolutional architecture that performs well on limited\ndatasets, requiring less than 1M parameters and less than 12 MB size which is\n24M parameter and 88 MB less than the lightest available model.\n3\nMinape: A Multimodal Isotropic Neural Framework\nWe consider a target process that generates a sequence of M-modal multi-\ndimensional data points, X(1), X(2), . . . , X(l), consisting of time series and\nimages and expressed as the set of tensors T1,1, T1,2, ..., TL,M, where X \u2208 IRD\nand the mth tensor is Dh\u00d7b\u00d7d\nl,m\ndimensional in IRD, where h \u00d7 b \u00d7 d describes\ntensor dimension, as height, bright and depth respectively, M is the number of\nauxiliary sensors, and l is the number of data points. Tensors are grouped into\ng groups according to their modality, where Ti,G1, i = 1..L, G1 \u2208 M is the set of\nthe tensors belonging to modality type one of the size g1.\nOur objective is to train a classi\ufb01er f : IRD \u2192 y where y is the class of\ndata point X(i). This is measured by sparse categorical cross-entropy in our\nexperiments. We assume the samples are drawn from an ergodic and stationary\nprocess. Multiple sensors are grouped in a cluster to reduce the Wasserstein\ndistance between multimodal feature distributions [34].\n\n\nMultimodal Isotropic Neural Architecture with Patch Embedding\n177\nFig. 2. The Minape general architecture.\nTo bene\ufb01t from the synergy of the available M multimodal tensors and\nenhance the manifolds mapping abilities, we propose deep concatenation of the\nlatent spaces of the linearly embedded two-dimensional representation of input\nsamples from f1:g classi\ufb01ers to train a classi\ufb01er f \u2032 : IRDu \u2192 y, where Du rep-\nresents the dimension of the deep concatenation layer, and g is the number of\nmodalities of the same type.\n3.1\nThe Minape Architecture Overview\nMinape (Fig. 2) is a two-pathway convolutional-based model with visual and\ntime-series feature extractions to obtain a joint representation of the multimodal\ndata. The shared multimodal representation is then passed to the recurrent neu-\nral layer to capture the long-term dependencies of the data sequence. The data\npoints comprise M tensors, which are initially grouped into g groups based on\ntheir modality type and input into representative fi, i = (1 . . . g) subnetworks\nthat share parameters across tensors in the same modality group. Then, the\nlearned representations of modalities ui, i = (1 . . . g) are merged using the con-\ncatenation layer, resulting in vector u:\nu = [f1(H1), . . . fi(Hi) . . . fg(Hg)], fi : IRDHi \u2192 IRDui(i = 1 . . . g), u \u2208 IRDu.\n(1)\nThe merged representation, u, serves as the input of the TempMixer block, repre-\nsenting the temporal convolutional network to model the temporal dependencies.\nThis is followed by fully connected and softmax layers:\nu\u2032 = ftemp(u), ftemp : IRDu \u2192 IRD\u2032\nu,\nv = W \u00b7 u\u2032 + b, P(y(i)) =\nevi\n\ufffdy\nj=1 evj for i = 1, 2, . . . , y.\n(2)\n\n\n178\nH. Truchan et al.\n3.2\nUnimodal Feature Representation\nThe unimodal pathway takes as input data points consisting of gv tensors Tj,\nj = 1 . . . gv, of one modality, and stacks them together along their third dimen-\nsion to create the tensor Tv with Dhv\u00d7bv\u00d7dv dimensions. To assure high-precision\nrepresentation, the tensors are \ufb01rst linearly patched, which decreases their inter-\nnal resolution, and then fed into a sequence of isotropic channel-point convolu-\ntional blocks. The patching procedure and linear embedding are seamlessly inte-\ngrated into the network [30] using a single convolution layer with dgv input chan-\nnels, kernel size k, stride p, and cgv output channels. This step enables the e\ufb00ec-\ntive representation and results in the Qv tensor with the depth of cv = gv \u00d7 dgv:\nQv = BN(\u03c3{Conv(Xi,j, stride = p, kernelSize = k)}) : Dhgv /p\u00d7bgv /p\u00d7cv. (3)\nThe downsampled input is then processed by the Gaussian Error Linear Unit (\u03c3)\nactivation function, which applies nonlinearity by weighting the input by their\npercentile and can be considered as a smoothed version of the ReLU activation\nfunction. The fast version of the GELU [15] is used, which exhibits a su\ufb03cient\ntrade-o\ufb00 between the latency and the degree of approximation:\nHv = BN(Qv \u00b7 1\n2[1 + erf( Qv\n\u221a\n2]).\n(4)\nThe activation is followed by a batch normalization (BN) layer, which normalizes\neach channel across mini-batch samples, helping to decrease the susceptibility to\nvariations throughout the data. The channel-point convolutional block consists\nof the repeated sequence of grouped convolutions and pointwise convolutions\nwith the skip connection that fosters information propagation.\nThe outcome of the convolution process of each group is combined inde-\npendently as the dot product of the input and the \ufb01lters sliding vertically and\nhorizontally across the input \ufb01eld and a sum of the bias term. The grouped\nconvolution with groups equal to the number of channels (cv) is used. Thus, it\nallows for a channel-wise separable (depth-wise separable) convolution:\nH\n\u2032\nv = BN(\u03c3{DepthConv(Hv), groups = cv}) + Hv.\n(5)\nThe depthwise convolution is followed by pointwise convolution, de\ufb01ned as the\nconvolution layer with a kernel size of 1 \u00d7 1 and a number of \ufb01lters equal to the\nnumber of channels output by the patch embedding block that allows for linear\ncombinations across channels:\nHv+1 = BN(\u03c3{PointConv(H\n\u2032\nv), kenel = 1 \u00d7 1}).\n(6)\nThe last layer is a two-dimensional global average pooling layer that performs\ndownsampling by calculating the average of the vertical and horizontal dimen-\nsions of the input volume.\n\n\nMultimodal Isotropic Neural Architecture with Patch Embedding\n179\nFig. 3. The structure of the visual modality pathway using patch embeddings.\nVisual Domain Pathway: In our architecture, the \ufb01rst pathway groups the\ntensors in the visual domain into Gv and normalizes their spatial dimensions to\nmeet the subnetwork dimension requirements. These tensors are stacked together\nalong their third dimension to create the tensor Tv with the depth of dgv:\n\u2200i = 1 . . . L, \u2200j \u2208 Gv normv(Ti,j) : Dhj\u00d7bj\u00d7dj \u2192 Dhgv \u00d7bgv \u00d7dgv .\n(7)\nFollowing the above procedure, as depicted in Fig. 3, we patch and linearly embed\nthe input tensor Tv using a single convolution layer. The outcome of Eq. 3 is then\nnormalized using the fast version of GELU, followed by batch normalization as\ndescribed in Eq. 4. The resulting tensor Hv serves as input to the sequence of\nisotropic depth-wise and point-wise convolutions that perform the operations in\nEqs. 5 and 6, respectively. The outcome of the visual domain pathway is the\nvector uv. The adjustable depth of the isotropic architecture ensures scalability\naccording to the task complexity.\nTime Series Pathway: The second pathway (Fig. 4) groups time series tensors\ninto Gs, transforms them into two-dimensional embeddings, normalizes and spa-\ntially aligns them to ensure temporal correlations, applies patch embedding, and\nextracts signal representations with the channel-point convolutions block. The\ntime series sequences of tensor Tj are \ufb01rst resampled to ngs, and the dimension\nhj is trimmed or padded to the hgs length:\n\u2200i = 1 . . . L, \u2200j \u2208 Gs norms(Ti,j) : Dhj\u00d7bj\u00d7dj \u2192 Dhgs\u00d7bj.\n(8)\nThe two-dimensional embedding is de\ufb01ned as the logarithmically scaled\namplitudes of the Mel spectrogram, which performs a windowing transforma-\ntion of the signal to create a local frequency analysis.\nFirst, the window used for the STFT calculation with a size of nstft \u2264\nngs is de\ufb01ned. Due to zero padding and varying amplitudes of Ts, the Hann\nwindowing function is used to normalize the signals. The Mel \ufb01lter bank is\nde\ufb01ned as mel(sr, nstft, nmels), where sr is the sampling rate of the incoming\nsignal, nstft is the length of the STFT window, and nmels is the number of Mel\nbands to generate. The output is a two-dimensional array IRnmels\u00d7(1+nstft/2).\n\n\n180\nH. Truchan et al.\nFig. 4. The structure of the time series modality pathway.\nTable 1. Summary of the characteristics of multimodal datasets.\nDataset\nTask\nModalities Types of Modalities\nInstances Classes\nVisuo-Haptic [4] Object recognition\n4\nImage, Pressure, Texture,\nProximity\n305\n63\nMeX [29]\nActivity recognition\n4\nImage, Acceleration, Proximity,\nPressure\n710\n7\nBAUM-1a [10]\nEmotion recognition\n2\nImage, Audio\n273\n9\nemoFBVPs [25]\nEmotion recognition\n2\nImage, Audio\n1380\n23\nHA4M [7]\nAssembly task\n6\nRGB images, Depth maps, IR\nimages, RGB-to-Depth-Alignments,\nSkeleton data\n2604\n12\nEnergy [11]\nConsumption classi\ufb01cation 5\nphotoplethysmography (PPG),\nelectrocardiography (ECG),\nAccelerometer, a fraction of oxygen\nin expired breath (VO2),\nGyroscope\n1192\n3\nMudestreda\nDevice State recognition\n4\nImage, Dynamometer data\n512\n3\nThe \ufb01nal two-dimensional embedding is obtained using the following formula:\n2Demb = log10{mel(STFT(Inputseq, stride, hann(nstft)))},\n(9)\nwhere hann(nstft) refers to the Hanning window function applied with a\nwindow length of nstft for the short-time Fourier transformation (stft). The\nstride size determines the sliding window step used for STFT, and the Mel\n\ufb01lterbank downsamples the signal to obtain the Mel spectrogram. The obtained\ntwo-dimensional representations of tensors grouped in Gs are stacked, resulting\nin the tensor Ts with ds = \ufffd\nj\u2208Gs bj channels:\n\u2200j \u2208 Gs, \u2200k = 1 . . . bj Ts = 2Demb[T:,j(:, k)] : Dhgs\u00d7bgs\u00d7ds.\n(10)\nNext, the spectrograms are patched and linearly embedded using Eq. 3, followed\nby the channel-point convolutional block in Eqs. 5 and 6.\n\n\nMultimodal Isotropic Neural Architecture with Patch Embedding\n181",
        "page_start": 3,
        "page_end": 9,
        "chunk_ids": []
      },
      {
        "id": "section_3",
        "title": "4\nExperiments",
        "content": "4\nExperiments\n4.1\nExperimental Setup\nWe test the performance of the Minape framework on several open-source mul-\ntimodal benchmarks (Table 1)1. All of them contain visual and time series data:\nVisuo-Haptic object recognition for robots contains time series (pressure, tex-\nture, proximity) and images, MeX human activity recognition contains time\nseries data (acceleration, proximity, pressure) and images, BAUM-1a face mimic\nrecognition and emoFBVPs physiological signals recognition contain images and\naudio signals, HA4M assembly task recognition contains images (RGB, depth\nmaps, infra-red images, RGB-to-Depth alignments) and time series data (skele-\nton data), and Energy consumption estimation contains images (photoplethys-\nmography) and time series data (electrocardiography, accelerometer, fraction of\noxygen in expired breath, gyroscope).\nWe present two versions of multimodal fusion neural network architecture:\none that uses isotropic neural architecture with patch embedding Minape, and\nthe second which uses patch embedding with e\ufb03cientnetv2-m for intermediate\nfeature representation (Minape-S). The TempMixer block consists of four one-\ndimensional convolution layers with 64 \ufb01lters each. The model is trained accord-\ning to an improved training procedure presented in timm [28] that combines best\npractices for training, such as novel optimization and data augmentation2. We\nhave enriched this procedure with the AdamW [23] optimizer and AutoAugment\n[8]. The results of Minape and Minape-S are reported for depth size = 16, 3\u00d7103\niterations, learning rate = 0.001, kernel size = 13, embedding dimension = 1,\nglobalAveragePooling layer, and the GELU activation function.\nIn addition, we varied the batch size, number of epochs, steps per epoch, the\npooling layer (globalMaxPooling2dLayer, globalAveragePooling), and the acti-\nvation function (GELU, ReLu). We compare Minape with two well-established\nunimodal algorithms, two recent multimodal versions, and transformers-based\narchitectures. The selected algorithms allow for testing a broad range of fusion",
        "page_start": 9,
        "page_end": 9,
        "chunk_ids": []
      },
      {
        "id": "section_4",
        "title": "methods",
        "content": "methods, e.g., multiplicative combination, embracement, averaging, concatena-\ntion, and fusion methods:\n\u2013 The Temporal Convolutional Network (TCN)3 leverages dilated causal con-\nvolutions and residual blocks for e\ufb03ciently processing long input sequences\nand learning complex temporal patterns [3].\n\u2013 E\ufb03cientNetv2-m is a convolution-based image learning method that com-\nbines training-aware neural architecture search and scaling, which adaptively\nadjusts regularization along with image size.\n\u2013 Multiplicative Multimodal network (Mulmix)4 uses multiplicative modality\nmixture combination by \ufb01rst additively creating mixture candidates and then\n1 All experiments were performed on a single Nvidia GTX1080Ti 12 GB GPU.\n2 https://github.com/martinsbruveris/tensor\ufb02ow-image-models.\n3 https://github.com/locuslab/TCN.\n4 https://github.com/skywaLKer518/MultiplicativeMultimodal.\n\n\n182\nH. Truchan et al.\nTable 2. The average accuracy percentage of our proposed framework ( Minape-S and\nMinape with primary learner) compared to other state-of-the-art methods on seven\nmultimodal benchmarks. The results are calculated on \ufb01ve trials and the ranks are\nreported.\nDataset\nTCN\u2217\nE\ufb03cientNetv2-m\u2217\u2217 Mulmix\nEmbraceNet ViT\nMinape-S\nMinape\nVisuo-Haptic\n71.9 \u00b1 2.0 (5) 70.2 \u00b1 3.8 (6)\n67.6 \u00b1 2.2 (7) 73.3 \u00b1 3.1 (4) 78.1 \u00b1 2.7 (3) 80.7 \u00b1 1.7 (2) 85.4 \u00b1 2.3 (1)\nMeX\n72.9 \u00b1 3.7 (7) 85.0 \u00b1 4.3 (5)\n81.5 \u00b1 3.5 (6) 90.8 \u00b1 3.1 (2) 87.7 \u00b1 3.2 (3)\n86.2 \u00b1 4.8 (4) 93.4 \u00b1 1.9 (1)\nBAUM-1a\n31.3 \u00b1 5.5 (7) 41.8 \u00b1 5.1 (6)\n44.0 \u00b1 3.5 (5) 48.2 \u00b1 4.5 (3) 45.6 \u00b1 2.9 (4)\n51.5 \u00b1 2.5 (2) 57.7 \u00b1 3.2 (1)\nemoFBVPs\n81.8 \u00b1 7.2 (7) 83.2 \u00b1 8.0 (6)\n86.6 \u00b1 6.0 (4) 83.4 \u00b1 5.8 (5) 89.3 \u00b1 1.8 (2)\n88.1 \u00b1 3.3 (3) 92.7 \u00b1 3.1 (1)\nHA4M\n57.5 \u00b1 9.0 (7) 61.3 \u00b1 6.5 (6)\n65.1 \u00b1 4.8 (5) 68.9 \u00b1 6.2 (4) 74.8 \u00b1 1.4 (2)\n72.7 \u00b1 3.7 (3) 76.5 \u00b1 2.9 (1)\nEnergy\n65.2 \u00b1 6.1 (7) 67.8 \u00b1 6.6 (6)\n72.3 \u00b1 5.7 (5) 74.1 \u00b1 3.9 (4) 77.3 \u00b1 2.7 (3)\n78.9 \u00b1 2.1 (2) 81.6 \u00b1 1.8 (1)\nMudestreda\n71.4 \u00b1 3.5 (7) 79.9 \u00b1 2.8 (6)\n87.1 \u00b1 2.5 (5) 91.3 \u00b1 2.0 (4) 93.7 \u00b1 2.3 (3)\n94.7 \u00b1 1.5 (2) 98.2 \u00b1 1.0 (1)\nAverage rank 6.7\n5.9\n5.3\n3.7\n2.9\n2.6\n1.0\n\u2217 - trained only on time series, \u2217\u2217 - trained only on the visual modality.\nselecting useful modality mixtures with multiplicative combination proce-\ndure [21].\n\u2013 EmbraceNet5 is a deep learning method that leverages cross-modal corre-\nlations during the training phases [6]. It uses the embracement process to\nprobabilistically select subsets of information from each modality to model\nthe inter-modal correlations.\n\u2013 Visual Transformer (ViT)6 uses the Transformer layer with self-attention\napplied to image patch sequences [9].\n4.2\nExperimental Results\nFor increased reliability, we performed \ufb01ve independent trials using ten-fold\ncross-validation by partitioning each dataset into training (80%), validation\n(10%), and testing (10%) subsets. The outcomes of this comprehensive com-\nparison are detailed in Table 2. For each method, the result of the best hyperpa-\nrameter setting is reported. All results are in terms of accuracy percentage, and\nthe mean and standard deviation of ten repeats are reported.\nThe tested datasets have varied sensor sequence lengths ranging from 276 in\nVisuo-Haptic to 1.5 \u00d7 105 in Mudestreda. We change the depth of the Minape\narchitectures from four with a width of 128 in Visuo-Haptic to 16 with a width\nof 256 in BAUM-1a. We observe that wider networks and successively down-\nsampled convolutional network designs yield better results, even when trained\nfor fewer epochs. Moreover, changing the kernel size from 5 in Mex to 13 in Mud-\nestreda indicates that larger kernels result in better performance. The kernels\nin the depthwise layer de\ufb01ne the size of the receptive \ufb01eld. Thus, large ker-\nnels mix arbitrarily distant spatial locations, allowing them to capture spatial\ndependencies more comprehensively.\nOur experiments indicate that the best results belong to patch embedding\nand isotropic feature extraction architectures. In all tested datasets, Minape\n5 https://github.com/idearibosome/embracenet.\n6 https://github.com/keras-team/keras-io/blob/master/examples/vision/\nvit_small_ds.py.\n\n\nMultimodal Isotropic Neural Architecture with Patch Embedding\n183\nFig. 5. (a) Example of force signals (Fx, Fy, Fz) from tool nr. 1 (T1) for 30 milling\nphases. (b) Comparison of multimodal models on Mudestreda. Various modalities are\nrepresented with i: image, t: time series, and m: multimodal. Minape\u2020 is a model with\nno patches. Minape (m) is 12 MB and SL-ViT (m) is 863 MB.\nachieves signi\ufb01cantly better results compared to other methods of comparison.\nFurthermore, we should note that the corresponding number of subnetworks does\nnot grow exponentially with the number of multimodalities, making the solution\nscalable, even to high-dimensional spaces. Both the patch embedding and the\nlarge kernel size preserve locality well, suggesting that the spatial representation\nis sensitive to the relative embedding dimensions and \ufb01lter size and generates\nequivalent results even without downsampling the representation in subsequent\nlayers. Given that Minape yields better results than Minape-S and the fact that\ntheir di\ufb00erence lies only in the convolutional layer architecture, it can be con-\ncluded that the performance is more related to the kernel dimension and internal\nnetwork resolution than to the depth of the classi\ufb01er.\n4.3\nMudestreda: A Multimodal Device State Recognition Use Case\nIn addition to the existing benchmarks, we study a real-case application of the\nmilling process and propose a new multimodal industrial device state recogni-\ntion dataset called Mudestreda. Mudestreda comprises 512 four-dimensional\nmultimodal observations consisting of three force signal sequences and one RGB\nimage of the shaft milling tool over \ufb01ve weeks from the Production Centrum.\nThe three dimensions of time series modality are forces recorded in three axes,\nFx, Fy, and Fz, with a frequency of 10 kHz. After each milling phase, a picture\nof the tool is taken, and the \ufb02ank wear is measured to assign each observation to\nthe respective class based on the de\ufb01ned metric: class-1 [0, 71)\u03bcm (sharp), class-2\n[71, 110)\u03bcm (used), and class-3 [110, +\u221e)\u03bcm (dulled). Figure 5a shows an exam-\nple of the collected signal, where a strong correlation between the tool wear and\nthe force amplitudes can be observed. It indicates the smallest amplitudes for\nthe sharp tool increase with tool wear.\n\n\n184\nH. Truchan et al.\nTable 3. Performance comparison of various models and modalities (image (i), time-\nseries (t), both modalities (m)) on the Mudestreda dataset in terms of accuracy and area\nunder the curve (AUC). Model\u2020 results belong to a model without patch embedding.\nThe inference time on the test set and the number of model parameters are reported\nas ms/img and #Params, respectively. The base model without a primary learner is\nindicated in bold.\nModel\nModality\nFusion-Type\nms/img\nSize(MB)\n#Params\nAUC\nAccuracy\nViT\ni\n-\n27\n421\n36.3M\n0.89\n87.5 \u00b1 3.1\nt\n-\n28\n422\n36.4M\n0.88\n83.3 \u00b1 2.7\nm\nConcat\n55\n844\n72.8M\n0.98\n93.7 \u00b1 2.3\nSL-ViT\ni\n-\n29\n428\n36.5M\n0.84\n72.9 \u00b1 2.2\nt\n-\n36\n434\n36.4M\n0.97\n92.7 \u00b1 1.8\nm\nConcat\n62\n863\n73.7M\n0.95\n91.6 \u00b1 1.5\nMinape\ni\n-\n5\n6\n0.4M\n0.95\n89.3 \u00b1 1.9\nt\n-\n6\n6\n0.4M\n0.97\n93.4 \u00b1 1.6\nm\nConcat\n11\n12\n0.9M\n0.98\n96.2 \u00b1 1.2\nMinape \u2020\ni\n-\n1,118\n3\n0.2M\n0.83\n63.6 \u00b1 2.6\nt\n-\n1,122\n3\n0.2M\n0.89\n83.5 \u00b1 1.9\nm\nConcat\n1,239\n6\n0.4M\n0.97\n93.7 \u00b1 2.2\nTable 3 provides a detailed analysis of the impact of di\ufb00erent modalities for\nMinape and ViT as the second-best comparison method in Table 2. In addition\nto ViT, we report the results for SL-ViT7, which is a modi\ufb01ed version of the\nViT with Shifted Patch Tokenization (SPT) and Locality Self-Attention (LSA)\naddressing the problem of locality inductive bias and allowing training a small-\nsize datasets [16]. The SL-ViT parameters are set as ViT with \ufb02ags spt = true,\nlsa = true.\nTable 3 shows that considering the combination of various modalities signi\ufb01-\ncantly enhances model accuracy across all examined models. Patches in Minape\ne\ufb00ectively improve the model\u2019s accuracy while reducing inference times. How-\never, transformer models (ViT, SL-ViT), despite their comparable accuracy, have\nlonger inference times and higher memory demands, which limit their applica-\nbility. The Minape model stands out with the highest accuracy, lowest memory\nusage, and shortest latency, making it a top choice for practical applications,\ngiven its balanced performance metrics. Figure 5b illustrates that Minape signif-\nicantly outperforms other models in terms of inference latency and model size\n(MB). Minape exhibits a model parameter size reduction of more than 80 times\ncompared to the multimodal ViT.\nWe scrutinized the sensitivity of Minape on the Mudestreda dataset, as\nshown in Table 4. The results demonstrate that the base model, when utilizing a\nconcatenation fusion strategy, signi\ufb01cantly outperforms other fusion types. The\nkey advantage of concatenation fusion is its simplicity, as it does not introduce\nadditional hyperparameters or necessitate complex computations. This not only\n7 https://github.com/keras-team/keras-io/blob/master/examples/vision/\nvit_small_ds.py.\n\n\nMultimodal Isotropic Neural Architecture with Patch Embedding\n185\nTable 4. The results for sensitivity analysis of Minape on the Mudestreda dataset.\nThe best model is indicated in bold.\nMinape\nFusion-Type\nPrime Learner\nms/img\nSize(MB)\n#Param(M)\nAccuracy\n\u0394(%)\nFusion\nAdd\n\u00d7\n10\n6\n1.3\n87.3 \u00b1 2.8\n\u22128.9\nMultiply\n\u00d7\n12\n6\n1.3\n91.6 \u00b1 1.7\n\u22124.6\nAverage\n\u00d7\n11\n6\n1.3\n87.5 \u00b1 2.5\n\u22128.7\nLearner\nConcat\nGRU\n10\n13\n1.9\n96.3 \u00b1 1.2\n+0.1\nConcat\nLSTM\n10\n16\n2.1\n94.3 \u00b1 1.6\n\u22121.9\nConcat\nTCN\n11\n39\n4.0\n98.2 \u00b1 1.0\n+2\nAttention \u2713\nConcat\n\u00d7\n15\n8\n2.0\n95.1 \u00b1 1.4\n\u22121.1\nAttention \u2713\nConcat\nTCN\n16\n17\n2.8\n94.9 \u00b1 1.8\n\u22121.3\nensures e\ufb03cient memory utilization but also meets the computational require-\nments of the system. Among our evaluated primary learners, which include GRU,\nLSTM, and TCN, the TCN model enhanced the baseline performance by 2%\nwithout any impact on the inference time. Our study also underscores the rel-\nevance of the attention mechanism in this work\ufb02ow. Further experiments show\nthat the attention layer does not improve the model performance8.",
        "page_start": 9,
        "page_end": 13,
        "chunk_ids": []
      },
      {
        "id": "section_5",
        "title": "5\nConclusion",
        "content": "5\nConclusion\nWe proposed Minape, a novel multimodal isotropic neural architecture with\npatch embedding, to improve multimodal learning for time series and image data.\nWe observed that the patch representation and wider networks that successively\ndown-sample the convolutional network size yield better results than deeper\nmodels and even ViT. It uses less memory and has faster inference allowing\nthe use of the model on stand-by devices. The empirical results demonstrate\nthat the modality fusion with the patch embedding yields higher accuracy than\nstate-of-the-art and baseline methods on six multimodal test benchmarks and\nMudestreda, our newly introduced real-case multimodal device state recognition\ndataset.\nReferences\n1. van Amsterdam, B., Kadkhodamohammadi, A., Luengo, I., Stoyanov, D.: Aspnet:\naction segmentation with shared-private representation of multiple data sources.\nIn: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 2384\u20132393 (2023)\n2. Aslam, M.H., Zeeshan, M.O., Pedersoli, M., Koerich, A.L., Bacon, S., Granger, E.:\nPrivileged knowledge distillation for dimensional emotion recognition in the wild.\nIn: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 3337\u20133346 (2023)\n8 Further ablation studies on the impact of hyperparameters can be found at https://\ngithub.com/hubtru/Minape.\n\n\n186\nH. Truchan et al.\n3. Bai, S., Kolter, J.Z., Koltun, V.: An empirical evaluation of generic convolutional\nand recurrent networks for sequence modeling. arXiv:1803.01271 (2018)\n4. Bonner, L.E.R., Buhl, D.D., Kristensen, K., Navarro-Guerrero, N.: Au dataset for\nvisuo-haptic object recognition for robots. arXiv preprint arXiv:2112.13761 (2021)\n5. Chen, S., Guhur, P.L., Schmid, C., Laptev, I.: History aware multimodal trans-\nformer for vision-and-language navigation. Adv. Neural Inform. Process. Syst.\n(NeurIPS) 34, 5834\u20135847 (2021)\n6. Choi, J.H., Lee, J.S.: Embracenet: a robust deep learning architecture for multi-\nmodal classi\ufb01cation. Inform. Fusion 51, 259\u2013270 (2019)\n7. Cicirelli, G., et al.: The ha4m dataset: multi-modal monitoring of an assembly task\nfor human action recognition in manufacturing. Sci. Data 9(1), 745 (2022)\n8. Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning\naugmentation policies from data. arXiv preprint arXiv:1805.09501 (2018)\n9. Dosovitskiy, A., et al.: An image is worth 16x16 words: transformers for image\nrecognition at scale. In: International Conference on Learning Representation\n(ICLR) (2021)\n10. Eroglu Erdem, C., Turan, C., Aydin, Z.: Baum-2: a multilingual audio-visual a\ufb00ec-\ntive face database. Multimed. Tools Appl. 74(18), 7429\u20137459 (2015)\n11. Gashi, S., Min, C., Montanari, A., Santini, S., Kawsar, F.: A multidevice and mul-\ntimodal dataset for human energy expenditure estimation using wearable devices.\nSci. Data 9(1), 537 (2022)\n12. Geng, T., Wang, T., Duan, J., Cong, R., Zheng, F.: Dense-localizing audio-visual\nevents in untrimmed videos: A large-scale benchmark and baseline. In: IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 22942\u2013\n22951 (2023)\n13. Girdhar, R., et al.: Imagebind: one embedding space to bind them all. In:\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\npp. 15180\u201315190 (2023)\n14. Gong, X., et al.: MMG-ego4D: multimodal generalization in egocentric action\nrecognition. In: IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition (CVPR), pp. 6481\u20136491 (2023)\n15. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415 (2016)\n16. Lee, S.H., Lee, S., Song, B.C.: Vision transformer for small-size datasets. arXiv\npreprint arXiv:2112.13492 (2021)\n17. Li, Y., Quan, R., Zhu, L., Yang, Y.: E\ufb03cient multimodal fusion via interactive\nprompting. In: IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pp. 2604\u20132613 (2023)\n18. Lialin, V., Rawls, S., Chan, D., Ghosh, S., Rumshisky, A., Hamza, W.: Scalable and\naccurate self-supervised multimodal representation learning without aligned video\nand text data. In: IEEE/CVF Winter Conference on Applications of Computer\nVision (WACV), pp. 390\u2013400 (2023)\n19. Lin, Y.B., Sung, Y.L., Lei, J., Bansal, M., Bertasius, G.: Vision transformers are\nparameter-e\ufb03cient audio-visual learners. In: IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 2299\u20132309 (2023)\n20. Lin, Y.B., Tseng, H.Y., Lee, H.Y., Lin, Y.Y., Yang, M.H.: Exploring cross-video\nand cross-modality signals for weakly-supervised audio-visual video parsing. Adv.\nNeural Inform. Process. Syst. (NeurIPS) 34, 11449\u201311461 (2021)\n21. Liu, K., Li, Y., Xu, N., Natarajan, P.: Learn to combine modalities in multimodal\ndeep learning. arXiv preprint arXiv:1805.11730 (2018)\n\n\nMultimodal Isotropic Neural Architecture with Patch Embedding\n187\n22. Liu, X., Lu, H., Yuan, J., Li, X.: Cat: causal audio transformer for audio classi\ufb01ca-\ntion. In: IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pp. 1\u20135 (2023)\n23. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International\nConference on Learning Representation (ICLR) (2018)\n24. Ramazanova, M., Escorcia, V., Caba, F., Zhao, C., Ghanem, B.: Owl (observe,\nwatch, listen): Audiovisual temporal context for localizing actions in egocentric\nvideos. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 4879\u20134889 (2023)\n25. Ranganathan, H., Chakraborty, S., Panchanathan, S.: Multimodal emotion recog-\nnition using deep learning architectures. In: IEEE winter conference on Applica-\ntions of Computer Vision (WACV), pp. 1\u20139 (2016)\n26. Ryan, F., Jiang, H., Shukla, A., Rehg, J.M., Ithapu, V.K.: Egocentric auditory\nattention localization in conversations. In: IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 14663\u201314674 (2023)\n27. Senocak, A., Kim, J., Oh, T.H., Li, D., Kweon, I.S.: Event-speci\ufb01c audio-visual\nfusion layers: a simple and new perspective on video understanding. In: IEEE/CVF\nWinter Conference on Applications of Computer Vision (WACV), pp. 2237\u20132247\n(2023)\n28. Wightman, R., Touvron, H., J\u00e9gou, H.: Resnet strikes back: an improved training\nprocedure in timm. arXiv preprint arXiv:2110.00476 (2021)\n29. Wijekoon, A., Wiratunga, N., Cooper, K.: Mex: multi-modal exercises dataset for\nhuman activity recognition. arXiv preprint arXiv:1908.08992 (2019)\n30. Wu, H., et al.: Cvt: introducing convolutions to vision transformers. In: IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 22\u201331\n(2021)\n31. Xiao, Y., Ma, Y., Li, S., Zhou, H., Liao, R., Li, X.: Semanticac: semantics-assisted\nframework for audio classi\ufb01cation. In: IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pp. 1\u20135 (2023)\n32. Xu, R., Feng, R., Zhang, S.X., Hu, D.: Mmcosine: multi-modal cosine loss towards\nbalanced audio-visual \ufb01ne-grained learning. In: IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pp. 1\u20135 (2023)\n33. Xue, Z., Marculescu, R.: Dynamic multimodal fusion. In: IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pp. 2574\u20132583 (2023)\n34. Zhang, X., Tang, X., Zong, L., Liu, X., Mu, J.: Deep multimodal clustering with\ncross reconstruction. In: Paci\ufb01c-Asia Conference on Knowledge Discovery and Data\nMining (PAKDD), pp. 305\u2013317 (2020)\n35. Zhang, Z., et al.: Abaw5 challenge: a facial a\ufb00ect recognition approach utilizing\ntransformer encoder and audiovisual fusion. In: IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pp. 5724\u20135733 (2023)\n36. Zhong, Z., Schneider, D., Voit, M., Stiefelhagen, R., Beyerer, J.: Anticipative fea-\nture fusion transformer for multi-modal action anticipation. In: IEEE/CVF Winter\nConference on Applications of Computer Vision (WACV), pp. 6068\u20136077 (2023)\n37. Zhu, W., Omar, M.: Multiscale audio spectrogram transformer for e\ufb03cient audio\nclassi\ufb01cation. In: IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 1\u20135 (2023)",
        "page_start": 13,
        "page_end": 15,
        "chunk_ids": []
      }
    ],
    "total_pages": 15,
    "processed_at": "2025-11-17T01:51:51.780178",
    "created_at": "2025-11-17T01:51:51.780178"
  },
  "ba4afb9e-74b2-4f72-aa88-39eb6486c8a9": {
    "id": "ba4afb9e-74b2-4f72-aa88-39eb6486c8a9",
    "filename": "Kleinberg+_NBER19.pdf",
    "status": "ready",
    "metadata": {
      "title": "Discrimination In The Age Of Algorithms",
      "authors": [
        "Sendhil Mullainathan"
      ],
      "abstract": null,
      "keywords": [
        "Children",
        "Development Economics",
        "Economics of Education",
        "Health Care",
        "Health Economics",
        "Law and Economics",
        "Labor Studies",
        "Public Economics"
      ],
      "publication_date": null,
      "doi": null
    },
    "sections": [
      {
        "id": "section_0",
        "title": "ABSTRACT",
        "content": "ABSTRACT\nThe law forbids discrimination. But the ambiguity of human decision-making often makes it \nextraordinarily hard for the legal system to know whether anyone has actually discriminated. To \nunderstand how algorithms affect discrimination, we must therefore also understand how they \naffect the problem of detecting discrimination. By one measure, algorithms are fundamentally \nopaque, not just cognitively but even mathematically.  Yet for the task of proving discrimination, \nprocesses involving algorithms can provide crucial forms of transparency that are otherwise \nunavailable.  These benefits do not happen automatically.  But with appropriate requirements in \nplace, the use of algorithms will make it possible to more easily examine and interrogate the \nentire decision process, thereby making it far easier to know whether discrimination has occurred.  \nBy forcing a new level of specificity, the use of algorithms also highlights, and makes \ntransparent, central tradeoffs among competing values. Algorithms are not only a threat to be \nregulated; with the right safeguards in place, they have the potential to be a positive force for \nequity.\nJon Kleinberg\nDepartment of Computer Science \nDepartment of Information Science \nCornell University\nIthaca, NY 14853\nkleinber@cs.cornell.edu\nJens Ludwig\nHarris School of Public Policy \nUniversity of Chicago\n1307 East 60th Street\nChicago, IL 60637\nand NBER\njludwig@uchicago.edu\nSendhil Mullainathan\nBooth School of Business\nUniversity of Chicago\n5807 South Woodlawn Avenue Chicago, \nIL 60637\nand NBER\nSendhil.Mullainathan@chicagobooth.edu\nCass R. Sunstein\nAreeda Hall 225\nHarvard Law School\nCambridge, MA 02138\ncsunstei@law.harvard.edu\n\n\n2 \nI.",
        "page_start": 2,
        "page_end": 3,
        "chunk_ids": []
      },
      {
        "id": "section_1",
        "title": "INTRODUCTION",
        "content": "INTRODUCTION\nThe law forbids discrimination, but it can be exceedingly difficult to find out whether human beings have \ndiscriminated.1 Accused of violating the law, people might well dissemble. Some of the time, they \nthemselves might not even be aware that they have discriminated. Human decisions are frequently \nopaque to outsiders, and they may not be much more transparent to insiders. A defining preoccupation of \ndiscrimination law, to which we shall devote considerable attention, is how to handle the resulting \nproblems of proof.2 Those problems create serious epistemic challenges, and they produce predictable \ndisagreements along ideological lines. \nOur central claim here is that when algorithms are involved, proving discrimination will be easier \u2013 or at \nleast it should be, and can be made to be. The law forbids discrimination by algorithm, and that \nprohibition can be implemented by regulating the process through which algorithms are designed. This \nimplementation could codify the most common approach to building machine learning classification \nalgorithms in practice, and add detailed recordkeeping requirements.  Such an approach would provide \nvaluable transparency about the actual ingredients of decisions and the choices made in building them \u2013 \nand also about tradeoffs among relevant values. \nWe are keenly aware that these propositions are jarring, and that they will require considerable \nelaboration. They ought to jar because in a crucial sense algorithms are not decipherable \u2013 one cannot \ndetermine what an algorithm will do by reading the underlying code. This is more than a cognitive \nlimitation; it is a mathematical impossibility. To know what an algorithm will do, one must run it.3 The \ntask at hand, though, is to take an observed gap, such as differences in hiring rates by gender, and to \ndecide whether the gap should be attributed to discrimination as the law defines it.  Such attributions need \nnot require that we read the code.  Instead, they can be accomplished by examining the data given to the \nalgorithm and probing its outputs, a process that (we will argue) is eminently feasible.  The opacity of the \nalgorithm does not prevent us from scrutinizing its construction or experimenting with its behavior \u2013 two \nactivities that are impossible with humans.4  \nCrucially, these benefits will only be realized if policy changes are adopted, such as the requirement that \nall the components of an algorithm (including the training data) must be stored and made available for \nexamination and experimentation. It is important to see that without the appropriate safeguards, the \nprospects for detecting discrimination in a world of unregulated algorithm design could become even \nmore serious than they currently are.  \n1 For an instructive account in the constitutional context, see David A. Strauss, Discriminatory Intent and the Taming of \nBrown, 56 U. CHI. L. REV.935 (1989). On the general problem, see Audrey Lee, Unconscious Bias Theory in Employment \nDiscrimination Litigation, 40 HARV. C.R.-C.L. L. REV. 483 (2005). \n2 See, e.g., McDonnell Douglas Corp. v. Green, 411 U.S. 792 (1973); Watson v. Fort Worth Bank & Trust, 487 U.S. 977 \n(1988). \n3 See Michael Sipser, Introduction to the Theory of Computation (2012).  For a discussion of some of these issues in a legal \nsetting, see also Deven R. Desai and Joshua A. Kroll, Trust But Verify: A Guide to Algorithms and the Law, 31 HARV. J.L. & \nTECH. 1 (2017). \n4 We employ this dichotomy \u2013 processes where decisions are made by algorithms and ones where decisions are made by \nhumans \u2013 to make the issues clear. In practice, there can be a hybrid, with humans overriding algorithmic judgments with their \nown. These hybrid processes will clearly have the two elements of each process we describe here, with the additional \nobservation that, with proper regulation, we can see the exact instances where humans overrode the algorithm.  \n\n\n3 \n \nOur starting point is the current American legal system that has developed a complex framework for \ndetecting and regulating discriminatory decisions.5 It is increasingly clear that this framework must be \nadapted for regulating the growing number of questions \u2013 involving hiring, credit, admissions, criminal \njustice \u2013 where algorithms are now involved in how public and private institutions decide.6 Algorithms \nprovide new avenues for people to incorporate past discrimination, or to express their biases and thereby \nto exacerbate discrimination. Getting the proper regulatory system in place does not simply limit the \npossibility of discrimination from algorithms; it has the potential to turn algorithms into a powerful \ncounterweight to human discrimination and a positive force for social good of multiple kinds.  \n \nWe aim here to explore the application of discrimination law to a particularly important category of \ndecisions: screening decisions, where a person (or set of people) is chosen from a pool of candidates to \naccomplish a particular goal, as when college students are admitted on the basis of academic potential or \ndefendants are jailed on the basis of flight risk. 7 Algorithms can be used to produce predictions of the \ncandidate\u2019s outcomes, such as future performance after acceptance of a job offer or admission to an \nacademic program. We focus on one kind of machine learning algorithm often applied to such problems, \nwhich uses training data to produce a function that takes inputs (such as the characteristics of an \napplicant) and produces relevant predictions.8 The terminology here can be confusing since there are \nactually two algorithms: one algorithm (the \u2018screener\u2019) that for every potential applicant produces an \nevaluative score (such as an estimate of future performance); and another algorithm (the \u2018trainer\u2019) that \nuses data to produce the screener that best optimizes some objective function.9 The distinction is \nimportant and often overlooked; we shall emphasize it here. \n \nThe existing legal framework for these types of screening decisions is necessarily shaped by practical \nconsiderations involving the typical difficulty of uncovering human motivations. Simply knowing that \nthere is a disparity in hiring outcomes is not enough to determine whether anyone has discriminated.10  \nPerhaps there are genuine differences in average performance across groups.11 A central aspect of the \nlegal challenge is to determine how and why the hiring decisions were made and whether protected \npersonal characteristics, such as race and gender, played a role.12  Deciding whether there has been \ndiscrimination is difficult for one obvious and one less obvious reason. The obvious reason is generic to \nany legal system: people dissemble, obfuscate, and lie. The less obvious reason is that people may not \neven know themselves.  \n \nA large body of research from behavioral science, described below, tells us that people themselves may \nnot know why and how they are choosing \u2013 even (or perhaps especially) when they think that they do.13 \n                                                 \n5 For a clear, recent treatment, see Solon Barocas & Andrew D. Selbst, Big Data's Disparate Impact, 104 CAL. L. REV. 671 \n(2016).  \n6 This point has been made by a large and growing literature in computer science. While the literature is vast, some canonical \npapers include Cynthia Dwork et al., Fairness Through Awareness, PROCEEDINGS OF THE 3RD INNOVATIONS IN THEORETICAL \nCOMPUTER SCIENCE CONFERENCE 214, 214-26 (2012); Solon Barocas & Andrew Selbst, Big Data\u2019s Disparate Impact, 104 \nCALIF. L. REV. 671 (2016) and the curated set of studies assembled at Scholarship, FAIRNESS, ACCOUNTABILITY, AND \nTRANSPARENCY IN MACHINE LEARNING, https://www.fatml.org/resources/relevant-scholarship (last visited Jan. 14, 2019).  \n7 For one example, see Jon Kleinberg et al., Human Decisions and Machine Predictions, 133 Q.J. ECON. 237 (2018). \n8 See id.  \n9 There are clearly other kinds of algorithms and decisions, and they will require an independent analysis.  \n10 See McDonnell Douglas Corp. v. Green, 411 U.S. 792 (1973). \n11 See Lee, supra note; Tristin K. Green, Making Sense of the McDonnell Douglas Framework: Circumstantial Evidence and \nProof of Disparate Treatment under Title VII, 87 CALIF. L. REV. 983 (1999).  \n12 See, e.g.,  Corning Glass Works v. Brennan, 417 US 188 (1974). \n13 See Richard E. Nisbett & Timothy DeCamp Wilson, Telling More Than We Can Know: Verbal Reports on Mental \nProcesses, 84 PSYCHOL. REV. 231 (1977). \n\n\n4 \n \nMany choices happen automatically; the influences of choice can be subconscious; and the rationales we \nproduce are constructed after the fact and on the fly.14 This means that witnesses and defendants may \nhave difficulty accurately answering the core questions at the heart of most discrimination cases: What \nscreening rule was used? And why? Even the most well-intentioned people may possess unconscious or \nimplicit biases against certain groups.15 \n \nIn contrast, a well-regulated process involving algorithms stands out for its transparency and specificity: \nit is not obscured by the same haze of ambiguity that obfuscates human decision-making. Access to the \nalgorithm allows us to ask questions that we cannot meaningfully ask of human beings. For any \ncandidate, we can ask: \u201cHow would the screening rule\u2019s decision have been different if a particular \nfeature (or features) of the applicant were changed?\u201d  We can ask exactly which data were made available \nfor training the algorithm (and which were not), as well as the precise objective function that was \nmaximized during the training. We will show that, as a result, we can attribute any observed disparity in \noutcomes to the different components of the algorithm design or conclude that the disparity is due to \nstructural disadvantages outside of the decision process.  In a nutshell: For the legal system, discovering \n\u201con what basis are they choosing?\u201d and \u201cwhy did they pick those factors?\u201d becomes much more feasible. \n \nIt would be na\u00efve \u2013 even dangerous \u2013 to conflate \u201calgorithmic\u201d with \u201cobjective,\u201d or to think that the use of \nalgorithms will necessarily eliminate discrimination against protected groups.16 The reliance on data does \nnot provide algorithms a presumption of truth; the data they are fed can be biased, perhaps because they \nare rooted in past discrimination (as, for example, where past arrest records are used to predict the \nlikelihood of future crime).17  It would also be naive to imagine that the specificity of algorithmic code \ndoes not leave room for ambiguity elsewhere. Algorithms do not build themselves. The Achilles\u2019 heel of \nall algorithms is the humans who build them and the choices they make about outcomes, candidate \npredictors for the algorithm to consider, and the training sample. A critical element of regulating \nalgorithms is regulating humans.18  Algorithms change the landscape \u2013 they do not eliminate the problem. \n \n \nA. Implications of Our Framework \n \nFive points are central to our analysis of discrimination law in an age of algorithms. First, the challenge of \nregulating discrimination is fundamentally one of attribution. When a screening process produces a \ndisparity for a particular group, to what do we attribute that gap? It could come from the screening rule \nused. Perhaps the screening rule explicitly takes account of gender. Perhaps the chosen objective \u2013 the \noutcome the screening rule aims to optimize \u2013 disadvantages women (or some other protected group). \nPerhaps the disparity comes from the set of inputs made available to the screener. Perhaps the screening \nrule fails to optimize for a given outcome using the inputs. The answers to these questions may or may \nnot be relevant, depending on what the pertinent law considers to be \u201cdiscrimination.\u201d \n \nImportantly, the disparity may not result from any of these problems with the screening rule itself. It \ncould also be the consequence of average differences in the outcome distributions across groups. For \n                                                 \n14 See Timothy Wilson, Strangers to Ourselves (2004). \n15 See Mahzarin Banjali & Anthony Greenwald, Blind Spots (2013). \n16 See Barocas & Selbst, supra note. \n17 See id.; Sandra G. Mayson, Bias In, Bias Out, 128 YALE L.J. (forthcoming 2019), available \nat https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3257004; Sharad Goel et al., The Accuracy, Equity, and Jurisprudence \nof Criminal Risk Assessment (2019), available at https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3306723. \n18 As another example, in some cases, humans may choose to override the algorithm and this re-introduces human ambiguity. It \nis worth noting here that we can see when these overrides arise.  \n\n\n5 \n \nexample, if some groups truly have worse access to K-12 schooling opportunities \u2013 perhaps their schools \nhave lower levels of resources \u2013 then their college application packets may be less strong on average. \nDecomposing the source of disparities in screening decisions can be enormously difficult, but it is critical \nfor determining when legal remedies should be applied, and which ones. \n \nSecond, this decomposition becomes easier once an algorithm is in the decision loop. Now the decisions \nwe examine are far more specific than \u201cwhy was this particular candidate chosen?\u201d  For example, a key \ninput into the training algorithm is a choice of objective \u2013 given the data, the trainer must produce a \nscreening rule that identifies people predicted to do well on some outcome (for example, salespeople \nexpected to have the highest revenues generated). Algorithms are exceedingly sensitive to these choices.19 \nIn searching for discrimination, the legal system may or may not make it important to ask, \u201cWas the \ntraining algorithm given an appropriate outcome to predict?\u201d The ability to ask this question is a luxury: \nInstead of trying to infer why a salesperson was hired, the algorithm's objective function provides us with \nsuch information. \n \nThe luxury of this knowledge unlocks the power of scrutiny: was this a reasonable choice of outcome?20 \nThe same point holds for the other key choices for the trainer. Of course, the ability to obtain the relevant \nknowledge requires (and we argue for) a high degree of transparency. At a minimum, these records and \ndata should be stored for purposes of discovery. Algorithms do not only provide the means to scrutinize \nthe choices we make in building them, they demand that scrutiny: it is with respect to these choices that \nhuman bias can creep into the algorithm. \n \nThird, such scrutiny pays a dividend: if we regulate the human choices well, we might be willing to be \nmore permissive towards how the algorithm uses information about personal attributes in certain ways. \nWhen we ensure human choices are made appropriately, some of the concerns that animate the existing \nlegal framework for human discrimination are rendered moot for the algorithm. Suppose, for example, \nthat college applications require recommendations from high school teachers. Any racial bias by teachers \ncould lead to differences in average letter quality across race groups. Interestingly, in some cases the best \nway to mitigate the discriminatory effects of biased data is to authorize the algorithm to have access to \ninformation about race.21 To see why, note that only an algorithm that sees race can detect that someone \nfrom a given group has better college grades than their letters would suggest, and then adjust predicted \nperformance to address this disparity. Yet much of the time, considerations of factors like race is what \nantidiscrimination law seeks to prevent (though in this setting, the legal result is not entirely clear).22  \n \nFourth, algorithms will force us to make more explicit judgments about underlying principles. If our goals \nare in tension \u2013 as, for example, if admitting more minority students into an elite college would reduce \nfirst-year GPAs because of disparities in K-12 school quality or other structural disadvantages \u2013 the \n                                                 \n19 See Kleinberg et al., supra note.  \n20 We are bracketing, for the moment, the question whether that is legally relevant. \n21 Jon Kleinberg, Jens Ludwig et al., Algorithmic Fairness, 108 AM. ECON. REV. PAPERS & PROCEEDINGS 22 (2018). See also \nTalia Gillis and Jann Spiess, Big Data and Discrimination, U. Chi. L. Rev. (forthcoming 2019), available at \nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=3204674.  \n22 See Loving v. Virginia, 388 U.S. 1 (1967); Miller v. Johnson, 515 U.S. 800, 811 (1995). One possible response to this \nexample is to argue that if the data contain bias, we simply should not use them at all. But in many applications, it is difficult to \nimagine any alternative to data-driven decision-making. In the case of mortgage lending, for instances, absent information \nabout the current income or assets of a borrower, or prior repayment history, on what basis should a lender decide risk?  A \nmiddle-ground approach might be only to use those data that are not so biased, but as argued above, the algorithm has a much \ngreater chance of being able to tell which data elements contain a differential signal for one group relative to another than \nwould any human being. \n\n\n6 \n \nalgorithm precisely quantifies this tradeoff. And we must now articulate a choice. What tradeoff do we \nfind acceptable? We will now be in a position to grapple with such questions quantitatively.23  \n \nOur fifth and final point is that if appropriate regulation can protect against malfeasance in their \ndeployment, then algorithms can become a potentially powerful force for good: they can dramatically \nreduce discrimination of multiple kinds. A variety of research shows that unstructured decision-making is \nexactly the sort of environment in which implicit biases can have their biggest impact.24 Of course it is \ntrue that in building an algorithm, human beings can introduce biases in their choice of objectives and \ndata; importantly, they might use data that are themselves a product of discrimination. But conditional on \ngetting objectives and data right, the algorithm at least removes the human bias of an unstructured \ndecision process. The algorithm, unlike the human being, has no intrinsic preference for discrimination, \nand no ulterior motives.  \n \nAnd this might not even be the source of the most important gains for disadvantaged groups. In many \ncontexts, efficiency improvements alone have large disparate benefits for members of such groups. For \nexample, Kleinberg et al. examine pre-trial release decisions in New York, and find that algorithms better \ndistinguish low-risk from high-risk defendants.25 By prioritizing the highest-risk people to detain, it \nbecomes feasible in principle to jail 42% fewer people with no increase in crime.26 The biggest benefits \nwould accrue to the two groups that currently account for nine of every ten jail inmates: African-\nAmericans and Hispanics.  \n \nWe develop these points at length, beginning with an exploration of discrimination law, the relevance of \nprinciples from behavioral science, and the tensions introduced into this framework by the use of \nalgorithms.27 Our central claim, stated in simple form, is that safeguards against the biases of the people \nwho build algorithms, rather than against algorithms per se, could play a key role in ensuring that \nalgorithms are not being built in a way that discriminates (recognizing the complexity and contested \ncharacter of that term). If we do that, then algorithms go beyond merely being a threat to be regulated; \nthey can also be a positive force for social justice. \n \n \nII. \nThe Law of Discrimination: A Primer \n \nDiscrimination law has long been focused on two different problems. The first is disparate treatment; the \nsecond is disparate impact. The Equal Protection Clause of the Constitution,28 and all civil rights laws, \n                                                 \n23 See infra for details. \n24 For evidence, see Crystal S. Yang, Free at Last? Judicial Discretion and Racial Disparities in Federal Sentencing, 44 J. \nLEGAL STUD. 75 (2015); Alma Cohen & Crystal S. Yang, Judicial Politics and Sentencing Decisions, AM. ECON. J.: ECON. \nPOL\u2019Y (forthcoming 2018). \n25 See Jon Kleinberg et al., Human Decisions and Machine Predictions, 133 Q.J. ECON 237 (2018). \n26 Id.  \n27 We are building on an emerging line of research connected to developments in computer science, including valuable recent \nwork by Barocas and Selbst that seeks to situate algorithms within the framework of discrimination law. See Barocas & Selbst, \nsupra note. \n28 See Vasquez v. Hillery, 474 US 254 (1986). \n\n\n7 \n \nforbid disparate treatment.29 The Equal Protection Clause of the Constitution does not concern itself with \ndisparate impact,30 but some civil rights statutes do.31   \n \nDisparate treatment. The prohibition on disparate treatment reflects a commitment to a kind of \nneutrality.32 For example, public officials are not permitted to favor men over women or white people \nover black people. Civil rights statutes forbid disparate treatment along a variety of specified grounds, \nsuch as race, sex, national origin, religion, and age.33  \n \nIn extreme cases, the existence of disparate treatment is obvious, because a facially discriminatory \npractice or rule can be shown to be in place (\u201cno women may apply\u201d).34 In other cases, no such practice or \nrule can be identified, and for that reason, violations are more difficult to police.35 A plaintiff might claim \nthat a facially neutral practice or requirement (such as a written test for employment) was actually adopted \nin order to favor one group (whites) or to disfavor another (Hispanics).36 To police discrimination, the \nlegal system is required to use what tools it has to discern the motivation of decision-makers.37 To \nparaphrase the Supreme Court, the key question under the Equal Protection Clause is simple: Was the \nrequirement or practice chosen because of, rather than in spite of, its adverse effects on relevant group \nmembers?38 That question might be exceedingly challenging to answer, but the law makes it necessary to \ntry.39 \n \nIt is important to see that the disparate treatment idea applies whether discrimination is taste-based or \nstatistical.40 An employer might discriminate (1) because he himself prefers working with men to working \nwith women; (2) because the firm\u2019s coworkers prefer working with men to working with women; or (3) \n                                                 \n29 For helpful discussion, see Cary Franklin, Inventing the \u201cTraditional Concept\u201d of Sex Discrimination, 125 HARV. L. REV. \n1309 (2012); Elizabeth Bartholet, Application of Title VII to Jobs in High Places, 95 HARV. L. REV. 945 (1982); Miguel \nMendez, Presumptions of Discriminatory Motive in Title VII Disparate Treatment Cases, 32 STAN. L. REV. 1129 (1980). \n30 See Washington v. Davis, 426 US 229 (1976); McCleskey v. Kemp, 481 US 279 (1987). \n31 See, e.g., Griggs v. Duke Power Co., 401 U.S. 424 (1971); Meacham v. Knolls Atomic Power Lab., 554 U.S. 84 (2008). In \nthe context of age discrimination, see Smith v. City of Jackson, 544 U.S. 229 (2005). For discussion, see George Rutherglen, \nDisparate Impact, Discrimination, and the Essentially Contested Concept of Equality, 74 FORDHAM L. REV. 2313 (2006); \nRichard A. Primus, Equal Protection and Disparate Impact: Round Three, 117 HARV. L. REV. 493 (2004); Samuel R. \nBagenstos, The Structural Turn and the Limits of Antidiscrimination Law, 94 CALIF. L. REV. 1, 4 (2006); Michael Selmi, Was \nthe Disparate Impact Theory a Mistake?, 53 UCLA L. REV.. 701, 732\u201345 (2006).  For objections, see Michael Evan Gold, \nGriggs\u2019 Folly: An Essay on the Theory, Problems, and Origin of the Adverse Impact Definition of Employment Discrimination \nand a Recommendation for Reform, 7 INDUS. REL. L.J. 429, 491\u2013500 (1985).  \n32 See Paul Brest, In Defense of the Antidiscrimination Principle, 90 HARV L. REV. 1 (1976). \n33 See, e.g., 42 U.S.C. \u00a7 2000e-2 (\u201cIt shall be an unlawful employment practice for an employer - \n(1) to fail or refuse to hire or to discharge any individual, or otherwise to discriminate against any individual with respect to his \ncompensation, terms, conditions, or privileges of employment, because of such individual\u2019s race, color, religion, sex, or \nnational origin.\u201d). \n34 Reed v. Reed, 404 U.S. 71 (1971). \n35 See Bartholet, supra note; Christine Jolls & Cass R. Sunstein, The Law of Implicit Bias, 94 CALIF. L. REV. 969 (2006). \n36 Washington v. Davis, 426 U.S. 229 (1976). \n37 A defining framework can be found in McDonnell Douglas Corp. v. Green, 411 U.S. 792 (1973). See Tristin K. Green, \nMaking Sense of the McDonnell Douglas Framework: Circumstantial Evidence and Proof of Disparate Treatment under Title \nVII, 87 CAL. L. REV. 983 (1999).  \n38 Personnel Adm'r of Massachusetts v. Feeney, 442 U.S. 256 (1979). \n39 We are bracketing some of the differences between the Equal Protection Clause and the civil rights statutes. On the latter, see \nGreen, supra note. \n40 The classic discussion of taste-based discrimination is Gary Becker, The Economics of Discrimination (1971). On statistical \ndiscrimination, see Edmund Phelps, The Statistical Theory of Racism and Sexism, 62 AM. ECON. REV. 659 (1972).  On the \ndifference, see Jonathan Guryan & Kerwin Kofi Charles, Taste-Based or Statistical Discrimination: The Economics of \nDiscrimination Returns to Its Roots, 123 ECON. J. F417 (2013); Cass R. Sunstein, Why Markets Won\u2019t Stop Discrimination, 8 \nSOC. PHIL. & POL\u2019Y 22 (1991). \n\n\n8 \n \nbecause customers prefer men in the relevant positions. In all of these cases, disparate treatment is strictly \nforbidden.41 Or suppose that an employer is relying on a statistical demonstration that (for example) \nwomen leave the workforce more frequently than men do, or that women over 50 are more likely than \nmen over 50 to leave within ten years. Even if the statistical demonstration is convincing, facial \ndiscrimination against women, or some kind of boost for men or penalty for women, is strictly \nprohibited.42  \n \nAll this is relatively straightforward as a matter of law, but as a matter of principle, it raises some serious \npuzzles, to which we will return.  \n \nDisparate impact. The prohibition on disparate impact means, in brief, that if some requirement or \npractice has a disproportionate adverse effect on members of protected groups (such as women and \nAfrican-Americans), the defendant must show that the requirement or practice is adequately justified.43 \nSuppose, for example, that an employer requires members of its sales force to take some kind of written \nexamination, or that the head of a police department institutes a rule requiring new employees to be able \nto run at a specified speed. If these practices have disproportionate adverse effects on members of \nprotected groups, they will be invalidated unless the employers can show a strong connection to the actual \nrequirements of the job.44 Employers must show that the practices are justified by \u201cbusiness necessity.\u201d45  \n \nThe appropriate justification of the disparate impact standard is widely disputed and raises fundamental \nquestions about the nature of the antidiscrimination principle.46 The standard can be defended in two \ndifferent ways.47  First, it might be seen as a way of ferreting out some kind of illegitimate motive \u2013 and \nmight therefore be essentially equivalent, at the level of basic principle, to the disparate treatment \nstandard. Lacking the tools to uncover bad motives, the legal system might ask: Does the manager have a \nsufficiently neutral justification for adopting a practice that has adverse effects on (say) women? If not, \nwe might suspect that some kind of discriminatory motive is at work. \n \nAn alternative defense of the disparate impact standard would not speak of motivation at all.48 It would \ninsist that if a practice has disproportionate adverse effects on members of traditionally subordinated \n                                                 \n41 David A. Strauss, The Law and Economics of Racial Discrimination in Employment: The Case for Numerical Standards, 79 \nGEO. L.J. 1619 (1991).  \n42 This is a clear implication of Craig v. Boren, 429 US 190 (1976), and JEB v. Alabama ex rel. TB, 511 U.S. 127 (1994). For \nsome complications, see Nguyen v. INS, 533 U.S. 53 (2001). See John J. Donohue III, The Law and Economics of \nAntidiscrimination Law, (Nat\u2019l Bureau of Econ. Research, Working Paper No. 11631, 2006), \nhttps://www.nber.org/papers/w11631. \n43 The defining decision is Griggs v. Duke Power Co., 401 U.S. 424 (1971). For our purposes, the full intricacies of the \ndoctrine do not require elaboration. See sources cited in note supra for discussion.  \n44Griggs v. Duke Power Co., 401 U.S. 424 (1971). The disparate impact standard is now under constitutional scrutiny, but we \nbracket those issues here. See Richard A. Primus, Equal Protection and Disparate Impact: Round Three, 117 HARV. L. REV. \n493, 498 (2003).    \n45 See Christine Jolls, Antidiscrimination and Accommodation, 115 HARV. L. REV. 642 (2001); Jake Elijah Struebing, Note, \nReconsidering Disparate Impact Under Title VII: Business Necessity as Risk Management, 34 YALE L. & POL'Y REV. 499 \n(2016); Linda Lye, Comment, Title VII\u2019s Tangled Tale: The Erosion and Confusion of Disparate Impact and the Business \nNecessity Defense, 19 BERKELEY J. EMP. & LAB. L. 315 (1998).  \n46 See Strauss, supra note; George Rutherglen, Disparate Impact, Discrimination, and the Essentially Contested Concept of \nEquality, 74 FORDHAM L. REV. 2313 (2006). \n47 See Pamela Perry, The Two Faces of Disparate Impact Discrimination, 59 FORDHAM L. REV. 523 (1991).  \nOn the relevant history, see Reva Siegel, The Constitutionalization of Disparate Impact, 106 CALIF. L. REV. (forthcoming \n2019). On the theory, see Jack Balkin & Reva Siegel, The American Civil Rights Tradition, 58 U. MIAMI L. REV. 9 (2003). \n48 See Owen Fiss, Groups and the Equal Protection Clause, 5 PHIL. & PUB. AFF. 107 (1976); Cass R. Sunstein, The Anticaste \nPrinciple, 92 MICH. L. REV. 2410 (1993).  \n\n\n9 \n \ngroups, it should be stuck down, unless it has a strong independent justification.49 On this view, the \nmotivation of the decision-maker is not relevant. What matters is the elimination of social subordination \nof certain groups or something like a caste system.50 The disparate impact standard does not, of course, go \nnearly that far.51 But by requiring a strong justification for practices with discriminatory effects, it tends in \nthat direction.  \n \nFair representation. Some people, of course, go beyond both disparate treatment and disparate impact.  \nThey want members of certain groups to be chosen at socially desirable rates. Call this the principle of \nfair representation.52 The desire for fair representation may derive from numerous sources. For example, \nfair representation might have an instrumental value.53 Having greater numbers of African-Americans in \nthe police force could be important if it improves the functioning of the force, whose relationship to the \nrelevant community might be better if it is not entirely or mostly white.54 There may also be collateral \nbenefits \u2013 including externalities \u2013 to such inclusion. Numerous economic examples fit this condition. \nAfrican-Americans lacking past credit records may not be viable borrowers for lenders, which in turn \nmeans they will not ever be able to build a credit record.  Even if each individual borrower acts fairly in a \nlocal sense, the externality ensures a global unfairness.55   \n \nThe law does not concern itself with fair representation as such, and indeed an effort to pursue that goal, \nthrough race-conscious policies, would itself raise serious legal problems under the Constitution and civil \nrights statutes.56 Race-conscious affirmative action programs remain constitutional, but only under narrow \nconditions.57 Our only claim here is that some people think that fair representation, as such, is a legitimate \nand important goal. \n \nThis discussion thus far should be sufficient to reveal that as a normative matter, the line between \nantidiscrimination principles and affirmative action is less clear than is often thought.58 Suppose that an \nemployer is forbidden to take account of customer tastes (even if he deplores them), or that an employer is \nprohibited from considering statistical reality (even if he deeply wishes it were otherwise). By hypothesis, \nhe is not biased in any way; he is simply trying to succeed in business. Nonetheless, he is required to \nsacrifice his own economic interests for the sake of attaining broader social goals.59 In such cases, there is \na clear tradeoff. The sometimes-blurry theoretical boundary between anti-discrimination law and \naffirmative action policy has not posed much of an issue in a world of human decision-making, since the \nnature of the tradeoffs are hard or even almost impossible to see when so little can be quantified. But this \nchanges when algorithms are introduced, as we discuss below. \n \nIII. \nANTI-DISCRIMINATION LAW FOR HUMANS \n                                                 \n49 See Ruth Colker, Anti-Subordination Above All: Sex, Race, and Equal Protection, 61 N.Y.U. L. REV. 1003 (1986); Owen \nFiss, Groups and the Equal Protection Clause, 5 PHIL. & PUB. AFF. 107 (1976). \n50 Susan Carle, A New Look at the History of Title VII Disparate Impact Doctrine, 63 FLA. L. REV. 251 (2011).  \n51 See Ian Ayres & Peter Siegelman, The Q-Word as Red Herring: Why Disparate Impact Liability Does Not Induce Hiring \nQuotas, 74 TEX. L. REV. 1487 (1996).  \n52 This principle was at the heart of Mobile v. Bolden, 446 U.S. 55 (1980). \n53 See Sounman Hong, How Racial Diversity Makes Police Forces Better, WASHINGTON POST (Dec. 5, 2017), \nhttps://www.washingtonpost.com/news/monkey-cage/wp/2017/12/05/how-racial-diversity-makes-police-forces-\nbetter/?utm_term=.7d41f485c23e. \n54 See Kathleen Sullivan, Sins of Discrimination, 100 HARV. L. REV. 78, 94 (1986). \n55 See Cass R. Sunstein, Why Markets Won\u2019t Stop Discrimination, 8 SOC. PHIL. & POL\u2019Y 22 (1991).  \n56 See Gratz v. Bollinger, 539 U.S. 244 (2003).  \n57 See id. \n58 See Strauss, supra note. \n59 See Sunstein, supra note. \n\n\n10 \n \n \nWe will focus throughout on a single category of decisions. Call them \u201cscreening\u201d decisions. For \nexample, a manager or team of managers must screen candidates and decide whom to hire. At its essence, \ndecisions in this category are those where: \n \n\u2022 We must make a choice about people. Hire or not? Promote or not? Jail or not? Give credit to or \nnot? The decision materially affects people\u2019s lives.  \n\u2022 We must use features of people within the relevant class to make this decision. What is their \neducation? What is their past arrest record? How much do they earn? Have they ever defaulted on \nloans in the past?  \n\u2022 There is some output or outputs the decision-makers say they care about \u2013 job performance, loan \nrepayment, flight risk, public safety.  \n\u2022 There is some uncertainty (at the least from the perspective of a third party or a judge) as to which \ncombination of features best predicts the outcome of interest. This uncertainty is both qualitative \nand quantitative. Does height matter at all for job performance in a physical job? And how much \nexactly does occupation matter for predicting default given that we already have income data?  \n \nIn each of these decisions, managers will in effect, implicitly or explicitly, have a screening rule, simple \nor complex, they are using. We discuss in what follows how the law currently tries to identify and prevent \npeople from engaging in discrimination. But in order to understand how and why we have developed our \ncurrent legal rules regarding discrimination for humans, it is useful to understand first how human \ncognition works and exactly what we are worried about. \n \nA. Human cognition and biases \n \nRemarkably large numbers of Americans admit to discriminatory attitudes when asked. Among white \nrespondents to the General Social Survey (GSS), 28% believe \u201cit\u2019s okay to discriminate when selling a \nhome,\u201d 60 34% say \u201cblacks shouldn\u2019t push themselves where they\u2019re not wanted,\u201d and 45% say \u201cmost \nblacks don\u2019t have the motivation or willpower to pull themselves out of poverty.\u201d61  \n \nBut the problems run far deeper than even these striking self-reports would suggest. This goes beyond the \nobvious problem of dissembling \u2013 we cannot trust that everyone who discriminates will tell us. The key \nlesson of a large body of psychology research tells us that people who discriminate often are not aware of \nit. Much of the time, human cognition is the ultimate \u201cblack box,\u201d even to ourselves.62 \n \n                                                 \n60 See The Views of White Americans, THE NEW YORK TIMES UPSHOT (2014), \nhttps://www.nytimes.com/interactive/2014/04/30/upshot/100000002853274.embedded.html. See also Bobo et al., The Real \nRecord on Racial Attitudes, in SOCIAL TRENDS IN AMERICAN LIFE: FINDS FROM THE GENERAL SOCIAL SURVEY SINCE 1972 38-\n83 (Peter V. Marsden ed., 2012). \n61 This is not specific to American race relations; prejudice against out-groups occurs almost anywhere there are people. For \nexample, data from the World Values Survey show: \n\u2022 \n35% of those in Turkey would not want to be neighbors with someone of a different religion \n\u2022 \n41% of South Koreans say they would not wish to be neighbors with an immigrant \n\u2022 \n66% of Russians say they would not wish to be neighbors with someone who was homosexual \n\u2022 \n77% of respondents in India believe that men make for better political leaders than do women \nWORLD VALUES SURVEY, WVS Wave 6, http://www.worldvaluessurvey.org/WVSDocumentationWV6.jsp?COUNTRY=875 \n(last visited Jan. 16, 2019). \n62 See Wilson, supra note. \n\n\n11 \n \nTo understand this point and its implications for discrimination law, it will be useful to say a few words \nabout psychology in general. Dual-processing theories suggest that human cognition consists of two \nfamilies of cognitive operations: (1) deliberate, conscious thought (what is often called by psychologists \n\u201csystem II thinking\u201d), which is effortful, and (2) rapid, automatic responses, which is not effortful, and of \nwhich people may not even be aware (\u201csystem I thinking\u201d).63 Because conscious thinking requires effort, \nwe tend to rely on our automatic systems to conserve mental energy.64 If someone says \u201ctwo plus two,\u201d a \nnumber automatically comes to mind in response to this frequently-encountered problem. If you read the \nword \u201cbread,\u201d you might also think \u201cbutter.\u201d  \n \nAutomatic responses are not limited to these sorts of small situations; they also extend to behaviors that \nwe would presume are quite mindful, such as how we interact with our social environment. For example, \nLanger, Blank and Chanowitz asked subjects in their study to make some copies.65 Just as they were \nabout to do so, a confederate asked to jump in front of them. In some cases, the confederate gave no \nreason at all (\u201cExcuse me, I have 5 pages. May I use the Xerox machine?\u201d); in others, a reason was given \n(\u201cExcuse me, I have 5 pages. May I use the Xerox machine, because I\u2019m in a rush?\u201d) The third condition \nhad the veneer of a reason, but it actually had no informational content (\u201cExcuse me, I have 5 pages. May \nI use the Xerox machine, because I have to make copies?\u201d) People complied at similar rates to both the \nactual reason and the pseudo-reason (94% and 93%, versus 60% with no reason). What explains this \nfinding? People see a situation they have seen before and their automatic response kicks in, to avoid the \neffort of processing the rest of this setting. Consistent with this view, when the costs of complying go up \u2013 \nwhen the confederate wants to copy 20 pages, not just 5 \u2013 compliance to the pseudo-reason is similar to \nwhat it is in the no-reason condition.  \n \nWe are often unaware of these automatic responses.66 Latane and Darley carried out experiments showing \nthat people are less likely to help a stranger in need when there are relatively more people around.67 They \nthen asked why subjects did not help, \u201cevery way we knew how: subtly, directly, tactfully, bluntly. \nAlways we got the same answer. Subjects persistently claimed that their behavior was not influenced by \nthe other people present. This denial occurred in the face of results showing that the presence of others did \ninhibit helping.\u201d68 Subsequent research has repeatedly confirmed that we often fail to understand why we \ndo what we do.69 The title of Richard Nisbett and Timothy Wilson\u2019s influential essay from decades ago is \nprescient; in describing our own cognition, we are often \u201ctelling more than we can know.\u201d70  \n \nThese implicit cognitions can sometimes even be in direct conflict with our conscious thoughts, including \nwith respect to discrimination.71 The tendency to categorize people and favor \u201cin-groups\u201d and disfavor \n                                                 \n63 The literature is voluminous. For an influential summary and exposition of dual systems models in psychology, see Daniel \nKahneman, Thinking Fast and Slow (2011).Within economics, models of dual systems thinking include Tom Cunningham, \nBiases and Implicit Knowledge (Munich Personal RePEc Archive, Working Paper No. 50292, 2013), and for impulse control, \nsee Drew Fudenberg & David Levine, A Dual-Self Model of Impulse Control, 96 AM. ECON. REV 1449 (2006). \n64 Daniel Kahneman, Attention and Effort (1973), is a classic treatment. \n65 See Ellen Langer et al., The Mindlessness of Ostensibly Thoughtful Action: The Role of \u2018Placebic\u2019 Information in \nInterpersonal Interaction, 36(6) J. OF PERSONALITY & SOC. PSYCHOL. 635 (1978).  \n66 See Geoffrey Cohen, Party Over Policy, 85 J. PERS. SOC. PSYCHOL. 808 (2003). \n67 See Bibb Latane & John M. Darley, Bystander Apathy, 57 AM. SCIENTIST 244 (1969).  \n68 See id. at 124. \n69 See, e.g., Cohen, supra note \n70 See Richard E. Nisbett & Timothy DeCamp Wilson, Telling More Than We Can Know: Verbal Reports on Mental \nProcesses, 84 PSYCHOL. REV. 231 (1977). \n71 On implicit bias and law, see Jolls & Sunstein, supra note. On whether implicit attitudes map onto behavior, see Hal Arkes \n& Philip E. Tetlock, ATTRIBUTIONS OF IMPLICIT PREJUDICE, 15 PSYCHOL. INQUIRY 257 (2004); Philip E. Tetlock & Gregory \nMitchell, Implicit Bias and Accountability Systems, 29 RESEARCH IN ORG. BEHAVIOR 3 (2009). \n\n\n12 \n \n\u201cout-groups\u201d is ubiquitous,72 and indeed the automatic system pays particularly close attention to other \npeople\u2019s sex, age and race.73 But the personal characteristics that distinguish groups do not even need to \nbe so obviously distinctive. Consider the famous experiment by Sherif et al. involving two groups of \nmiddle-school youth at Robber\u2019s Cave State Park in Oklahoma.74 Over the course of the study, the two \ngroups (the Eagles and the Rattlers) exhibited increasingly negative views about the trustworthiness, \nintegrity, and athletic skill of the other group, even culminating in aggression. This out-group hostility \narose even though the two groups were actually formed by random assignment of a quite homogenous \npool of white Protestant boys.75 There were no actual differences across groups that generated the out-\ngroup hostility. All it took was to pit them against each other in a few small competitions. \n \nA major contribution of psychological research has been to show that even implicit biases can be \nmeasured.76 In an early study, Word, Zanna and Cooper asked study subjects \u2013 white Princeton \nundergraduates \u2013 to carry out interviews of white and African-Americans confederates.77 Study subjects \nwere found to sit closer to and spend more time talking with white than African-American interviewees, \nboth measures of how positively inclined one person is toward another. In a follow-up experiment, the \nsubjects now were the ones being interviewed, by a set of confederates randomly assigned to do the \ninterviews either the way whites were interviewed in the first experiment (sitting closer, more time) or \nhow African-Americans were interviewed (further, shorter). Those interviewed as African-Americans had \nbeen in the first experiment were rated by observers in this second experiment as having worse interview \nresponses. That is, the interviewer\u2019s behavior changes the applicant\u2019s responses. Beyond showing that we \ncan measure implicit bias, this study also showed that these biases can create a self-fulfilling prophecy: \npeople are basically creating their own reality.78 \n \n                                                 \n72 Indeed, when meeting someone new these are typically the most likely things to be remembered about the person, and often \nthe only things that are remembered. This automatic encoding may stem from the same basic cognitive processes related to \ncooperation and paying attention to social coalitions. Some evidence to this effect comes studies that create new social \ncoalitions among study subjects in laboratory conditions that are unrelated to race, which seems to substantially reduce mental \nencoding of race. See Leda Cosmides et al., Perceptions of Race, 7 TRENDS IN COGNITIVE SCIENCES 173 (2003). \n73 See id.  \n74 See Muzafer Sherif et al., Intergroup Conflict and Cooperation: The Robbers Cave Experiment (1961).  \n75 Id.  \n76 See Banaji & Greenwald, supra note. \n77 See Carl O. Word et al., The Nonverbal Mediation of Self-Fulfilling Prophecies in Interracial Interaction, 10 J. EXP. SOC. \nPSYCHOL. 109 (1974).  \n78 A great deal of attention has been devoted to other ways of measuring implicit biases, such as the implicit association test, or \nIAT. See Anthony Greenwald et al., Measuring Individual Differences in Implicit Cognition: The Implicit Association Test. 74 \nJ. OF PERSONALITY AND SOCIAL PSYCHOL. 1464 (1998); Marianne Bertrand et al., Implicit Discrimination, 95 AM. ECON. REV. \nPAPERS & PROCEEDINGS 94 (2005). The societal consequences of implicit bias remain difficult to determine, partly because we \nonly have IAT results for convenient (rather than truly representative) samples of respondents. See Arkes & Tetlock, supra \nnote. But implicit bias seems to be prevalent among those who volunteer to take the tests, and some people argue that it is \ncorrelated with actual behavior. For example, counties or metropolitan areas with relatively higher rates of measured implicit \nbias have been shown to have higher rates of police use of force against blacks, and larger black-white disparities in low birth \nweight or preterm birth rates See Eric Hehman et al., Disproportionate Use of Lethal Force in Policing Is Associated With \nRegional Racial Biases of Residents, 9 SOC. PSYCHOL. & PERSONALITY SCI 393 (2018); Jacob Orchard & Joseph Price, County-\nLevel Racial Prejudice and the Black-White Gap in Infant Health Outcomes, 181 SOC. SCI. & MED. 191 (2017). One study \nadministered IATs to managers in a French grocery store chain. They found that the productivity of minority workers was \nlower when they interacted with managers who had IAT scores indicating more bias. See Dylan Glover et al., Discrimination \nas a Self-Fulfilling Prophecy: Evidence from French Grocery Stores, 132 Q.J. OF ECON. 1219 (2017). It turns out managers \nwith higher levels of implicit bias interacted less with minority workers, leading to reduced worker effort. This helped support \nbiased manager stereotypes about minority workers being less productive \u2013 another example of a self-fulfilling prophecy. \n\n\n13 \n \nIn sum, with respect to human cognition we are, as suggested by the title of Timothy Wilson\u2019s 2004 book, \noften \u201cstrangers to ourselves.\u201d79 In important domains, human behavior often looks quite different from \nwhat we think of as the conscious optimization of some clear objective.  \n \nConsider in this light the consequence of either conscious or subconscious bias for screening decisions.80 \nWhen managers make a rank-ordered list of job applicants, those who discriminate by (say) gender will \nput men higher on the list than they deserve to be based on expected productivity. Managers will rank \nsome lower-productivity men above some higher-productivity women because, as in the canonical \nformulation from Gary Becker, they hire a woman over a man only if her productivity advantage is large \nenough to compensate the manager for the disutility he gets from hiring a woman.81 Put differently, the \ndiscriminating manager creates a prioritized list of job applicants for hiring that is in some sense \nreshuffled; it is no longer rank-ordered purely by expected job performance. We now turn to the challenge \nthat the law faces in determining when this type of reshuffling has taken place. \n \nB. The challenge of detecting discrimination \n \nFor decades, the challenge of ferreting out illicit motivations has preoccupied analysts of discrimination \nagainst protected groups.82 Indeed, it may be their central preoccupation. We know that racial disparities \nexist for important outcomes such as income, wealth, credit, schooling, criminal justice, health, health \ncare, and many others.83 One cause of these disparities involves people\u2019s choices and behaviors, including \ndisparate treatment. We know that because people explicitly tell us they discriminate (as in the GSS \nsurvey results above), and also from carefully controlled \u201caudit studies.\u201d For example, Bertrand and \nMullainathan sent fictitious resumes out to a large number of employers in Boston and Chicago that were \nidentical except that half had a white-sounding name and the others had an African-American-sounding \nname.84 Resumes with white names received 50% more call-backs from employers who wanted to carry \nout an interview.85 \n \nBut that evidence for discrimination is statistical. Without more, it cannot establish disparate treatment \nwith respect to any individual decision. This question about what the former can tell us about the latter is \n                                                 \n79 Timothy D. Wilson, Strangers to Ourselves: Discovering the Adaptive Unconscious (2004). \n80 As briefly noted in the text, differential treatment on grounds of race, sex, and other factors can arise even when people are \nnot trying to express personal animosity, but merely looking out for the bottom line. Suppose that an employer favors whites \nover African-Americans, not because he wants to discriminate (he does not), but because his customers like dealing with \nwhites, and he does not want to lose money. Or suppose that a firm relies on a statistical demonstration that on average women \nleave the workforce more frequently than men do. If a hiring manager cannot tell which specific women are more likely to \nleave, then \u2013 if turnover is bad for the bottom line \u2013 a profit-maximizing company might use gender as a proxy for the \npropensity to leave. This is an example of what economists call \u201cstatistical discrimination.\u201d See, e.g., Kenneth Arrow, The \nTheory of Discrimination, in DISCRIMINATION IN LABOR MARKETS (Orley Ashenfelter & Albert Rees eds., 1973); Edmund \nPhelps, The Statistical Theory of Racism and Sexism, 62 AM.ECON. REV. 659 (1972). Statistical discrimination might be good \nfor the firm, but it penalizes women (including those who are at low risk of leaving the firm themselves). Under the Equal \nProtection Clause and civil rights laws, it is generally unlawful to discriminate on the basis of customer preferences or to \nengage in statistical discrimination, even if these are profit-maximizing. See Strauss, supra note. \n81 See Gary S. Becker, The Economics of Discrimination (2nd ed. 1971) (1957).   \n82 Charles R. Lawrence III, The Id, the Ego, and Equal Protection: Reckoning With Unconscious Racism, 39 STAN. L. REV. 317 \n(1987); Strauss, supra note. \n83 See for example Stanford Center on Poverty and Inequality, State of the Union, 2017, \nhttps://inequality.stanford.edu/sites/default/files/Pathways_SOTU_2017.pdf  \n84 See Marianne Bertrand & Sendhil Mullainathan, Are Emily and Greg More Employable Than Lakisha and Jamal? A Field \nExperiment on Labor Market Discrimination, 94 AM. ECON. REV. 991 (2004).  \n85 See id. \n\n\n14 \n \nat the heart of the debate about the defining case of McCleskey vs. Kemp.86 An African-American \ndefendant convicted of killing a white police officer and then sentenced to death challenged his sentence \nby citing statistical evidence that capital punishment was administered at a relatively higher rate in cases \nwith a black offender and a white victim. The Court rejected the challenge and insisted on the need for \nproof of discrimination in the particular case. It objected that the plaintiff\u2019s claim \u201cthat these statistics are \nsufficient proof of discrimination, without regard to the facts of a particular case, would extend to all \ncapital cases in Georgia, at least where the victim was white and the defendant was black.\u201d87 \n \nHere is another way to understand the problem. Consider the fact that African-Americans constitute 15% \nof all college-age young people but that only 6% of all students enrolled at top private universities in the \nUS.88 This disparity seems more than sufficient to conclude there are important barriers somewhere in \nAmerican life. But for the purposes of the law, we must identify a concrete decision that we believe was \naffected by discrimination, so that we can name a defendant. Was it the college admissions committee? Or \nsomeone else more \u201cupstream\u201d in the process whose decisions contributed to disparities in K-12 learning \nopportunities? The frequent inscrutability of human decision-making, combined with the relative rarity of \nexplicit archival evidence (memos, verbal statements) of discriminatory motives, makes violations \ndifficult to police.89  \n \nWithout some kind of formal discrimination (\u201cno women need apply\u201d90) or a \u201csmoking gun\u201d document, \nthe only other direct way to tell whether someone discriminated in a specific case may be to ask them.91 \nEven setting aside the risk they lie, as noted above, they honestly might not even know themselves. Many \nscreening decisions in practice do not involve any sort of formula or guidelines, but instead are largely (or \nentirely) subjective. A manager may tell us they selected applicants for hiring because they seemed like \n\u201cgood workers.\u201d That could mean almost anything. And given the black-box nature of human cognition, \neven cooperative managers may not be able to explain it. Nor may they be able to articulate what \npredictor variables for future productivity were used, or why those were chosen over other candidate \npredictors. And given the possibility of implicit bias, can every manager really say in good faith \u201cI did not \npay attention to gender (or race, etc.) in making my decision\u201d? In the analogy to algorithms raised in the",
        "page_start": 3,
        "page_end": 15,
        "chunk_ids": []
      },
      {
        "id": "section_2",
        "title": "introduction",
        "content": "introduction, we cannot understand the choices that went into the \u201ctrainer\u201d that creates the screening rule \nthe person used. Statistical tests are quite complex.92 \n \nThe black-box nature of the human mind also means that we cannot easily simulate counterfactuals. If \nhiring managers cannot fully understand why they did what they did, how can even a cooperative \nmanager answer a hypothetical about how he would have proceeded if an applicant had been of a different \nrace or gender? And if it is hard to imagine a counterfactual that involves changing a major salient \npersonal characteristic of that kind, what hope do we have for a counter-factual entailing some \nincremental change to a different, less salient qualification like educational attainment or years of work \n                                                 \n86 481 U.S. 279 (1987). \n87 See id. at 296. \n88See Jeremy Ashkenas, et al., Even With Affirmative Action, Blacks and Hispanics Are More Underrepresented at Top \nColleges Than 35 Years Ago, THE NEW YORK TIMES, (Aug. 24 2018), \nhttps://www.nytimes.com/interactive/2017/08/24/us/affirmative-action.html.  \n89 See Carle, supra note. An influential treatment is Charles R. Lawrence III, The Id, the Ego, and Equal Protection: Reckoning \nWith Unconscious Racism, 39 STAN. L. REV. 317 (1987). \n90 Reed v. Reed, 404 U.S. 71 (1971). \n91 Statistical measures may of course be relevant. See Barbara Norris, A Structural Approach to Evaluation of Multiple \nRegression Analysis as Used to Prove Employment Discrimination: The Plaintiff's Answer to Defense Attacks of \"Missing \nFactors\" and \"Pre-Act Discrimination,\" 49 LAW & CONTEMP. PROBLEMS 63,  65.; Kingsley Browne, Statistical Proof of \nDiscrimination, 68 WASH. L. REV. 477 (1993). \n92 See id. \n\n\n15 \n \nexperience? Put differently, not only can we not readily understand the \u201ctrainer\u201d behind the human\u2019s \ndecision, we may also be unable to understand the \u201cscreener\u201d that was actually used. \n \nOf course, courts have developed tools for addressing these challenges, including statistical analysis.93 If \nwe cannot get a meaningful answer from people about what they did, we might at least try to reconstruct \ntheir decision-making by interrogating data, as courts often do.94 But a data-driven investigation faces its \nown challenges.95 For example, it can be difficult for judges to know whether a screening rule treats \nequally productive candidates the same when they cannot really measure output, but only qualifications. \nConsider how difficult defining the \u201coutput\u201d of a job is even for a relatively simple occupation like, say, \nretail cashier. Is output the fraction of days worked where the cash register and recorded sales balance \nout? Or how often the worker shows up on time, not late? Or how often customers come back to the \nstore? Or total sales more generally? \n \nWhat if we focused on qualifications? With the aid of existing tools,96 determining whether applicants \nwith the same qualifications are treated the same may be helpful or even sufficient, but it runs into the \nproblem of which qualifications matter. Attempts to focus on those qualifications that are most predictive \nof output runs into the problem of measuring output. We might be tempted to solve this problem by \ncomparing applicants who are the same on every qualification. But this can often lead to a very long list. \nAnd in many cases there are just not that many people actually hired into a given job by a given firm over \na given time period. If we have, say, 10 people hired into a job over the past 5 years but 20 plausibly \nrelevant qualifications, it becomes impossible to tell if a firm really hired someone from an advantaged \ngroup over a \u201csimilarly qualified\u201d person from a disadvantaged group.  \n \nWith these questions, we do not mean to suggest that the problem of ferreting out discrimination is \ninsuperable. A great deal of illuminating work casts light on that problem and on potential solutions.97 \nThe only point is that when human behavior is involved, it can be extremely challenging to ascertain \nrelevant motivations and hence to see whether disparate treatment was at work. \n \nWe have seen that in the face of a demonstration of disparate impact, statistical evidence alone is not \nenough to conclude there was unlawful discrimination; the defendant is given a chance to provide some \nqualitative evidence of a justifiable, neutral reason for the disparity (such as \u201cbusiness necessity\u201d).98 For \nexample, imagine that the head of a private security firm institutes a rule requiring new employees to be \nable to run at a specified speed. If these practices have disproportionate adverse effects on some group, \nsuch as women, they will be invalidated unless they can show a strong connection to the actual \nrequirements of the job. \n \nNotice this takes us back not only to the difficulty of defining and measuring \u201coutput,\u201d noted above, but \nmore generally puts judges and jurors in the position of having to make potentially difficult judgments \n                                                 \n93 Kevin Tobia, Note, Disparate Statistics, 126 YALE L.J. 2260 (2017); Michael O. Finkelstein, The Judicial Reception of \nMultiple Regression Studies in Race and Sex Discrimination Cases, 80 COLUM. L. REV. 737 (1980). \n94 See Segar v. Smith, 738 F.2d 1249 (D.C. Cir. 1984), cert. denied, 105 S. Ct. 2357 (1985); supra note.  \n95 D. James Greiner, Causal Inference in Civil Rights Litigation, 122 HARV. L. REV. 533 (2008).  \n96 See supra note.  \n97 See Greiner, supra note; Michael O. Finkelstein, The Judicial Reception of Multiple Regression Studies in Race and Sex \nDiscrimination Cases, 80 COLUM. L. REV. 737 (1980); Delores A. Conway & Harry V. Roberts, Regression Analyses in \nEmployment Discrimination Cases, in STATISTICS AND THE LAW 107 (Morris H. DeGroot et al. eds., 1986); David C. Baldus & \nJames W.L. Cole, Statistical Proof of Discrimination (1980).  \n98 On the dynamics, see Barocas & Selbst, supra note; Linda Lye, Commentary, Title VII\u2019s Tangled Tale: The Erosion and \nConfusion of Disparate Impact and the Business Necessity Defense, 19 BERKELEY J. EMP. & LAB. L. 315 (1998); Amy L. Wax, \nDisparate Impact Realism, 53 WM. & MARY L. REV. 621 (2011).  \n\n\n16 \n \nabout the best way to carry out some task that is far from their own expertise. The head of the security \nfirm, for example, argues that the ability to run fast is an important part of the job. He notes that there is \nvariability in the running speed of suspects, and variability in the distance of foot-chases, but he believes \nthat a security guard who could run 1.5 miles in 12 minutes should be able to catch the suspect in \u201cmost\u201d \nchases. It is challenging for judges to decide on the merits whether any of the security firm head\u2019s \nassertions are correct. It is no wonder, in these circumstances, that much of the debate turns on an \ninstitutional question: how aggressively the legal system should scrutinize those assertions.99 \n \nIV. \nALGORITHMS AND HOW THEY WORK \n \nOne way to think about the goal of prediction is to overcome a missing information problem. A firm gets \napplicants for some job. To decide whom to hire, it would like to know each applicant\u2019s productivity if he \nor she were hired. But that information is \u201cmissing,\u201d in the sense that the applicant has not yet worked for \nthe firm; this is the crux of the question in hiring. Our best guess for what will happen in the future often \nis what happened in the recent past. So a natural way to fill in (predict) the missing information about the \nfuture performance of current job applicants is to look at the performance of \u201csimilar\u201d job applicants \nwhom the firm has hired before. The algorithm in this sense is functioning as a data summarizer. \n \nA. Two Algorithms, Not Just One \n \nWe often refer to anything that involves data and the resulting prediction as an \u201calgorithm.\u201d But as we \nhave noted, this misses the fact that there are actually two separate \u201calgorithms\u201d in any screening rule \napplication of the sort that we consider here.  \n \nOne algorithm \u2013 what we call the screener \u2013 simply takes the characteristics of an individual (a job \napplicant, potential borrower, criminal defendant, etc.) and reports back a prediction of that person\u2019s \noutcome. This prediction for the person\u2019s outcome then feeds into a decision (hiring, loan, jail, etc.).  \n \nThe second algorithm \u2013 what we call the trainer \u2013 is the thing that produces the screening algorithm. This \ninvolves (among other things) deciding which past cases to assemble to use in predicting outcomes for \nsome set of current cases, which outcome to predict, and what candidate predictors to consider.  \n \nThe distinction between these two separate algorithms is often under-appreciated but is actually quite \nimportant in practice: \n \n\u2022 People often worry that the algorithm could be doing literally anything, including unforeseen \nthings that introduce bias.100 This concern shows up when, for example, people worry about using \nmachine learning in situations where standard statistical tools like logistic regression have been \nused to inform predictions for years.101 Recognizing there are actually two algorithms helps clarify \nthat conditional on the choices that go into constructing the trainer, the screener cannot do \n\u201cliterally anything\u201d \u2013 it is mechanically the result of whatever human decisions were made for the \ntrainer. \n \n                                                 \n99 See Kevin Tobia, Note, Disparate Statistics, 126 YALE L.J. 2260 (2017). \n100 Some of these concerns are noted and addressed in Goel et al., supra note. \n101 See id. \n\n\n17 \n \n\u2022 The distinction between the screener and trainer is also important for detecting and regulating \ndiscrimination, since the way we would carry out audits and monitoring is different for the two \nalgorithms (as discussed further below). \n \nB. What situations do (and do not) fit this framework \n \nWe have in mind situations where the decision we are examining is made many times, so that the training \nalgorithm has enough cases to learn the relationship between the candidate predictors and the specified \noutcome. This means that algorithms will be better for \u201cmicro-prediction\u201d tasks, such as hiring, than for \n\u201cmacro-prediction\u201d tasks, such as who will win a presidential election.102  \n \nOur framework is relevant to situations where the training algorithm, screening algorithm and training \ndataset are all fixed, stored objects that can be inspected. In cases where the criterion for the decision is \nchanging at a rapid rate \u2013 as in many online learning applications, like Google search or online ad \nplacement \u2013 new training observations flow in at a massive rate, and the training algorithm is constantly \nrunning to produce new screeners. Things move so quickly in these settings that screeners, trainers and \nunderlying training data samples may not even be stored in a way that would suffice to reconstruct a \nsnapshot of the system at every instant. This means we cannot go back and examine what the training \nalgorithm would have done under different choices. A new legal framework, on the statutory side, might \nrequire companies to begin to store the inputs to its algorithms even in settings like this, a point to which \nwe return below. \n \nThe settings for which our framework is currently relevant affect many people\u2019s lives and accounts for a \nlarge share of total economic activity each year, settings such as education, hiring, criminal justice, and \nhealth care. These decisions do not just have important stakes for society; they are also ones where the \nUnited States has had long-standing concerns about the possibility of discrimination.103 \n \nC. How algorithms work  \n \nWe discuss the mechanics of how algorithms work within the context of a concrete example: hiring. The \nbasic steps to constructing a training algorithm involve the following components.104 \n \n\u2022 Collecting a dataset \n\u2022 Specifying a concrete outcome to be predicted in that data set \n\u2022 Deciding which candidate predictors to construct and make available to the algorithm to be \nconsidered for inclusion in the final statistical model \n                                                 \n102 A related reason why machine learning tools are better for \u201cmicro-prediction\u201d than \u201cmacro-prediction\u201d is that these \nalgorithmic procedures are useful only to the extent to which the new data for which we seek to make predictions are generated \nby the same underlying data-generating process that produced the data we use to train the algorithm. If the world is \nfundamentally changing over time, an algorithm built to understand the relationship between X and Y in one state of the world \nmay not carry over to a new state of the world. This assumption of some basic stability to the world is plausible in situations \nwhere, for instance, we use data on yesterday\u2019s credit card applicants to predict default risk of tomorrow\u2019s applicants. But this \ncan break down when we predict low-frequency events like, say, US presidential elections, where the nature of the underlying \npolitical dynamics may be changing and we only have one data point every four years to try to discern those changes. \n103 See, e.g., supra note.  \n104 Textbook treatments of the basic road map for training prediction algorithms can be found in C. Bishop, Pattern \nRecognition and Machine Learning, (2006) and T. Hastie, R. Tibshirani, & J. Friedman, The Elements of  \nStatistical Learning: Data Mining, Inference, and Prediction (2nd edition, 2009). \n \n\n\n18 \n \n\u2022 Building a procedure for finding the best predictor, which uses all the other variables to predict the \noutcome of interest. The result is the screener: we give the screening algorithm a given set of job \napplicant characteristics, it gives back a prediction of the outcome for that applicant \n\u2022 Validating the procedure in a \u201chold out\u201d set \u2013 a data set that was not used to train the algorithm on. \n \nThe first two steps involve critically important human choices that shape what the algorithm does, and \nhence the degree to which it might discriminate. For that reason, we discuss them in much greater detail \nbelow within the context of regulating algorithmic discrimination, and touch on them only briefly here. \n \nThe dataset on past workers can be thought of like a spreadsheet, with each row being someone whom the \nfirm hired in the past. One or more columns would be measures of how each worker performed on the \njob. The remaining columns might capture application data that can serve as candidate predictors. The \nhuman building the algorithm makes decisions about which set of workers to include in the dataset (the \nrows), and which outcomes and candidate predictors (columns) to invest resources into collecting as well. \n \nThe next step is to specify the outcome the algorithm should predict. A human hiring manager might say \nthey are just looking for people who will be \u201cgood workers.\u201d But for the people building a training \nalgorithm, it is necessary to be more specific. Suppose we are hiring new police officers. When we look at \npast officers, we can see that some of them are good at making arrests, while others are good at \ndeveloping rapport with the community. If we specify arrests as the key outcome of interest for the \ntraining algorithm, we will wind up ranking highly those current applicants who would be good at making \narrests in the future and skew the composition of the department towards people like this. So what \noutcome is specified, or how different outcomes are weighted together for a \u201cproductivity index,\u201d matters \ncritically for what the algorithm does and the hiring process that results. \n \nThe third step is deciding which candidate variables to make available to the algorithm to be considered \nfor inclusion in the final statistical model. This might include, for example, the construction of new \nvariables from ones that are in the original dataset (like constructing body mass index using height and \nweight). It also includes decisions about whether the algorithm should not even have the option to \nconsider some candidate predictors in the final model, because for example they suffer from severe \nmeasurement error or are disallowed for legal reasons. \n \nThe fourth step is the procedure that determines which of the candidate predictors that have been collected \nshould be used in predicting the outcome of interest. Given how important human decisions are in the \nchoice of training data and outcome to predict, it is natural to assume that human judgment plays a \ncritically important role here, too. Indeed, in discussing algorithms that might be used, for instance, to \nhelp rank-order police applicants, we find many people ask questions like: Who exactly will be \nconstructing this algorithm? Have they ever been part of the police force? Or lived in a distressed \nneighborhood where the stakes for picking the right police officers are particularly high? How do we \nknow the algorithm\u2019s programmer will not make misguided choices in deciding what factors to use in \npredicting the outcome?  \n \nBut this step \u2013 deciding which candidate predictors to include in the statistical model, given a full set of \ncandidate predictors and a choice of outcome to predict \u2013 is actually the one step in building the training \nalgorithm where the human usually does not play so much of a role, or at least plays a role very different \nfrom the one that many people imagine. A key feature of machine learning is that the data themselves are \nused to determine what predictors to include in the final prediction model.105 If being left-handed is \n                                                 \n105 Id. \n\n\n19 \n \nunrelated to performance as a police officer, in the training dataset of past police who have been hired we \nwould see that left- and right-handed officers would have similar values of the outcome variable on \naverage \u2013 and so the algorithm would choose not to use it as a predictor. If there are instead big \ndifferences in average performance of past officers by, say, college degree receipt, the training algorithm \nwould be more likely to select that as a predictor for the final model.  The algorithm can only select from \namong the candidate predictors made available to it by its human designers, but given the choice of \ntraining data and outcome, it is the underlying relationships between the variables in the training data that \ndetermines which predictors wind up in the screener, and how much weight they get; it is basically just a \nstatistical matter of which variables are most correlated with the outcome.  \n \nThis helps us see that it is not the case the training algorithm could do \u201cliterally anything\u201d at this step. \nAnother way to think about what the algorithm is doing at this step is to use the candidate predictors (job \napplication information) to group past workers together based on who has similar job performance. We \nthen essentially predict the future performance of current applicants as being something like the average \nperformance of past workers in the same group.  \n \nThis type of \u201cgrouping\u201d is at the heart of prediction, whether that prediction is done by a human being or \nby an algorithm. The whole idea of prediction is sometimes viewed as objectionable because it has the \nflavor of profiling. But if we truly believed that every job applicant was fundamentally different from \neveryone else, then there is literally nothing we could say about how they would perform on the job \nunless we actually hired them and observed what happens. If we really thought people were so \nidiosyncratic that we could never learn anything about them from observing what happened with anyone \nelse, there would be no way to develop a \u201cJoseph Michael Smith, DOB 1/1/1980\u201d predictor \u2013 because \nthere is only one such person. If we gave up on the prediction, on what basis would hiring be done at all?  \nMoreover, as have noted above, the designers of the algorithm have the ability to specify, and thereby \nrestrict, the set of candidate predictors available for the procedure to consider. The alternative to machine \nprediction is some combination of much-less-accurate human prediction, and other subjective human \ndecision-making of the sort that is fraught with risk of bias. \n \nA final question we might have is: How do we even know that these algorithms actually work? This is \nwhy the last step in the process is so critically important \u2013 we must validate the algorithm\u2019s performance \nin a new dataset that the algorithm has not seen yet during the algorithm training process. Consider the \nNetflix Challenge, where research teams competed to develop the most accurate possible prediction \nmodel in response to a publicly available dataset.106 The only reason the Netflix prize competition worked \nwas that the company made one dataset available to research teams to develop their prediction models on, \nbut then evaluated their performance on a separate dataset that was not made public. Imagine Netflix had \nmade the hold-out set public. We would be worried that if the competing teams had access to the hold-out \nset, they would simply try a vast number of candidate algorithms until they stumbled across one that \nhappened to fit the idiosyncrasies of that particular dataset well, rather than uncover a generalizable way \nin which nature works that would show up consistently across different datasets. In many applications we \ndo not have the same level of control over the hold-out set, and so have no choice but to trust the \nalgorithm builder not to have looked at the hold out set during the algorithm construction stage. \n \nThis description of how the training algorithm works also makes clear why the choice of screening \nalgorithm is in some sense the inevitable byproduct of statistical principles once the choice of outcome, \ncandidate predictors, and training dataset have been made. The real fear with algorithms is not what \n                                                 \n106 For an outline, see RM Bell and Y. Koren, Lessons from the Netflix Prize Challenge,  9 SIGKDD EXPLORATIONS 95 \n(2007). \n\n\n20 \n \nhappens inside the algorithmic \u201cblack box.\u201d The training algorithm is in some sense the opposite of a \nblack box; the algorithm summarizes the data according to the data given to it and the outcome that is \nspecified. It is those human choices where the potential for problems, including discrimination, really \narise. \n \nD. What the algorithm cannot do \n \nThe training algorithm is better at prediction than it is at causal inference.107 It is designed to identify a \ncollection of predictors that help predict the outcome as accurately as possible among a new set of \nobservations. Suppose there are multiple candidate predictors in a dataset that are highly correlated with \none another \u2013 for example, given the high levels of racial segregation that persist in the US, race might be \nhighly correlated with both neighborhood of residence and high school attended. An algorithm will pick \nwhatever subset of them happen to be most useful for prediction purposes, but may not include all of the \ncorrelated variables. One implication is that if we see a predictor included in the model, we cannot tell \nwhether it is that predictor or some other correlated predictor that is actually causally affecting the \noutcome.108 Another implication is that if we see an algorithm that does not include a protected personal \nattribute like race in the final model, that does not mean that a correlated proxy for race is not playing a \nrole. It is worth underlining this point: An algorithm that is formally blind to race or sex might be using a \ncorrelated proxy. Whether that is a problem, for legal purposes, depends on the governing legal standard. \nIt is more obviously relevant to a disparate impact claim than to a disparate treatment claim.  \n \nThere is a different way in which the algorithm is better for prediction than causation. Suppose we are \ntrying to predict performance on the job at some firm that has created a hostile workplace environment for \nwomen. Because of that hostile environment, we may see lower levels of productivity on average for \nwomen the firm has hired in the past compared to men.109 Changing the firm\u2019s workplace to make it more \naccepting and accommodating for women may well improve the average productivity we see among \nfuture female hires,110 but the algorithm cannot speak to this possibility. The algorithm is a tool designed \nfor prediction (how do women do at this type of workplace?), not causal inference (what happens if we \nchange the work environment?). \n \nAnother important limitation is that we can train the algorithm only using observations for which we \nobserve the outcome. We cannot observe productivity on the job at some firm for, say, women if women \nnever apply to that place to work (as they might not if the firm has a hostile workplace). We also cannot \nobserve productivity on the job for women at some firm if the firm never hires women. This limitation \nimplies that past choices have a powerful impact on prediction, by determining for whom we have \noutcome values. We return to this point in the next section when discussing the important role played by \nthe human choice of training data to give to the algorithm. \n \nRecall that the training algorithm can only optimize whatever outcome, candidate predictors and training \ndata are given to it. The flip side is that this is all the algorithm does. It has no ulterior motives or hidden \nagenda. And much of what it does is transparent to us. \n \n                                                 \n107 See Jon Kleinberg et al., Prediction Policy Problems, 105 AM. ECON. REV. 491 (2015). \n108 Id. \n109 For relevant evidence and discussion, see Amna Anjun et al., An Empirical Study Analyzing Job Productivity in Toxic \nWorkplace Environments, 15 INT. J. ENVIRON. RES. PUBLIC HEALTH 1035 (2018). \n110 Id. \n\n\n21 \n \nThat is unlike with the human being. Eye-tracking studies of how HR personnel screen resumes tell us \nthat the average hiring manager spends an average of merely six seconds looking at each resume, with \n80% of the time spent on just six items.111 How people decide which factors to look at, or how to weight \nthem together, remains largely a mystery.  \n \n \nV. \nWHERE DISCRIMINATION CAN (AND CANNOT) ARISE WITH THE \nALGORITHM \n \nOne concern with algorithms is that they may seem technocratic and dispassionate in a way that creates \nthe veneer of unimpeachable objectivity. But as we have noted in the previous section, given the \nimportant role that human decisions play in the construction of algorithms, it would be irresponsible \u2013 \neven dangerous \u2013 to confuse \u201cdata-driven\u201d with \u201cnondiscriminatory,\u201d \u201cunbiased,\u201d or \u201cobjective.\u201d112 In \nwhat follows, we outline how discrimination in the human choices about outcomes, candidate predictors, \nand training procedures can infect the resulting algorithm; as we demonstrate in the Appendix to the \npaper, the overall level of discrimination can be fully decomposed into contributions from these three \ncomponents. We also discuss ways in which we might worry that algorithms might be discriminating, but \nare not really; these sorts of misconceptions can lead to restrictions that are well-intentioned but \nultimately counter-productive. \n \nIt may be useful, by way of preface, to distinguish among four kinds of problems. First, the algorithm \nmight be engaging in disparate treatment \u2013 as, for example, if it considers race or gender and \ndisadvantaged protected groups (perhaps because racial or gender characteristics turned out to be relevant \nto the prediction problem it is attempting to solve). Second, the algorithm might be producing a disparate \nimpact \u2013 as, for example, if it considers some factor whose usage disproportionately burdens members of \nprotected groups (say, certain kinds of test scores). Third, the algorithm might be considering some factor \nthat is itself a product of past discrimination (as might be true, for example, of credit scores or arrest \nrecords). The law does not concern itself with this problem except insofar as it reflects disparate treatment \nor can make out a case of disparate impact in the relevant domain, but it might nonetheless be counted as \na problem. Fourth, the algorithm might produce an imbalance of a kind that many people find \nobjectionable. The law is not concerned with this problem, at least not by itself,113 but many people would \nbe concerned to see that an algorithm ends up giving (for example) some benefit to women far less than to \nmen. \n \nA. Where discrimination can arise with algorithms \n \n1. Choice of outcome \n \nWe noted above that what outcome to predict, or how to weight together different outcomes, is one of the \ncritical choices that must be made in building the training algorithm.  The choice of outcome is non-trivial \neven in the simplest of examples. Consider for example a data-entry firm, where employee output seems \neasily quantified through the amount of data entered. Even here, a firm might choose to measure output \nthrough number of hours at work \u2013 arguing that an unused computer is a wasted resource. Such a choice is \n                                                 \n111The six items are: Name; current title and company; previous title and company; previous position start and end dates; \ncurrent position start and end dates; and education. See THE LADDERS, Keeping an Eye on Recruiter Behavior (2012), \nhttps://cdn.theladders.net/static/images/basicSite/pdfs/TheLadders-EyeTracking-StudyC2.pdf. \n112 See Goel et al., supra note. \n113 Mobile v. Bolden, 446 U.S. 55 (1980). \n\n\n22 \n \nnon-trivial since home-life differences may lead to gender differences in hours at work.114 Gender \ndifferences that are bigger for some outcomes than others open the door to intentional bias by the \nalgorithm constructor. It is possible that the case is one of disparate treatment. It might instead involve \ndisparate impact. \n \nDisparate impact could easily be introduced inadvertently. Perhaps the most important way in which this \ncould arise is if the training algorithm is asked to predict a human judgment, rather than an external \nmeasure of the underlying outcome of interest itself. Suppose for instance that instead of predicting how \nproductive a worker is on the job in the future, an algorithm is asked to make screening decisions based \non the people who were hired in the past. Any human bias in past hiring decisions will then be baked into \nthe algorithm itself.  This is a form of biased data that the algorithm builder may or may not be aware of, \nbut is quite pervasive \u2013 arising, for example, with manager assessments of worker performance, or with \ncustomer ratings of workers. Use of such data could well give rise to disparate impact and, on certain \nassumptions, could constitute disparate treatment as well. \n \n2. Choice of predictors to collect, construct, and give to the trainer to consider \n \nBoth disparate treatment and disparate impact can also be introduced through decisions about what \ncandidate predictors to collect, construct, and give to the training algorithm to consider for possible \ninclusion in the final statistical model. For example, an employer that is screening applicant resumes \nmight believe that college quality could matter in predicting worker performance. It might then invest \ntime to assemble US News rankings of four-year colleges (which whites attend at relatively higher rates) \nwithout investing the same effort in measuring rankings of two-year colleges or for-profit universities \n(which black students attend at relatively higher rates).115  \n \nThe problem can arise not just when the firm collects information that is more favorable to one group than \nanother, but also when the firm simply collects too little information overall. For instance, imagine that a \nfirm collects information about where applicants attended high school, but does not collect any \ninformation about how someone did in high school. High schools continue to be highly segregated on the \nbasis of race in the United States, and minority students continue to attend high schools of lower average \nquality than those attended by whites.116 If academic performance is positively related to performance on \nthe job, an algorithm trained using just data on where someone went to high school might have fewer \nminorities ranked highly on the hiring list than would one that also collected information about individual \nstudent performance. Additional information on academic outcomes allows the algorithm to identify those \nstudents who manage to do well in high school despite having attended a disadvantaged, under-resourced \nschool. \n \nThe human algorithm builder might also choose to collect candidate predictors that are human judgments, \nrather than objective measures of reality, and these can lead to difficulties in much the same way that \nusing human judgments as outcome variables can cause problems. Consider the problem of predicting \nwho will do well in college, as a way to inform admissions decisions. Many, or even most, selective \nschools rely on teacher recommendation letters as part of the admissions process. But previous research \n                                                 \n114 As another example, among financial analysts previous research suggests men on average may have slightly more accurate \nearnings forecasts than women, but women have better overall professional reputations. See Clifton Green et al., Gender and \nJob Performance: Evidence from Wall Street, 65 FIN. ANALYSTS J. 6 (2009).  \n115 Almanac 2018, Racial and Ethnic Distribution of Students Among Higher Education Sectors, Fall 2016, THE CHRONICLE OF \nHIGHER EDUCATION, (Aug. 19, 2018), https://www.chronicle.com/article/Distribution-of-Students-Among/244069. \n116 See Linda Darling-Hammond, Unequal Opportunity: Race and Education, BROOKINGS, (March 1, 2018), \nhttps://www.brookings.edu/articles/unequal-opportunity-race-and-education/. \n\n\n23 \n \nsuggests that teachers have biases that run along both gender and race lines.117 Using teacher \nrecommendations to predict student performance may bake teacher bias into the university\u2019s prediction of \nthe college performance of applicants.  \n \n3. Choice of training procedure \n \nFinally, once the designers of an algorithm have chosen an outcome measure and a set of input variables, \nthey need to run a training procedure based on past data in order to produce the screener.  Discrimination \ncan easily be introduced in this phase, deliberately or unintentionally.  Perhaps most simply, the designers \ncould under-optimize in the training phase and produce a screener that helps some groups and hurts others \nwhile only poorly approximating the intended outcome measure.  Such under-optimization, which could \namount to disparate treatment, should be detectable by a separate party auditing the firm\u2019s procedures if \nthey are provided with the choices of outcome, input variables, and training data, since they could then \nrun their own version of the training procedure and see if they obtain a significantly more accurate \napproximation to the intended outcome measure. \n \nThe more subtle, complex routes for discrimination in the training procedure tend to arise through the \nchoice of the training dataset used to produce the screening.  At the most basic level, humans may in \nprinciple deliberately introduce discrimination into the algorithm through their construction of the training \ndata. Suppose a firm wanted to establish a \u201cbad track record\u201d for minority workers to avoid hiring them in \nthe future. It could intentionally select the least productive minority workers from its applicant pool for \nsome period of time.118 Using the data on how these workers performed on the job to train an algorithm \nwould then predict a disparity in worker productivity even if minority and white applicants in the larger \npopulation are equally productive.  \n \nBeyond these relatively extreme examples, there are many ways in which the choice of training sample \ncould also inadvertently produce some kind of problem, possibly in the form of disparate impact. Suppose \nthat we are trying to predict college success to inform admission decisions. We use as training data a set \nof records from a university that has a campus climate hostile to female students. This would depress \nobserved academic performance of women and hence lead to systematic disparities in predicted \nperformance of future college applicants from these groups.119 \n \nA major problem that occurs repeatedly, and is often overlooked, is that we can only predict outcomes \nusing training observations about those for whom we observe the outcome. Suppose we have a firm to \nwhich women simply do not apply (perhaps because it has a hostile workplace environment toward them), \nor the firm just does not hire many women (for one reason or another). This means we sometimes have to \nrely on very small samples to estimate predictions for some groups, and will predict outcomes for the \nmajority group more accurately. \n \n                                                 \n117 See Thomas S. Dee, Teachers, Race and Student Achievement in a Randomized Experiment, 86 THE REV. OF ECON. & STAT. \n195 (2004); John J. Donohue III, The Law and Economics of Antidiscrimination Law, (Nat\u2019l Bureau of Econ. Research, \nWorking Paper No. 11631, 2006); Victor Lavy & Edith Sand, On The Origins of Gender Human Capital Gaps: Short and \nLong Term Consequences Of Teachers\u2019 Stereotypical Biases (Nat\u2019l Bureau of Econ. Research, Working Paper No. 20909, \n2015). \n118 See Dwork et al., supra note. \n119 Of course, if we had a training sample from multiple college campuses and collected some information about how minority \nor female students rated their experience at different places, the algorithm would have an opportunity to learn that \ndisadvantaged groups perform less well at universities that have a hostile campus climate against them. \n\n\n24 \n \nA subtler problem arises from the fact that our datasets for applications like hiring are filtered through \npast decisions (who applies, who gets hired), and the humans who make those decisions often have access \nto information that is not captured in any dataset (for example, what happens during an in-person \ninterview). Suppose we have a man in charge of hiring for a firm who feels threatened by women who are \n\u201ctoo competent.\u201d As a result, he hires women only if they had below-average interviews. If interview \nperformance is correlated with performance on the job, an algorithm trained using data just on those \nwomen who get hired will understate the performance of women relative to men for a given set of \nobservable applicant characteristics.  \n \nThis critically important problem \u2013 what we called the selective labels problem \u2013 has received relatively \nlittle systematic investigation in both the scientific literature and practical applications of algorithms to \nsocial science or policy settings.120 The solution to this problem is likely to require combining insights \nfrom computer science with the sort of \u201cnatural experiment\u201d research designs developed in social \nscience.121 \n \nNote that if we took any of the training datasets described above and asked humans to learn about the \nrelationship between the outcome and the candidate predictors, they would likely form impressions that \nare mistaken in the same way that the algorithm would. Put differently, the discrimination that gets \nintroduced into the training algorithm from biased training samples is not indicative of a problem with \nwhat the algorithm is doing, but rather with the skewed data itself.  \n \nB. Where discrimination is unlikely to originate with algorithms \n \n \nHaving now seen the three fundamental ways in which discrimination can arise in the design of the \nalgorithm, we return to the converse point (developed more formally in the Appendix): any discrimination \nin the algorithmic screening can be fully decomposed into contributions from these three sources.  This \nmeans, in particular, that a number of other dimensions of the algorithm\u2019s behavior do not represent \navenues for potential discrimination to originate.  It is useful to list some of these explicitly, as they form \nthe basis for common misconceptions about algorithmic bias. \n \n1. Discriminatory selection of what candidate predictors to include in prediction function \n \nObservers often worry that literally anything could happen within the machine learning black box, which \ncould in turn lead to discrimination.122 This concern can be seen, for instance, in objections to the use of \nmachine learning algorithms for prediction tasks that have long relied on standard statistical tools like \nlogistic regression.123 But if the algorithm designer has fixed the outcome, the set of available input \nvariables, and the training procedure, then standard machine learning algorithms will select the subset of \ninput variables that optimize the resulting estimate of the outcome measure.  There should be a genuine \nconcern whether the designer may have omitted a crucial variable from the available input that the \ntraining algorithm considers for inclusion in the screener, or included variables whose use constitutes \n                                                 \n120 See Kleinberg et al., supra 7. \n121 For an excellent overview of the natural experiment style of data analysis, see, e.g., Joshua Angrist & J\u00f6rn-Steffen Pischke, \nMostly Harmless Econometrics: An Empiricist\u2019s Companion (2009). For an example of how this type of research design can be \ncombined with machine learning predictions to overcome the selective labels problem, see Kleinberg et al., supra 7.  \n122 See, e.g., Yavar Bathaee, The Artificial Intelligence Black Box and The Failure of Intent and Causation, 31 HARV. J. L. \nTECH. 889, 920 (2018). \n123 For relevant discussion, see Michael L. Rich, Machine Learning, Automated Suspicion Algorithms, and the Fourth \nAmendment, 164 U. PA. L. REV. 871 (2016); David Lehr and Paul Ohm, Playing with the Data: What Legal Scholars Should \nLearn About Machine Learning, 51 U. C. DAVIS L. REV. 653 (2017). \n\n\n25 \n \ndiscrimination as described above, but less of a concern that the training algorithm will include the wrong \nvariables from the set of available options when it produces the final screener.  Not coincidentally, this \nstep of choosing the optimal subset from among the available input variables is a part of the process that \nis data-driven, rather than closely programmed by a human. \n \nOf course, the algorithm designer could always choose to introduce discrimination manually at this stage \nby deviating from the standard pipeline for building the screener. Nothing that we have said rules out the \npossibility that a discriminatory algorithm builder might try to hide something deep in the code that \nintentionally reduces the predicted level of future productivity for every female job applicant. Or, as noted \nabove, the algorithm builder might under-invest in the machine learning engineering that is required to \nderive the most accurate prediction function possible with a given dataset. The fact that machine learning \nrequires skill and effort to optimize is what gives rise to competitions like the Netflix Challenge described \nabove.124 \n \nBut if the outcome, the input variables, and the training data are made available, there is a natural \napproach for detecting this type of deliberate discrimination, given that the optimization of the training \nalgorithm is a data-driven process that should be determined by the underlying statistical relationship in \nthe data between the outcome and the candidate predictors.  Specifically, third parties could run their own \ntraining procedure from the same starting point and determine whether they are able significantly to \nimprove on the firm\u2019s predictions. Malfeasance here is relatively feasible to detect, and hence it should be \ncorrespondingly feasible to deter. \n \n2. The screener algorithm \n \nRecall that the design of a prediction algorithm for a screening problem involves two algorithms: the \ntrainer and the screener.  Our discussion thus far has been about the trainer, and the multiple ways in \nwhich discrimination can be introduced through the choice of outcome, input variables, and training \nprocedure.  Once we have accounted for all these components, the screener itself has essentially been \ndetermined: we should think of it as the mechanical result of applying a training procedure to an outcome, \na set of input variables, and a set of training data. By itself, it cannot add more bias than what has \nappeared in the construction procedure that led to it, via the routes discussed above.   \n \nWith this in mind, consider what disparate treatment looks like. For hiring, an HR manager might do \nsomething like rank-order a less-productive white applicant over a more-productive minority applicant. \nBut with a training and a screening algorithm, if we have accounted for the possible sources of \ndiscrimination in the human\u2019s choices for the training algorithm (outcome, candidate predictors, and \ntraining procedure), then there is essentially no room for the screening algorithm to discriminate beyond \nwhat has already been accounted for in its construction. \n \nSo much, then, for disparate treatment by the screening algorithm. In the world of algorithms, that form of \ndiscrimination should be detectable as a deviation from the expressed specification for the algorithm\u2019s \nbehavior, so long as we have the ability to audit the training algorithm\u2019s design and interact with the \nscreening algorithm itself.  \n \nMoreover, if we have access to the screening algorithm, then we need never be surprised by its decisions.  \nWe can simulate screening decisions for individual cases or whole populations in a way that would be \n                                                 \n124 See Andrey Feuerverger, Statistical Significance of the Netflix Challenge, 27 STATISTICAL SCIENCE 202 (2012); \nBell & Koren, supra note.  \n\n\n26 \n \nimpossible to imagine in scenarios where a human was performing the screening.  For example, we can \nask about average rates of acceptance by the screener over subsets of the population.  We can also \nsimulate counterfactuals. Given access to the operation of the screener, it is trivial to ask \u201cWith this \nscreening rule, would this candidate have been hired if characteristic X had changed?\u201d (For example, if \ncertain applicants had a high school diploma instead of being high school dropouts, would their predicted \noutcome change enough to move them up enough in a firm\u2019s rank-ordered hiring priority list to have been \noffered a job?) \n \nIn all of these respects, screening algorithms are altogether different from human screeners. Far from \nbeing a \u201cblack box,\u201d they are far more transparent than humans.  \n \n3. Group differences in the raw data and predicted outcomes  \n \nConcerns about biased data, potentially making out either disparate treatment or disparate impact, are very \nreal, as we have discussed in detail above. But systematic differences across groups in the data \u2013 that is, in \nthe distributions of the candidate predictors, or in the outcome variable to be predicted \u2013 are not by \nthemselves proof that use of the data is discriminatory in any legally relevant sense.125 \n \nWe live in a world in which reality itself systematically differs across groups. For example, we know that \nwomen shoulder greater child-care burdens than do men.126 When we see in some dataset that women \nearn less than men, it is not necessarily the case that the data are biased in the sense that they \nsystematically mismeasure reality for some groups. The underlying reality itself may be a product of \ndiscrimination of some kind. Or consider a dataset that shows differences in average arrest rates or test \nscores for minorities and whites.127 We know that the poverty rate in the US is twice as high for blacks \nthan for whites;128 among the poor, blacks are twice as likely as whites to live in high-poverty (>40%) \nneighborhoods.129 The average net worth of black households is only about one-seventh that of whites.130 \nGiven the important role that family and community circumstances play in shaping people\u2019s opportunities \ngrowing up, it would be remarkable if these large differences in socio-economic circumstances did not \nlead to average differences in children\u2019s outcomes across groups. If the underlying predictors and/or \noutcomes differ systematically across groups, any well-calibrated algorithm will yield group differences \nin predictions. \n \nIt is possible that we might wish to adjust our algorithmic predictions or subsequent decisions in ways that \nhelp address the underlying social problem.  This is not the task of eliminating bias that has been \n                                                 \n125 For a useful application, see David Williams & Selina A. Mohammed, Discrimination and Racial Disparities in Health: \nEvidence and Needed Research, 32 J. BEHAV. MED. 20 (2009). \n126 See Henrik Kleven et al., Children and Gender Inequality: Evidence from Denmark, (Nat\u2019l Bureau of Econ. Research, \nWorking Paper No. 24219, 2018). \n127 See, e.g., Lauren Nichol Gase et al., Understanding Racial and Ethnic Disparities in Arrest: The Role of Individual, Home, \nSchool, and Community Characteristics, 8 RACE SOC. PROB. (2016); Christopher Jencks and Meredith Phillips, The Black-\nWhite Test Score Gap: An Introduction (2013), available at https://www.brookings.edu/wp-\ncontent/uploads/2013/01/9780815746096_chapter1.pdf.   \n128 See Pew Research Center, On Views of Race and Inequality, Blacks and Whites are Worlds Apart, PEW RESEARCH CENTER: \nSOCIAL AND DEMOGRAPHIC TRENDS (Jun. 27, 2016), http://www.pewsocialtrends.org/2016/06/27/1-demographic-trends-and-\neconomic-well-being/. \n129 See Elizabeth Kneebone & Natalie Holmes, U.S. Concentrated Poverty in the Wake of the Great Recession, BROOKINGS \nINSTITUTE (March 31, 2016)  https://www.brookings.edu/research/u-s-concentrated-poverty-in-the-wake-of-the-great-\nrecession/. \n130 See Edward N. Wolff, The decline of African-American and Hispanic Wealth since the Great Recession, (Nat\u2019l Bureau of \nEcon. Research, Working Paper No. 25198, 2018). \n\n\n27 \n \nintroduced by the algorithm, but the task of using the algorithm to address bias and structural \ndisadvantage in the world.  We return to this issue below. \n \nC. What questions we need to ask \n \nWith this framework for analyzing algorithmic discrimination in place, we now ask how to ferret out \ndiscrimination in a new world that involves algorithms in the loop. We discuss these issues within the \ncontext of a concrete example (hiring), and contrast what we want out of a new regulatory framework and \nhow this differs from our existing legal framework. \n \nFor orientation, it is useful to situate the standard legal analysis (without algorithms) in this context.  \nSuppose a woman applies to a small business for a job as a salesperson. Her application is rejected. She \nsues the firm for gender discrimination. For disparate treatment and disparate impact cases, the analysis is \nstraightforward, at least in broad outcome. Recall that disparate treatment might be shown if the employer \nuses some explicit or implicit rule that treats female applicants less favorably because they are female.131 \nIf no such rule is in place, the applicant might try to uncover discriminatory motive; that might be \ndifficult.132 Recall too that disparate impact might be shown if the employer uses some rule (for example, \na height and weight requirement) that treats women worse than men. If so, the rule must be justified in \nsome way.133 \n \nIf an algorithm is involved, the plaintiff\u2019s lawyers might again seek to show disparate treatment (if, for \nexample, sex is explicitly used as a factor) or disparate impact (if some factor is used that \ndisproportionately harms women). In addition, a key question that the plaintiff\u2019s lawyers would want to \nanswer is this: Why did the firm (or whoever built the algorithm for the firm) choose the outcome that it \ndid? One reason to ask that question is to establish disparate treatment. \n \nWe have seen that asking this sort of direct question in a situation of exclusively human decision-making \nis often a challenging exercise. If the algorithm is directed to take account of sex, the issue will be more \ntractable (with one qualification to which we turn below). And with a standard prediction algorithm in the \ndecision loop, whatever person builds the algorithm will have had no choice but to specify what outcome \nis being predicted \u2013 the training algorithm cannot be built without an outcome at least implicitly specified \nin the process.  \n \nMoreover, the nature of how self-incrimination arises has changed. People are ordinarily reluctant to \nadmit that they discriminated, not least because it subjects them to legal liability. But if the algorithm uses \ngender (or race, or sex) \u2013 even if it is merely directed to predict some outcome \u2013 disparate treatment will \nbe more readily apparent. Or consider the response options for someone who has intentionally selected a \nspecific, facially nondiscriminatory outcome to predict in order to discriminate. The firm that chose, say, \non-time attendance rate as the outcome to predict might be tempted to say it was chosen because that is \nwhat the firm values most in its workers. That response, however, is now hemmed in by the paper trail \nthat has accumulated over the years of the firm\u2019s operations that document what the firm has focused on \nwhen doing annual performance reviews and making promotions. The \u201csmoking gun\u201d comes not from \nsomeone confessing discrimination, but (much easier to obtain) someone making a claim about the \nselection of an outcome that transparently conflicts with everything else the firm has been doing. \n                                                 \n131 See McDonnell Douglas Corp. v. Green, 411 U.S. 792 (1973); Watson v. Fort Worth Bank & Trust, 487 U.S. 977 (1988). \n132 See Michael O. Finkelstein, The Judicial Reception of Multiple Regression Studies in Race and Sex Discrimination Cases, \n80 COLUM. L. REV. 737 (1980). \n133 See supra note. \n\n\n28 \n \n \nWhether the claim is based on disparate treatment or disparate impact, we might also want the plaintiffs to \nbe able to compare the various choices the firm made in constructing its training algorithm to different \nrelevant internal or external benchmarks, based on the three categories of discriminatory components \ndiscussed above:  \n \n\u2022 How do the hiring disparities that result from the outcome the firm actually chose compare to the \ndisparities that would have arisen if the firm had chosen other candidate outcomes for which the \nfirm has data available? (This question may be relevant to disparate impact.) \n\u2022 How do the firm\u2019s choices of candidate predictors to collect and construct compare to what other \nfirms in the same industry select? (This question may be relevant both to disparate treatment and \nto disparate impact.) The motivation for this type of industry benchmark comes from evidence that \nwhile discrimination remains an important problem, not all firms discriminate.134 \n\u2022 How does the training sample the firm used compare to the larger universe of past applicants and \nhired workers at the firm? How does it compare to the overall population? How do the statistical \nrelationships between the outcome and predictors in the training sample compare to what we have \nseen in other datasets? (These questions may be relevant both to disparate treatment and to \ndisparate impact.) \n \nNone of these comparisons by themselves would constitute sufficient proof of discrimination by the firm. \nWe need to assess the answers in view of the standards for disparate treatment and disparate impact. But \nif a firm\u2019s choice of outcome, candidate predictors or training sample is an outlier relative to the relevant \nbenchmark, we have at least suggestive evidence of disparate treatment. Or disparate impact might be \ninvolved. If the firm has a disparity in its hiring outcomes based on (say) choice of predictors, the burden \nmight shift to the firm to justify the decisions it made in constructing the training algorithm. \n \nNotice there can be some counter-intuitive results from this type of benchmarking. Suppose we have a \nfirm that proudly announces it will no longer collect information on whether applicants have a prior \ncriminal record. We compare that to the industry standard and find a similar firm in the same industry and \nlocal labor market that proudly announces they invest substantial resources in collecting information \nabout prior criminal records that is as detailed and extensive as possible. Which firm do we imagine will \nbe more likely to hire African-American applicants? A growing body of research in economics suggests \nthat the picture is complicated, in that suppressing information of one type (criminal record) can \nincentivize decision-makers to turn to other forms of information (race) in ways that may be overtly \ndiscriminatory.135 \n                                                 \n134 In the canonical economic model of discrimination from Becker, supra note, minority workers \u2013 who are a modest share of \nthe workforce in most local labor markets \u2013 will wind up seeking out and sorting into the least prejudiced set of employers. See \nBecker, supra note 24. So if employers do actually vary in their level of prejudice, and workers sort in the way that Becker\u2019s \nmodel predicts, then the racial wage gap should be determined by the level of prejudice of the marginal employer. One paper \nprovided some empirical evidence to support this prediction, showing that the prevalence of racial prejudice within a state or \nregion (as measured by survey data from the General Social Survey) is systematically related to the size of racial wage gaps. \nSee Kerwin Charles & Jonathan Guryan, Prejudice and Wages: An Empirical Assessment of Becker\u2019s The Economics of \nDiscrimination, 116 J.  POL. ECON. 773 (2008). \n135 See, e.g., Stephen Raphael, Improving Employment Prospects for Former Prison Inmates: Challenges and Policy, in \nCONTROLLING CRIME: STRATEGIES AND TRADEOFFS, (Philip J. Cook et al., eds., 2011) for studies that relate the propensity of \nfirms to check criminal backgrounds with their willingness to hire black applicants, and for studies showing that \nimplementation of a \u201cban the box\u201d policy that restricts employers from asking about prior record seems to reduce their \nwillingness to hire black applicants, see Amanda Agan & Sonja Starr, Ban the Box, Criminal Records, and Racial \nDiscrimination: A Field Experiment, 133 Q. J. ECON 191 (2018); and Jennifer Doleac & Benjamin Hansen, Does \u2018Ban The \n\n\n29 \n \n \nWe can also run simulated data through the training algorithm, where we know exactly what we should \nexpect in the resulting screening rule, to compare the screening rule that results with what we would \nexpect. For example, we might simulate a dataset where the average level of productivity for black job \napplicants is higher than for whites. If the screener that results from a given training algorithm gives us \nback a rank-ordered list of job applicants that nonetheless still hires white applicants at far higher rates \nthan blacks, we would suspect there has been something hidden inside the training algorithm that leads to \na discriminatory result, not a truly optimized algorithm. \n \nD. Problems of Proof: Human Beings vs. Algorithms \n \nSome of these arguments might seem difficult to assess in the abstract. A few stylized cases might \ntherefore be useful by way of clarification: \n \n1. A firm hires salespeople. It favors employees who will maximize sales. It ends up giving a \npreference to white applicants for one reason: it has found that the firm\u2019s customers are more \nlikely to buy from white salespeople.  \n2. A firm hires managers. Other things being equal, it seeks managers who will stay on the job \nfor a long time. In its experience, women are more likely to leave after a short period \u2013 under \nfive years \u2013 than men .  For that reason, the firm gives a preference to male applicants.  \n3. A state government is hiring entry-level budget analysts. It gives a preference to applicants \nfrom the most prestigious colleges and universities, because those applicants have done best in \nthe past. The preference has a disproportionate adverse effect on African-American applicants.  \n4. A firm hires security guards. It believes, on the basis of its experience, that security guards \nwith more than thirty years of experience do less well in the job. It therefore gives preference \nto applicants who have had fewer than thirty years of experience. The result is a disparate \nimpact on older people.  \n \nIn all four cases, the legal analysis reasonably straightforward. In cases (1) and (2), there is unlawful \ndiscrimination. Case (1) involves a form of taste-based discrimination, which is unquestionably a form of \ndisparate treatment, even if the firm is acting rationally.136 Case (2) involves statistical discrimination, \nwhich is also a form of disparate treatment. In both cases, the result is clear.137 In cases (3) and (4), we \nneed to identify the governing legal standard. Suppose that disparate treatment is forbidden and that if a \ndisparate impact is shown, a substantial business justification is necessary. If so, there would be a \nlegitimate legal challenge in both (3) and (4), not because of disparate treatment, but because the disparate \nimpact would trigger a burden of justification. \n \nWhile the legal analysis is reasonably straightforward, litigation might not be. For reasons we have \ndiscussed, the problem of proof might be formidable in cases (1) and (2): how are we to establish that race \nand gender were explicitly considered? Cases (3) and (4) might turn out to be similar. If documentary \nevidence is not available to prove the relevant preferences, a plaintiff might have to make some kind of \nstatistical demonstration. That might be challenging.138  \n \n                                                 \nBox\u2019 Help Or Hurt Low-Skilled Workers? Statistical Discrimination and Employment Outcomes When Criminal Histories Are \nHidden (Nat\u2019l Bureau of Econ. Research, Working Paper No. 22469, 2016).  \n136 See supra note. \n137 See supra note. \n138 See supra note.  \n\n\n30 \n \nNow let us consider versions of these cases where an algorithm is used.  \n \n5. A firm uses an algorithm to hire salespeople. The algorithm is designed to favor employees \nwho will maximize sales. It ends up giving a preference to white applicants for one reason: the \nfirm\u2019s customers are more likely to buy from white salespeople.  \n6. A firm uses an algorithm to hire managers. Other things being equal, the algorithm seeks \nmanagers who will stay on the job for a long time. Relevant data show that women are more \nlikely to leave after a short period \u2013 under five years \u2013 than men are. For that reason, the \nalgorithm gives a preference to male applicants.  \n7. A state government uses an algorithm to screen entry-level budget analysts. The algorithm \ngives a preference to applicants from the most prestigious colleges and universities, because \nthose applicants have done best in the past. The preference has a disproportionate adverse \neffect on African-American applicants.  \n8. A firm uses an algorithm to hire security guards. Relevant data show that security guards with \nmore than thirty years of experience do less well in the job. The algorithm therefore gives \npreference to applicants who have had fewer than thirty years of experience. The result is a \ndisparate impact on older people.  \n \nWith respect to the underlying law, cases (5) through (8) are the same as cases (1) through (4). In \nprinciple, they should not be treated differently.  \n \nThe real question is whether litigation would be different and whether the plaintiff would find it easier or \nharder to solve the problem of proof.  Finding out in cases (5) and (6) whether the algorithm used an illicit \nfactor (such as race or sex) should be more straightforward. Whether the algorithm was given access to \nsuch a factor can be seen directly in the training procedure.   \n \nThe problem is that even if the algorithm was not given access to information about race or sex, we can \nnonetheless have disparate treatment if the algorithm ends up favoring whites and men because of \ncustomer discrimination (as in case (5)) and statistical reality (as in case (6)). The real question is how to \nestablish that fact. The analysis we have developed thus far suggests an approach to addressing this \nquestion, via the framework outlined above for making the training data, objective function, and resulting \nscreening algorithm available for examination and experimentation.  We can use access to these objects to \ntry determining whether the facts are as stated in the two cases \u2013 much more effectively so than in cases \n(1) and (2). \n \nIn cases (7) and (8), the disparate impact issues should be equally straightforward. The existence of \ndisparate impact is clear; the data will prove it, and also show its magnitude. We can also attempt to \nspecify the practice that gives rise to those impacts. Then the question is the standard one: Can the \ndisparate impact be justified, given the relevant standard? That is the same question that would be asked if \nan algorithm were not involved. \n \nThe presence of the algorithm goes further \u2013 it makes it possible to quantify the tradeoffs that are relevant \nto determining whether there is \u201cbusiness necessity\u201d (or some other justification for disparate impact). \nAlgorithms by construction produce not just a single ranking of applicants. They can produce a set of \nrankings that vary based on one's tolerance for disparate impact. For each of these rankings, they \nadditionally quantify their effect on the objective, such as sales. This allows us to answer exactly the \nquestion, \u201cWhat is the magnitude of the disparate impact, and what would be the cost of eliminating or \nreducing it?\u201d  In case (8) we can say, \u201cWhat would be the effect on overall job performance if we were to \nreduce the disparate impact for every level of reduction?\u201d The issue of the business necessity behind \n\n\n31 \n \ndisparate impact now becomes easier to litigate, more readily separating specious from genuine \narguments. \n \nE. Clarifying nondiscrimination versus affirmative action \n \nPrivate and public institutions are motivated by two goals: ensuring nondiscriminatory behavior, \nregardless of gender, race, ethnicity, sexual orientation, or disability status, and helping members of \ndisadvantaged groups \u2013 sometimes producing affirmative action, understood as preferential treatment for \nmembers of such groups. The latter is of course far more controversial than the former. On one view, \naffirmative action compromises the goal of nondiscrimination and is fatally inconsistent with it;139 on \nanother view, the two are compatible.140 Algorithms help clarify the relationship between them.  \n \nAlgorithms have clearly stated objectives. There is no need to guess whether an algorithmic rule is rank-\nordering job applicants based on expected college GPA instead of by something else. Algorithms also let \nus precisely quantify tradeoffs among society\u2019s different goals. Suppose the university admissions \ncommittee cares about racial diversity and also about selecting a student body that would do as well as \npossible academically, as measured by grade point average (GPA) at (say) the end of the first year of \ncollege. An algorithm might give us the predicted GPA for every applicant, which would let us rank-order \nall applicants and count down the list until the target number of admissions is reached. But it would also \nbe possible to create separate rank-ordered lists for African-American and white applicants, which would \nallow the university to count down the list of African-American applicants until whatever diversity target \nis hit. We can then calculate what happens to average GPA of the admitted class under that hypothetical \nadmissions target. Put differently, with the algorithm it is now possible to see whether there is a tradeoff \nbetween diversity and academic performance of the incoming class, and if so, trace out what that tradeoff \ncurve looks like. \n \nThere is no obvious answer to the question of whether the drop in average GPA is worth whatever \nbenefits arise from improving racial diversity, and the algorithm should not be imagined to provide one.  \nThe key point is that the algorithm, by forcing us to specify what outcome we care about and providing us \nwith a rank-ordered list of applicants by the predicted outcome, lets us precisely quantify what we would \nbe giving up (if anything) to achieve these other objectives. We need a suitable legal framework to ensure \nthat we can capitalize on this opportunity. \n \nF. Combating Discrimination \n \nRegulation of the algorithm requires us to be able to identify and interrogate human choices in the \nconstruction of the training algorithm, and specifically their decisions about: \n \n\u2022 What outcome to predict \n\u2022 What inputs (candidate predictors) to make available to the algorithm for consideration \n\u2022 The training procedure itself \n \nAs a result, an important requirement for a new legal framework to prevent discrimination through the use \nof algorithms is transparency. These specific questions that come out of our framework help make clear \n                                                 \n139 An influential discussion is John Hart Ely, The Constitutionality of Reverse Racial Discrimination, 41 U. CHI. L. REV. 723 \n(1974). \n140 See Charles Lawrence, Two Views of the River: A Challenge of the Liberal Defense of Affirmative Action, 101 COLUM. L. \nREV. 928 (2001). \n\n\n32 \n \nthat for the successful regulation of algorithms, transparency is not an end in its own right but rather a \nmeans to an end. One might say that transparency is key for any legal system, including laws built for \npurely human decision-making: we need people to be transparent about what they are doing. But given \nthe limits of human cognition, even a well-intentioned person will have difficulty \u201chanding over\u201d what is \nasked for.  \n \nIn contrast, the inclusion of an algorithm in the decision loop now provides the opportunity for more \nfeasible and productive transparency. Effective transparency is easier to imagine once an algorithm is \ninvolved, because each of the things we are now asking the firm to hand over \u2013 the choice of outcome, \ncandidate predictors, training sample \u2013 is a tangible object. Each of them is, or at least could be, stored as \npart of the organization\u2019s standard daily operations. \n \nThe limits of this sort of transparency for many applications today is that several of the objects we would \nrequire for proper algorithmic regulation might not currently be stored by the relevant firm, particularly in \nonline learning settings where data flow in at a massive volume (such as Web search or online ad \ndelivery). Here is where the distinction becomes important between an engineering exercise of the sort \ndata scientists normally consider as opposed to the sort of legal exercise in which we are engaged here.  \n \nIn the design of a legal framework to deal with potential discrimination when algorithms are involved, \nthere are many options. New statutes or regulations could change data and storage requirements imposed \non algorithmic tasks like Web search to make them subject to the sort of discrimination inquiries we \noutline here, which in turn would expand the set of applications to which this framework applies. In the \ncontext of a specific legal challenge to alleged discrimination, it is not impossible that courts might insist \non such requirements in order to test whether a violation has occurred (though that would be an \nadmittedly unusual and aggressive step). The costs of compliance in this case could become quite large. \nWhether those costs are worth incurring will hinge on some assessment of whether the benefits justify \nthem. The history of regulation includes many cases of large record-keeping costs being imposed on \nprivate firms in cases where the benefits were believed to be considerable; for example, after the financial \nmarket crash of 2008 almost brought about the collapse of the global economy, the Dodd-Frank act \nrequired firms to keep records on every single transaction that occurred in the $400 trillion swaps \nmarket.141 That is a lot of transactions. In the abstract, it is not unreasonable to expect that if algorithms \nare being used, data and storage requirements could become automated at reasonable cost. \n \nFor regulating possible discrimination within the public sector in particular there is a different challenge \u2013 \nthe risk of broken procurement processes.142 Some of the organizations using algorithms to help inform \ncritical decisions, particularly government agencies, lack the internal capacity to build and maintain this \ntype of data-driven system.143 So they understandably turn for help to some outside organization with the \nrelevant expertise.144 Those outside organizations are frequently for-profit firms. What they are selling is \npartly the implementation of their algorithm, but more importantly it is the algorithm itself. They \u2013 \nunderstandably, though not laudably \u2013 do not want to make the detailed workings of their algorithm \npublic for fear of reducing the value of their intellectual property.  \n \n                                                 \n141 See, e.g., Dodd-Frank Act, Pub. L. No. 111-203, \u00a7 729, 83 Stat. 1376-2223 (2010) (prior to 1975 amendment).   \n142 See Christopher Yukins, The U.S. Federal Procurement System: An Introduction (2017), available at \nhttps://papers.ssrn.com/sol3/papers.cfm?abstract_id=3063559.  \n143 For relevant discussion, see Sarrah Chraibi et al., An Exploratory Model for Outsourcing-purchasing Activities Based on a \nComparative Study, 3 SUPPLY CHAIN FORUM 138 (2017). \n144 See id. \n\n\n33 \n \nFor the same reason: Once some organization has incurred all the up-front costs of building and installing \na new algorithmic system, the outside firm has leverage that tempts it to charge high prices for updating \nthe tool. If the algorithm is not regularly updated there is the risk of what Koepke and Robinson call \n\u201czombie predictions,\u201d where the algorithm is built on an old dataset that reflects conditions that are quite \ndifferent from when the predictions are now being applied.145 Yet cash-strapped agencies too frequently \ncannot or will not pay their consultants what is needed to update the models. \n \nVI. \nUSING ALGORITHMS TO PROMOTE EQUITY \n \nAlgorithms have the potential to help us to excise disparate treatment, to reduce discrimination relative to \nhuman decision-making, to limit disparate impacts, and also to predict much more accurately than humans \ncan in ways that disproportionately benefit disadvantaged groups \u2013 what we call the \u201cdisparate benefit\u201d of \nalgorithms. \n \nA. De-bias relative to humans \n \nAs we have seen, the tendency to classify others into \u201cin-groups\u201d and \u201cout-groups\u201d is a key feature of \nhuman psychology that contributes to explicit and implicit biases throughout society. For example, \nBertrand and Mullainathan show that resumes with a typically African-American name are much less \nlikely to receive a call-back than similar resumes that list a common white name.146 To overcome the \neffects of these human biases, we could try to exhort hiring managers to ignore irrelevant information on \nresumes. Or we could provide them with some sort of implicit bias training. But given the opacity of \nhuman decision-making, it would be very difficult to determine whether such efforts had actually been \nsuccessful.147  \n \nThe use of an algorithm is an alternative way to try to deal with the bias of human decision-making. To \nthe algorithm, the name on a resume, race, age, sex, or any other applicant characteristic are candidate \npredictors like any other: variable X42. If this variable is not predictive of the outcome, the algorithm will \nnot use it. And since predicting the outcome is all the algorithm is designed to do, we do not have to \nworry about any hidden agenda on the part of the algorithm itself. And recall that if use of a characteristic \nis predictive but would violate antidiscrimination law, use of that characteristic can and should be \nprohibited. But here there is an important qualification, to which we now turn. \n \n \nB. Access to the protected variable promotes equity for the algorithm \n \nConsider a firm that is trying to decide which sales people to steer towards its most lucrative clients based \non a prediction of their future sales level. Candidate predictors include (1) past sales levels and (2) \nmanager ratings. Suppose that for men, managers provide meaningful assessments that include useful \nsignals about employee performance that are not fully captured in the past sales data. But suppose that for \nwomen, the managers discriminate and give the lowest possible ratings. (This is of course disparate \ntreatment and therefore unlawful.) An algorithm that is prohibited from knowing gender might well use \nmanager ratings as a predictor, because it has a useful signal for half the sample. And because the \n                                                 \n145 See John Logan Koepke & David G. Robinson, Danger Ahead: Risk Assessment and The Future of Bail Reform, 93 WASH. \nL. REV. 1725-1807 (2017).  \n146 See Bertrand & Mullainathan, supra note.  \n147 See S.M. Jackson et al., Using Implicit Bias Training to Improve Attitudes Toward Women in STEM, 17 SOCIAL \nPSYCHOLOGY OF EDUCATION 419 (2014). \n\n\n34 \n \nalgorithm in this case does not know who is male and who is female, it has no choice but to assume that \nmanager ratings mean the same thing for all workers. The resulting predictions would understate the \nfuture productivity of women and hence contribute to gender gaps in earnings. \n \nBut what happens if we instead allowed the algorithm to be aware of gender? With adequate training data, \nthe algorithm could detect that manager ratings are predictive of future sales for men but not for women. \nSince the algorithm is tasked with one and only one job \u2013 predict the outcome as accurately as possible \u2013 \nand in this case has access to gender, it would on its own choose to use manager ratings to predict \noutcomes for men but not for women. The consequence would be to mitigate the gender bias in the \ndata.148  Clearly such a mitigating effect will not result from the use of a protected attribute in every \nsituation, but we can easily find additional scenarios where similar considerations arise. \n \nFor example, allowing the algorithm to have access to protected-class membership can also promote what \nmany people would consider to be equity in cases where the relationship between the candidate predictors \nand outcomes differ between the advantaged and disadvantaged groups. To use an admittedly more \ncontroversial example, suppose that we have two college applicants who both score 1,100 on the SAT. \nOne of them is from New Trier, an affluent north-shore suburb of Chicago where the median family \nincome is $145,000 (three times the national average) with public schools among the nation\u2019s best. The \nother is from Englewood, a south side Chicago neighborhood with median family income under $20,000 \nand among the city\u2019s highest homicide rates. Outside of extraordinarily unusual circumstances, it cannot \nbe the case that the amount of effort, persistence, and extra learning on one\u2019s own required to score 1,100 \non the SAT is the same for the student who starts with every possible advantage as for the one forced to \novercome a long list of difficult obstacles.  \n \nYet when we prohibit an algorithm from having access to information about the college applicant\u2019s \ndisadvantaged group membership (in this case, let us stipulate, race as well as economic circumstances \nand background149), that is exactly what we are forcing the algorithm to assume. From the standpoint of \ncurrent law, it is not clear that the algorithm can permissibly consider race, even if it ought to be \nauthorized to do so; the Supreme Court allows consideration of race only to promote diversity in \neducation.150 Whatever the law requires, many people would find it tempting to say that while the \nalgorithm should focus on factors independent of race (poor neighborhood, violent crime, and so forth), it \nshould not focus on race itself.151 But sometimes race itself is relevant as a predictor, and on plausible \nassumptions, there is nothing invidious about insisting on that point.  \n \n                                                 \n148 As another example, consider a city in which half of all residents are white, half are black. Suppose that white residents and \nblack residents engage in criminal activity at exactly the same rates, and that the police force in this city manages to arrest \neveryone who commits a crime. But in addition, the police department includes some discriminatory officers and as a result \nhalf of all arrests made to black residents are actually false arrests (that is, an arrest when the person was not reasonably \nthought to have committed a crime). In this case, the arrest rate will be higher for blacks than for whites even though true rates \nof crime are the same. Now suppose we use these data to predict failure to appear in court, or FTA, which (for the moment) we \nwill assume is accurately measured. An algorithm blinded to race has no choice but to assume that the relationship between \nprior arrests and FTA risk is the same for both groups; since the rate of prior arrests is higher for blacks as whites, the race-\nblind algorithm would generally predict a higher FTA risk for blacks. In contrast, an algorithm able to use race would detect in \nthe data that (in our stylized example) the effect of each prior arrest on FTA risk was smaller for black residents than for \nwhites; with more prior arrests for blacks than whites, the net result would be equalized predicted FTA risks for blacks and \nwhites by the race-aware algorithm. \n149 We acknowledge that the stipulation is controversial. To skeptics, we emphasize that it is a stipulation. \n150 See Grutter v. Bollinger, 539 U.S. 306 (2003). \n151 See Gratz v. Bollinger, 539 U.S. 244 (2003). \n\n\n35 \n \nThese are not just hypothetical examples; we can see them play out in actual data. In Kleinberg et al., 152 \nwe use data on a nationally representative sample of teens to predict college performance and simulate \ndifferent admission decisions.153 Table 1 shows that, consistent with past studies, high school outcomes \ndiffer on average by race, and as a result we see differences in average college outcomes as well. Figure 1 \nshows that a race-aware algorithm allows us substantially to increase the share of admitted students who \nare minority (holding average GPA constant) relative to ignoring information about race. The share of the \nadmitted class that has GPA<2.75 is on the y-axis and the share that is African-American is on the x-axis. \nWe show results from a race-aware algorithm, a race-blind algorithm, and an algorithm that pre-processes \nthe data so that the average of all high school predictors is the same on average for white and black \nstudents.154 The algorithms let us rank-order applicants by predicted GPA outcome.  \n \nIf we do this separately by race, we can select whatever share of the incoming class we wish to be \nminority by deciding how far down the minority student list to go. The tradeoff curve is upward-sloping \nfor each algorithm (more diversity, higher share GPA < 2.75), but the race-aware algorithm dominates the \nothers. For example, if we wanted to hold the share of admitted students with GPA<2.75 at 14%, the \nshare of admitted students who are black would be 7% with the race-blind model, 11% with the pre-\nprocessed model, and 19% with the race-aware algorithm.  \n \nFigure 2 shows why the race-aware algorithm dominates. This \u201cheat map\u201d shows the share of black \nstudents in different predicted GPA \u201cbins\u201d according to the race-blind (x-axis) or race-aware (y-axis) \nalgorithms. Observations that lie off of the 45-degree line show the models disagree. For example, the \nrace-blind algorithm says the students in the right-hand lower corner have high likelihood of GPA<2.75 \n(9th decile), but the race-aware predictor (accurately) tells us they are actually in the lowest-risk decile. \nThe race-blind model mis-ranks these students because the relationships between high school predictors \nof college success (such as test scores) turn out to be different for white versus black students.155  \n \nThis discussion highlights how the mechanical application of current antidiscrimination law to algorithms \nmight actually have harmful effects on exactly those populations we seek to protect. Our goal here is not \nto offer a final view on whether and when current law would forbid decision-makers \u2013 whether human \nbeings or algorithms \u2013 from explicitly considering race. It is enough to say that courts would be very \nuncomfortable with that practice, and their discomfort, in the context of algorithms at least, might well be \na mistake. \n \nC. Disparate benefit from improved prediction \n                                                 \n152 The dataset we use is the US Department of Education\u2019s National Education Longitudinal Study of 1988, which captures \ninformation on a nationally representative sample of students who entered 8th grade in the fall of 1988. U.S. DEPARTMENT OF \nEDUCATION, NATIONAL EDUCATION LONGITUDINAL STUDY OF 1988 (1988). Follow up surveys were conducted in 1990, 1992, \n1994 and in 2000 (when respondents were in their mid-20s). See Available Data, U.S. DEPARTMENT OF EDUCATION, \nhttps://nces.ed.gov/surveys/nels88/data_products.asp.  We limit our analysis sample to those who participated in the 2000 \nsurvey wave, and had ever attended a four-year post-secondary institution. To simplify the analysis, we focus on comparing \njust non-Hispanic white students (N=4,274) with black students (N=469). In the public-use version of the NELS we use here, \nwe do not have access to ACT or SAT scores. But we do have the results of how students did on standardized academic \nachievement tests that the NELS administered to students in four academic areas: math, reading, science, and social studies. \n153 See Kleinberg et al., supra note.  \n154 We might do this if, for example, we believed that systematic differences across groups in the average values of the \npredictor variables (Xs) was due to some sort of societal unfairness, or to biased data. \n155 When we use the race-blind algorithm, we force the statistical model to assume the relationship is the same for both whites \nand blacks, which distorts the predicted outcomes for black applicants. This leads us inadvertently to say some high-\nperforming black applicants are actually low-performing, that is, to understate their academic potential and hence reject them, \nwhile simultaneously admitting lower-performing black applicants instead. \n\n\n36 \n \n \nThere is a less controversial but comparably important point. The largest potential equity gains may come \nfrom simply predicting more accurately than humans can. This increase in accuracy can generate benefits \nthat disproportionately accrue to disadvantaged groups, leading to what we call \u201cdisparate benefit.\u201d \n \nFor example, landlords in the private housing market have difficulty predicting which tenants are at high \nrisk for skipping rent payments. This difficulty may lead landlords to implement blanket rules, such as \nrequiring first and last month rent to protect against missed payments. For affluent renters, this is a minor \ninconvenience. But for low-income, low-wealth families, this can be the difference between leasing an \napartment and doubling up with someone else (or even becoming homeless). Better prediction could \nallow landlords to relax collateral requirements, which would change the system in ways that helps \ndisadvantaged families. \n \nBetter prediction could also let us expand some systems in ways that help disadvantaged groups. \nCompared to affluent white families, low-income minority families tend to live in neighborhoods that do \nnot only have higher poverty rates, but are also located further from good schools, health care and jobs.156 \nThis distance is particularly a problem for families who rely on public transportation, which in the US \nremains, as Pendall puts it, \u201cslow, inconvenient, and [lacks] sufficient metropolitan-wide coverage to rival \nthe automobile.\u201d157 While car loans have increased in recent years, interest rates often remain high in part \nbecause of non-payment: 6 million people are at least 90 days behind on their car loans.158 High interest \nrates surely contribute to differences in car ownership by income and employment status.159 Better \nprediction of payment risk could allow lenders to lower interest rates for low-risk families and expand \naccess to car ownership in ways that disproportionately benefit the disadvantaged. \n \nWe are offering somewhat conjectural examples, but these potential gains are not merely conjectural. \nConsider an application where better prediction lets us shrink a system that disproportionately harms \ndisadvantaged groups: pre-trial release decisions for criminal defendants.160 State law in New York \nrequires judges make these decisions based on a prediction of defendant risk of failure to appear (FTA) in \ncourt in the future. We use data from New York City on all cases that were continued at arraignment over \na five-year period and build a model to predict FTA risk. The predictor variables in the model consist of \nage (a legally allowable variable), current offense, and prior criminal record. The data show that the \njudges\u2019 risk predictions (implicit in their release decisions) are correlated with the algorithm, but that \ncompared to the algorithm, the judges make numerous serious mistakes: they detain many low-risk people \nand release many high-risk ones.161  \n \n                                                 \n156 Rolf Pendall, For Many Low-Income Families, Car May Be Key to Greater Opportunity, URBAN INSTITUTE (2014), \nhttps://www.urban.org/urban-wire/many-low-income-families-cars-may-be-key-greater-opportunity. \n157 Ibid.  \n158 See Gwynn Guilford, American Car Buyers Are Borrowing Like Never Before \u2013 And Missing Plenty Off Payments, Too, \nQUARTZ (Feb. 21, 2017), https://qz.com/913093/car-loans-in-the-us-have-hit-record-levels-and-delinquencies-are-rising-fast-\ntoo/.  \n159 See Steven Raphael & Lorien Rice, Car Ownership, Employment, and Earnings, 52 J. OF URBAN ECON. 109 (2002).   \n160 See Kleinberg, supra note. \n161 In particular, judges seem substantially to over-weight the severity of the current charge (i.e., whatever offense the person \nwas arrested for that led them to wind up in court). See id.; Cass R. Sunstein, Algorithms, Correcting Bias, SOCIAL \nRESEARCH (forthcoming 2019). When the judges form their own implicit list of defendants to prioritize for detention based \non their subjective assessment of defendant risk, the judges wind up putting too many felony arrestees towards the top of their \nlist and too many misdemeanor arrestees towards the bottom of the list. But the machine learning algorithm makes clear that \nprior record matters a lot in terms of future risk, and human judges are not fully taking this into account. Kleinberg et al., supra \nnote. \n\n\n37 \n \nIf we made these decisions using the algorithm\u2019s risk prediction rather than those of the judge, we could \nprioritize detention just for those people who are high risk. Indeed, some of the high-risk people judges \nrelease are so high risk that if we detained them, we could release multiple low-risk people without \nincreasing FTAs. Indeed, if we followed the algorithm\u2019s release recommendations, we could reduce the \njail population by 42% without increasing FTA rates at all.162 \n \nFigure 3 provides a more concrete sense for what this means in practice. In a city in which around 50,000 \npeople spend time in jail each year, it would mean about 20,000 fewer people spending time behind bars \non Riker\u2019s Island each year.163 This would be the equivalent of closing Riker\u2019s Island at the end of July \nevery year, with no increase in FTA or re-arrests. And note who benefits the most from this large \nreduction in detentions: the two groups who together account for nearly 90% of all current jail spells -- \nAfrican-American and Hispanic defendants.164 This is a disparate benefit, and an especially large one. \n \nD. Algorithms can reveal our own biases \n \nA final potential benefit arises from the fact that algorithms can help reveal our own biases. Imagine a \nlarge, growing firm that is inundated with job applications. To help prioritize which resumes to consider \nas it hires and expands, the firm builds an algorithm to rank applications. The outcome predicted by this \nalgorithm is based on the people the firm\u2019s managers hired in the past. The firm then notices that the new \nalgorithm mostly hires men. A possible response would stem from the recognition that the algorithm, \ngiven how it was constructed, reveals implicit or explicit biases among the firm\u2019s HR team. In that light, \nthe firm might try to overcome biased human judgments by building a new algorithm that predicts an \noutcome less infected by human bias, such as a more objective measure of worker productivity once \nhired. A perhaps less helpful response would lose sight of what is responsible for the algorithm\u2019s bias, and \ndrop the algorithm altogether to revert back to a purely human hiring process.165 \n \nThe transparency of algorithms will have other consequences that might be uncomfortable for many \npeople. Recall that the disparate impact standard forbids disproportionate adverse effects on members of \ncertain groups, unless there is a strong independent justification for the requirement or practice that \ncreates those adverse effects. Here we can see the lines blurring between antidiscrimination principles and \naffirmative action.166 Suppose that we have two candidate algorithms to predict worker productivity. One \nof them would lead to hiring a set of workers that is 1% more productive than those hired by the other, but \nreduces the number of minorities hired by 10%. Is this productivity gain large enough to provide \u201cstrong \n                                                 \n162 Id.  \n163 See NEW YORK CITY DEPARTMENT OF CORRECTION, NYC DEPARTMENT OF CORRECTION AT A GLANCE (2018), \nhttps://www1.nyc.gov/assets/doc/downloads/press-release/DOC_At%20a%20Glance-entire_FY%202018_073118.pdf. \n164 These potential gains are not limited to pre-trial release decisions. See Aaron Chalfin et al., Productivity and Selection of \nHuman Capital with Machine Learning, 106 AM. ECON. REV.: PAPERS & PROCEEDINGS 124 (2016). Using algorithmic \npredictions rather than human judgment can help police departments hire officers at lower risk for adverse outcomes like \npolice-involved shootings, complaints for verbal abuse, or complaints for physical abuse, and can help school systems hire \nmore effective teachers. Because police misconduct so disproportionately involves disadvantaged populations, see Roland \nFryer, Reconciling Results on Racial Differences in Police Shootings, 108 AM. ECON. ASSOC. PAPERS & PROCEEDINGS 228 \n(2018), and similarly because the most effective teachers tend to prefer working with the most affluent students leaving the \nleast effective to serve disproportionately low-income and often predominantly minority schools, see, e.g., Mimi Engel et al., \nNew Evidence on Teacher Labor Supply, 51 AM. EDUC. RES. J. 36 (2014), any system for improving the average quality of \npolice officers and teachers will have particularly large benefits for low-income and minority populations. \n165 Interestingly, this is what some media accounts suggest may have happened in 2018 at Amazon. See Reuters, Amazon \nDitched AI Recruiting Tool That Favored Men for Technical Jobs, THE GUARDIAN, (Oct. 11, 2018), \nhttps://www.theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine. \n166 See Strauss, supra note. \n\n\n38 \n \nindependent justification\u201d for that algorithm? Answering that question unavoidably winds up requiring \nvalue judgments about how to trade off avoidance of disparate racial impacts against other social \nobjectives (such as output).  \n \nAnother way to put it is to say that those who favor affirmative action programs are sometimes willing to \nsacrifice some value for the sake of other goals (such as racial justice as they see it). Those who want to \nprevent disparate impacts, or who want to ensure that any disparate impact is strongly justified, are \nwilling to do exactly the same thing. In practice, current antidiscrimination law says that it is worth \nsuffering something, but not too much, to stop disparate impacts on (say) African-Americans or women. \nBut how much? Algorithms permit unprecedented clarity about these questions by allowing us to specify \nthe magnitude of tradeoffs. What if, instead of a 1% productivity gain for a 10% decline in minority \nhiring, it was a 10% productivity gain for a 1% decline in minority hiring? Many people might respond \nthat the relevant judgments will depend on the precise numbers, which are typically impossible to \nquantify with \u201cblack box\u201d human decision-making. \n \nVII.",
        "page_start": 15,
        "page_end": 39,
        "chunk_ids": []
      },
      {
        "id": "section_3",
        "title": "Conclusion",
        "content": "Conclusion  \n \nIt is tempting to think that human decision-making is transparent and that algorithms are opaque. We have \nargued here that with respect to discrimination, the opposite is true. The use of algorithms offers far \ngreater clarity and transparency about the ingredients and motivations of decisions, and hence far greater \nopportunity to ferret out discrimination.   \n \nFor those who wish to reduce discriminatory behavior, this is a massive opportunity. Countless decisions \nhave the flavor of the screening problems we discuss here; they hinge on a prediction. There is powerful \nevidence of discrimination in current human decision-making. To offer just a few examples: \n \n \n\u2022 Audit studies that randomly assign otherwise-equivalent white and black applicants to apply at \ndifferent firms find that white applicants are called back at more than twice the rate of black \napplicants, 34% versus 14%.167 Reducing this bias would do an enormous amount of social good \ngiven there are over 6 million job openings in the US at any point in time.168 \n\u2022 Audit studies of the US housing market, which originates $2 trillion in new mortgages each \nyear,169 find that minority borrowers are treated differently from and worse than white \nborrowers.170 \n\u2022 Bias also arises in the health sector, which accounts for $3.5 trillion in spending each year in the \nUS alone (equal to 18% of GDP).171 For example, when doctors were shown two equivalent \n                                                 \n167 This figure is for whites and blacks without criminal records. Among those with criminal records we see similarly large \ndifferences in call-back rates, equal to 17% versus 5%, respectively. See Devah Pager, The Mark of a Criminal Record, 108 \nAM. J. SOC. 937 (2003). \n168 Bureau of Labor Statistics, Economic News Release: Job Openings and Labor Turnover Summary, (Jan. 8, 2019), \nhttps://www.bls.gov/news.release/jolts.nr0.htm. \n169 See Tendayi Kapfidze, U.S. Mortgage Market Statistics: 2018, MAGNIFY MONEY BY LENDING TREE (Dec. 21, 2018), \nhttps://www.magnifymoney.com/blog/mortgage/u-s-mortgage-market-statistics-2017/.  \n170 See Margery Austin Turner et al., All Other Things Being Equal: A Paired Testing Study of Mortgage Lending Institutions \u2013 \nFinal Report (2002).  \n171 CENTERS FOR MEDICARE & MEDICAID SERVICES, NATIONAL HEALTH EXPENDITURE DATA FOR 2017 (2018), \nhttps://www.cms.gov/research-statistics-data-and-systems/statistics-trends-and-reports/nationalhealthexpenddata/nhe-fact-\nsheet.html; GDP, THE WORLD BANK, https://data.worldbank.org/indicator/NY.GDP.MKTP.CD. \n\n\n39 \n \npatient histories, the chances of recommending a beneficial procedure (cardiac catheterization) \nwere 40% lower for women and minorities than white males.172  \n \nFor each of these critically important decisions, more and more data are becoming available over time on \nhow people in the past made these decisions, the characteristics of these people and their decision-making \nenvironments, and the consequences of different decisions. These data make it increasingly possible to \nbuild data-driven statistical prediction models. Those models might be used to detect, reduce, or eliminate \nexisting discrimination. \n \nAlgorithms have extraordinary promise. They have the potential to make important strides in combating \ndiscrimination, at least as the legal system has long understood it. But principles of transparency and \nauditability, fair and nondiscriminatory choice of data, and reasonable algorithmic objective are essential, \nnot least to help understand and select the tradeoffs that people use algorithms to make.  \n \n \n                                                 \n172 See Kevin Schulman et al., The Effect of Race and Sex on Physicians' Recommendations for Cardiac Catheterization, 340 \nNEW ENGLAND J. OF MED. 618 (1999). \n\n\n40 \n \n \nI. \nAPPENDIX: A CHARACTERIZATION OF ALGORITHMIC BIAS \n \nIn this appendix, we justify the claim that algorithmic bias can be decomposed completely into three \ncomponents: bias in the choice of input variables, bias in the choice of outcome measure, and bias in the \nconstruction of the training procedure.  The disparity that remains after accounting for these three forms \nof bias corresponds to the structural disadvantage of one group relative to another.  \nTo make this claim precise, we use the following formalism.  Suppose that in a screening problem we \nhave applicants from two groups: an advantaged group and a disadvantaged group.  The complete \ndescription of each applicant is specified by a (long) feature vector \ud835\udc65\ud835\udc65, and their true productivity is \nspecified by a function \ud835\udc53\ud835\udc53(\ud835\udc65\ud835\udc65) of this feature vector.  \nSuppose that in the advantaged group, a \ud835\udc5d\ud835\udc5d(\ud835\udc65\ud835\udc65) fraction of the applicants has feature vector \ud835\udc65\ud835\udc65.  In other \nwords, a random applicant drawn from the advantaged group will have feature vector \ud835\udc65\ud835\udc65 with probability \n\ud835\udc5d\ud835\udc5d(\ud835\udc65\ud835\udc65).  Analogously, in the disadvantaged group, a \ud835\udc5e\ud835\udc5e(\ud835\udc65\ud835\udc65) fraction of the applicants have feature vector x.  \nThus, the average productivity of applicants in the advantaged group is the sum over all \ud835\udc65\ud835\udc65 of \ud835\udc5d\ud835\udc5d(\ud835\udc65\ud835\udc65)\ud835\udc53\ud835\udc53(\ud835\udc65\ud835\udc65), \nwhich we will write more compactly as \u2211 \ud835\udc5d\ud835\udc5d(\ud835\udc65\ud835\udc65)\ud835\udc53\ud835\udc53(\ud835\udc65\ud835\udc65)\n\ud835\udc65\ud835\udc65\n, and the average productivity of applicants in the \ndisadvantaged group is the corresponding sum \u2211 \ud835\udc5e\ud835\udc5e(\ud835\udc65\ud835\udc65)\ud835\udc53\ud835\udc53(\ud835\udc65\ud835\udc65)\n\ud835\udc65\ud835\udc65\n.   \nWe add one more piece of notation: for any function \ud835\udc63\ud835\udc63, we let \ud835\udc37\ud835\udc37(\ud835\udc63\ud835\udc63) denote the difference in the average \nvalue of \ud835\udc63\ud835\udc63 between the advantaged and disadvantaged groups; that is, \ud835\udc37\ud835\udc37(\ud835\udc63\ud835\udc63) = \u2211 [\ud835\udc5d\ud835\udc5d(\ud835\udc65\ud835\udc65) \u2212 \ud835\udc5e\ud835\udc5e(\ud835\udc65\ud835\udc65)]\ud835\udc63\ud835\udc63(\ud835\udc65\ud835\udc65)\n\ud835\udc65\ud835\udc65\n.  \nThus, the difference in average productivity between the two groups is equal to \ud835\udc37\ud835\udc37(\ud835\udc53\ud835\udc53); we can think of this \nas quantifying the structural disadvantage of one group relative to the other. \nNow, the designer of an algorithm for the screening problem does not have access to the true function \ud835\udc53\ud835\udc53, \nnor to an applicant\u2019s full feature vector \ud835\udc65\ud835\udc65.  Instead, the algorithm designer creates a screening rule \nfollowing the process described in the paper.   \n\u2022 The designer specifies the performance of each applicant using a function \ud835\udc54\ud835\udc54(\ud835\udc65\ud835\udc65), which will in \ngeneral be different from \ud835\udc53\ud835\udc53(\ud835\udc65\ud835\udc65).   \n\u2022 For each applicant, the designer does not use the full feature vector x, but instead a reduced \nrepresentation \ud835\udc5f\ud835\udc5f(\ud835\udc65\ud835\udc65).  This means that neither the function \ud835\udc53\ud835\udc53 nor \ud835\udc54\ud835\udc54 can be directly applied to the \nrepresentation of an applicant; rather, a function \u210e is applied to \ud835\udc5f\ud835\udc5f(\ud835\udc65\ud835\udc65), yielding a value \u210e(\ud835\udc5f\ud835\udc5f(\ud835\udc65\ud835\udc65)).  \nWe write \u210e \u2218  \ud835\udc5f\ud835\udc5f for the composition of the functions \u210e and \ud835\udc5f\ud835\udc5f (obtained by first applying \ud835\udc5f\ud835\udc5f, and \nthen applying \u210e); in this notation, the value computed for an applicant with true feature vector \ud835\udc65\ud835\udc65 is \n(\u210e \u2218  \ud835\udc5f\ud835\udc5f)(\ud835\udc65\ud835\udc65). \n\u2022 Finally, the designer must estimate \u210e from the training data they have collected, and this results in \na function \ud835\udc61\ud835\udc61.  Like the function \u210e, this function \ud835\udc61\ud835\udc61 can be applied to the representation \ud835\udc5f\ud835\udc5f(\ud835\udc65\ud835\udc65), \nyielding a value (\ud835\udc61\ud835\udc61 \u2218  \ud835\udc5f\ud835\udc5f)(\ud835\udc65\ud835\udc65). \nThe result of this design process is an algorithm that, to an applicant with feature vector \ud835\udc65\ud835\udc65, assigns a score \n\n\n41 \n \nof (\ud835\udc61\ud835\udc61 \u2218  \ud835\udc5f\ud835\udc5f)(\ud835\udc65\ud835\udc65).  The applicants are then ranked by this score, with the highest-scoring applicants selected. \nThe difference in average scores assigned to the advantaged and disadvantaged groups is thus \ud835\udc37\ud835\udc37(\ud835\udc61\ud835\udc61 \u2218  \ud835\udc5f\ud835\udc5f), \nwhereas the actual level of structural disadvantage is \ud835\udc37\ud835\udc37(\ud835\udc53\ud835\udc53).  How should we understand the contrast \nbetween these two quantities?  The key idea is to write \ud835\udc37\ud835\udc37(\ud835\udc61\ud835\udc61 \u2218  \ud835\udc5f\ud835\udc5f) in the following extended form: \n\ud835\udc37\ud835\udc37(\ud835\udc61\ud835\udc61  \u2218  \ud835\udc5f\ud835\udc5f) =   \ud835\udc37\ud835\udc37(\ud835\udc53\ud835\udc53) + (\ud835\udc37\ud835\udc37(\ud835\udc54\ud835\udc54) \u2013  \ud835\udc37\ud835\udc37(\ud835\udc53\ud835\udc53)) + (\ud835\udc37\ud835\udc37(\u210e \u2218  \ud835\udc5f\ud835\udc5f) \u2013  \ud835\udc37\ud835\udc37(\ud835\udc54\ud835\udc54)) + (\ud835\udc37\ud835\udc37(\ud835\udc61\ud835\udc61 \u2218  \ud835\udc5f\ud835\udc5f) \u2013  \ud835\udc37\ud835\udc37(\u210e \u2218  \ud835\udc5f\ud835\udc5f)) \nIt is easy to verify that the left- and right-hand sides are equal.  The advantage of the extended formulation \non the right-hand side is that each of its form terms is directly interpretable, as follows. \n\u2022 The first term, \ud835\udc37\ud835\udc37(\ud835\udc53\ud835\udc53), is the average difference in \ud835\udc53\ud835\udc53 between the two groups, and hence \ncorresponds to the structural disadvantage. \n\u2022 The second term, \ud835\udc37\ud835\udc37(\ud835\udc54\ud835\udc54) \u2013  \ud835\udc37\ud835\udc37(\ud835\udc53\ud835\udc53), is the additional bias introduced by the choice of outcome \nmeasure: \ud835\udc54\ud835\udc54 instead of \ud835\udc53\ud835\udc53. \n\u2022 The third term, (\ud835\udc37\ud835\udc37(\u210e \u2218  \ud835\udc5f\ud835\udc5f) \u2013  \ud835\udc37\ud835\udc37(\ud835\udc54\ud835\udc54)), is the additional bias introduced by the choice of input \nvariables: the reduced representation \ud835\udc5f\ud835\udc5f(\ud835\udc65\ud835\udc65) instead of the full feature vector \ud835\udc65\ud835\udc65. \n\u2022 The fourth term, \ud835\udc37\ud835\udc37(\ud835\udc61\ud835\udc61 \u2218  \ud835\udc5f\ud835\udc5f) \u2013  \ud835\udc37\ud835\udc37(\u210e \u2218  \ud835\udc5f\ud835\udc5f), is the additional bias introduced by the training procedure: \nthe fact that we must use an estimated \ud835\udc61\ud835\udc61 rather than \u210e. \nThis equation thus formalizes the sense in which, after accounting for three forms of bias introduced in \nthe design of the algorithm \u2013 the choice of outcome measure, the choice of input variables, and the design \nof the training procedure \u2013 what remains is the level of structural disadvantage between groups.  As \ndiscussed earlier, we may wish to pursue interventions aimed at reducing all three forms of bias as well as \nthe structural disadvantage, but it is important to understand how our interventions are distributed across \nthese different effects. \n \n \n \n \n\n\n42 \n \n \nTable 1 \nDescriptive statistics from NELS dataset for predicting college performance \n \n \nWhites \nBlacks \nN \n4,274 \n469 \nCollege outcomes (2000 follow-up)  \n       Graduate with BA by 2000 \n       College GPA \u2265 2.75 \n       College GPA \u2265 3.25 \n \n67.4% \n82.2% \n48.4% \n \n50.9% \n69.5% \n31.1% \nFemale \n \n53.2% \n \n56.7% \n \n10th grade NELS test scores (standard deviation = 10) \n       Reading \n       Math \n       Science \n       History \n \n56.3 \n57.1 \n56.2 \n56.0 \n \n50.0 \n49.5 \n47.5 \n50.0 \n10th grade course grades \n      English: Mostly A\u2019s \n      Math: Mostly A\u2019s \n      Science: Mostly A\u2019s \n      History: Mostly A\u2019s \n \n29.4% \n26.9% \n26.9% \n29.1% \n \n19.6% \n14.9% \n14.4% \n15.7% \n10th grade extracurricular hours per week \n        None  \n        0-1 \n        1-4 \n        5-9 \n       10-19 \n       20+   \n \n21.4% \n16.9% \n21.9% \n16.3% \n17.7% \n  2.1% \n \n30.2% \n14.4% \n23.4% \n13.6% \n11.3% \n   1.2% \n12th grade NELS test scores (standard deviation = 10) \n       Reading \n       Math \n       Science \n       History \n \n56.0 \n57.1 \n56.2 \n56.0 \n \n50.0 \n49.5 \n47.5 \n50.0 \nNumber of credits taken by 12th grade by subject: \n       English \n       Math \n       Science \n       Social Studies \n \n3.6 \n3.2 \n2.9 \n3.1 \n \n3.4 \n2.9 \n2.5 \n2.8 \nTook SAT \nTook ACT \n60.4% \n52.6% \n63.1% \n37.9% \n \n \n \n \n\n\n43 \n \nFigure 1 \nFairness vs. efficiency tradeoffs in college admissions using race-aware vs. race-blind algorithms \n \n \n \nSource: Results from Kleinberg, Ludwig, Mullainathan and Rambachan (2018) using data from the NELS:88 to predict college \nperformance for those students who attended a four-year college (as measured by GPA<2.75). The graph shows the results of \nrank-ordering all students by predicted performance and simulating an admission rule that admits the top half; the y-axis shows \nthe percent of the \u201cadmitted class\u201d that goes on to have a GPA below 2.75 (\u201cmostly B\u2019s\u201d) while the x-axis shows the share of \nthe admitted students who are African-American. The points in the graph show the result of rank-ordering applicants using \nlinear probability models that (a) are \u201cblinded\u201d to any information about each applicant\u2019s race [circle]; (b) use information \nabout each applicant\u2019s race to pre-process the data and orthogonalize the predictors to race, so that the average value of high \nschool grades, test scores, etc. are now forced to be equal for black and white applicants [triangle]; and (c) use information \nabout applicant race to form the prediction model [cross]. For each candidate algorithm, we also show what happens if we use \nour decision-making framework to achieve a target fairness level (share of the admitted college class that is African-\nAmerican). Specifically, we take the predictions from a given algorithm, create a list of white applicants rank-ordered by \npredicted college performance, and a different list of black applicants rank-ordered by their predicted college performance, \nthen work down the list of black applicants until we hit the target for minority enrollment in the hypothetical \u201cadmitted class.\u201d \nWe can see that admission decisions using the race-aware algorithm dominate those from the race-blind or the race-\northogonalized algorithms, in the sense that for any given level of academic performance of the incoming freshman class, the \nrace-aware algorithm lets us substantially increase the share of the admitted class that is African-American. \n \n \n\n\n44 \n \nFigure 3 \nMis-ranking of African-American applicants by race-blind algorithm \n \n \n \n \nSource: Kleinberg, Ludwig, Mullainathan and Rambachan (2018). Using data from the NELS:88 dataset, we first predict for \neach observation their predicted college performance (measured as Y=1 if GPA<2.75) using an algorithm that is blinded to \napplicant race, and then again using an algorithm that has access to each applicant\u2019s race. We then take the sample of black \nstudents in the NELS:88 and use their predicted values to bin them into deciles based on the race-blind predictions (x-axis) and \nrace-aware predictions (y-axis). If the two models rank-ordered everyone the same way, all the data would be along the 45-\ndegree line. The \u201coff diagonals\u201d in the figure show mis-ranking. \n \n \n \n \n\n\n45 \n \nFigure 3 \nPre-trial detention rates in New York City under current human (judge) decisions versus algorithmic \nrelease rule that holds failure to appear (FTA) rate constant at current level \n \n \n \n \n \n \nSource: Kleinberg, Lakkaraju, Leskovec, Ludwig and Mullainathan (2018). \u201cHuman decisions\u201d represent current outcomes \nfrom the existing criminal justice system, which technically combines the input of judges, district attorneys, public defenders \nand a six-item risk check-list created by a local non-profit, the Criminal Justice Agency. \u201cMachine decisions\u201d represent the \nhypothetical detention outcomes if one were to make pre-trial detention versus release decisions using a machine learning \nalgorithm (gradient boosted trees) to predict defendant risk, rank order defendants by risk, and detain only the number of \ndefendants needed to keep the failure to appear rate constant at current levels.  \n \n \n \n \n \n \n \n \n \n0\n10000\n20000\n30000\n40000\n50000\n60000\n70000\n80000\n90000\ntotal\nwhite\nminority\nhuman\nmachine",
        "page_start": 39,
        "page_end": 46,
        "chunk_ids": []
      }
    ],
    "total_pages": 46,
    "processed_at": "2025-11-17T19:35:52.878049",
    "created_at": "2025-11-17T19:35:52.878049"
  },
  "af31a1f9-c4a1-46b7-8561-e04e9c60852f": {
    "id": "af31a1f9-c4a1-46b7-8561-e04e9c60852f",
    "filename": "sparse-moe.pdf",
    "status": "ready",
    "metadata": {
      "title": "sparse-moe.pdf",
      "authors": [],
      "abstract": null,
      "keywords": [],
      "publication_date": null,
      "doi": null
    },
    "sections": [
      {
        "id": "section_0",
        "title": "ABSTRACT",
        "content": "ABSTRACT\nThe capacity of a neural network to absorb information is limited by its number of\nparameters. Conditional computation, where parts of the network are active on a\nper-example basis, has been proposed in theory as a way of dramatically increas-\ning model capacity without a proportional increase in computation. In practice,\nhowever, there are signi\ufb01cant algorithmic and performance challenges. In this\nwork, we address these challenges and \ufb01nally realize the promise of conditional\ncomputation, achieving greater than 1000x improvements in model capacity with\nonly minor losses in computational ef\ufb01ciency on modern GPU clusters. We in-\ntroduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to\nthousands of feed-forward sub-networks. A trainable gating network determines\na sparse combination of these experts to use for each example. We apply the MoE\nto the tasks of language modeling and machine translation, where model capacity\nis critical for absorbing the vast quantities of knowledge available in the training\ncorpora. We present model architectures in which a MoE with up to 137 billion\nparameters is applied convolutionally between stacked LSTM layers. On large\nlanguage modeling and machine translation benchmarks, these models achieve\nsigni\ufb01cantly better results than state-of-the-art at lower computational cost.",
        "page_start": 1,
        "page_end": 1,
        "chunk_ids": []
      },
      {
        "id": "section_1",
        "title": "1\nINTRODUCTION",
        "content": "1\nINTRODUCTION AND RELATED WORK\n1.1\nCONDITIONAL COMPUTATION\nExploiting scale in both training data and model size has been central to the success of deep learn-\ning. When datasets are suf\ufb01ciently large, increasing the capacity (number of parameters) of neural\nnetworks can give much better prediction accuracy. This has been shown in domains such as text\n(Sutskever et al., 2014; Bahdanau et al., 2014; Jozefowicz et al., 2016; Wu et al., 2016), images\n(Krizhevsky et al., 2012; Le et al., 2012), and audio (Hinton et al., 2012; Amodei et al., 2015). For\ntypical deep learning models, where the entire model is activated for every example, this leads to\na roughly quadratic blow-up in training costs, as both the model size and the number of training\nexamples increase. Unfortunately, the advances in computing power and distributed computation\nfall short of meeting such demand.\nVarious forms of conditional computation have been proposed as a way to increase model capacity\nwithout a proportional increase in computational costs (Davis & Arel, 2013; Bengio et al., 2013;\nEigen et al., 2013; Ludovic Denoyer, 2014; Cho & Bengio, 2014; Bengio et al., 2015; Almahairi\net al., 2015). In these schemes, large parts of a network are active or inactive on a per-example\nbasis. The gating decisions may be binary or sparse and continuous, stochastic or deterministic.\nVarious forms of reinforcement learning and back-propagation are proposed for trarining the gating\ndecisions.\n\u2217Equally major contributors\n\u2020Work done as a member of the Google Brain Residency program (g.co/brainresidency)\n1\narXiv:1701.06538v1  [cs.LG]  23 Jan 2017\n\n\nUnder review as a conference paper at ICLR 2017\nFigure 1: A Mixture of Experts (MoE) layer embedded within a recurrent language model. In this\ncase, the sparse gating function selects two experts to perform computations. Their outputs are\nmodulated by the outputs of the gating network.\nWhile these ideas are promising in theory, no work to date has yet demonstrated massive improve-\nments in model capacity, training time, or model quality. We blame this on a combination of the\nfollowing challenges:\n\u2022 Modern computing devices, especially GPUs, are much faster at arithmetic than at branch-\ning. Most of the works above recognize this and propose turning on/off large chunks of the\nnetwork with each gating decision.\n\u2022 Large batch sizes are critical for performance, as they amortize the costs of parameter trans-\nfers and updates. Conditional computation reduces the batch sizes for the conditionally\nactive chunks of the network.\n\u2022 Network bandwidth can be a bottleneck. A cluster of GPUs may have computational power\nthousands of times greater than the aggregate inter-device network bandwidth. To be com-\nputationally ef\ufb01cient, the relative computational versus network demands of an algorithm\nmust exceed this ratio. Embedding layers, which can be seen as a form of conditional com-\nputation, are handicapped by this very problem. Since the embeddings generally need to\nbe sent across the network, the number of (example, parameter) interactions is limited by\nnetwork bandwidth instead of computational capacity.\n\u2022 Depending on the scheme, loss terms may be necessary to achieve the desired level of\nsparsity per-chunk and/or per example. Bengio et al. (2015) use three such terms. These\nissues can affect both model quality and load-balancing.\n\u2022 Model capacity is most critical for very large data sets. The existing literature on condi-\ntional computation deals with relatively small image recognition data sets consisting of up\nto 600,000 images. It is hard to imagine that the labels of these images provide a suf\ufb01cient\nsignal to adequately train a model with millions, let alone billions of parameters.\nIn this work, we for the \ufb01rst time address all of the above challenges and \ufb01nally realize the promise\nof conditional computation. We obtain greater than 1000x improvements in model capacity with\nonly minor losses in computational ef\ufb01ciency and signi\ufb01cantly advance the state-of-the-art results\non public language modeling and translation data sets.\n1.2\nOUR APPROACH: THE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER\nOur approach to conditional computation is to introduce a new type of general purpose neural net-\nwork component: a Sparsely-Gated Mixture-of-Experts Layer (MoE). The MoE consists of a num-\nber of experts, each a simple feed-forward neural network, and a trainable gating network which\nselects a sparse combination of the experts to process each input (see Figure 1). All parts of the\nnetwork are trained jointly by back-propagation.\n2\n\n\nUnder review as a conference paper at ICLR 2017\nWhile the introduced technique is generic, in this paper we focus on language modeling and machine\ntranslation tasks, which are known to bene\ufb01t from very large models. In particular, we apply a MoE\nconvolutionally between stacked LSTM layers (Hochreiter & Schmidhuber, 1997), as in Figure 1.\nThe MoE is called once for each position in the text, selecting a potentially different combination\nof experts at each position. The different experts tend to become highly specialized based on syntax\nand semantics (see Appendix E Table 9). On both language modeling and machine translation\nbenchmarks, we improve on best published results at a fraction of the computational cost.\n1.3\nRELATED WORK ON MIXTURES OF EXPERTS\nSince its introduction more than two decades ago (Jacobs et al., 1991; Jordan & Jacobs, 1994),\nthe mixture-of-experts approach has been the subject of much research. Different types of expert\narchitectures hae been proposed such as SVMs (Collobert et al., 2002), Gaussian Processes (Tresp,\n2001; Theis & Bethge, 2015; Deisenroth & Ng, 2015), Dirichlet Processes (Shahbaba & Neal, 2009),\nand deep networks. Other work has focused on different expert con\ufb01gurations such as a hierarchical\nstructure (Yao et al., 2009), in\ufb01nite numbers of experts (Rasmussen & Ghahramani, 2002), and\nadding experts sequentially (Aljundi et al., 2016). Garmash & Monz (2016) suggest an ensemble\nmodel in the format of mixture of experts for machine translation. The gating network is trained on\na pre-trained ensemble NMT model.\nThe works above concern top-level mixtures of experts. The mixture of experts is the whole model.\nEigen et al. (2013) introduce the idea of using multiple MoEs with their own gating networks as\nparts of a deep model. It is intuitive that the latter approach is more powerful, since complex prob-\nlems may contain many sub-problems each requiring different experts. They also allude in their",
        "page_start": 1,
        "page_end": 3,
        "chunk_ids": []
      },
      {
        "id": "section_2",
        "title": "conclusion",
        "content": "conclusion to the potential to introduce sparsity, turning MoEs into a vehicle for computational\ncomputation.\nOur work builds on this use of MoEs as a general purpose neural network component. While Eigen\net al. (2013) uses two stacked MoEs allowing for two sets of gating decisions, our convolutional\napplication of the MoE allows for different gating decisions at each position in the text. We also\nrealize sparse gating and demonstrate its use as a practical way to massively increase model capacity.\n2\nTHE STRUCTURE OF THE MIXTURE-OF-EXPERTS LAYER\nThe Mixture-of-Experts (MoE) layer consists of a set of n \u201cexpert networks\" E1, \u00b7 \u00b7 \u00b7 , En, and a\n\u201cgating network\" G whose output is a sparse n-dimensional vector. Figure 1 shows an overview\nof the MoE module. The experts are themselves neural networks, each with their own parameters.\nAlthough in principle we only require that the experts accept the same sized inputs and produce the\nsame-sized outputs, in our initial investigations in this paper, we restrict ourselves to the case where\nthe models are feed-forward networks with identical architectures, but with separate parameters.\nLet us denote by G(x) and Ei(x) the output of the gating network and the output of the i-th expert\nnetwork for a given input x. The output y of the MoE module can be written as follows:\ny =\nn\n\ufffd\ni=1\nG(x)iEi(x)\n(1)\nWe save computation based on the sparsity of the output of G(x). Wherever G(x)i = 0, we need not\ncompute Ei(x). In our experiments, we have up to thousands of experts, but only need to evaluate\na handful of them for every example. If the number of experts is very large, we can reduce the\nbranching factor by using a two-level hierarchical MoE. In a hierarchical MoE, a primary gating\nnetwork chooses a sparse weighted combination of \u201cexperts\", each of which is itself a secondary\nmixture-of-experts with its own gating network. In the following we focus on ordinary MoEs. We\nprovide more details on hierarchical MoEs in Appendix B.\nOur implementation is related to other models of conditional computation. A MoE whose experts are\nsimple weight matrices is similar to the parameterized weight matrix proposed in (Cho & Bengio,\n2014). A MoE whose experts have one hidden layer is similar to the block-wise dropout described\nin (Bengio et al., 2015), where the dropped-out layer is sandwiched between fully-activated layers.\n3\n\n\nUnder review as a conference paper at ICLR 2017\n2.1\nGATING NETWORK\nSoftmax Gating:\nA simple choice of non-sparse gating function (Jordan & Jacobs, 1994) is to\nmultiply the input by a trainable weight matrix Wg and then apply the Softmax function.\nG\u03c3(x) = Softmax(x \u00b7 Wg)\n(2)\nNoisy Top-K Gating:\nWe add two components to the Softmax gating network: sparsity and noise.\nBefore taking the softmax function, we add tunable Gaussian noise, then keep only the top k values,\nsetting the rest to \u2212\u221e (which causes the corresponding gate values to equal 0). The sparsity serves\nto save computation, as described above. While this form of sparsity creates some theoretically\nscary discontinuities in the output of gating function, we have not yet observed this to be a problem\nin practice. The noise term helps with load balancing, as will be discussed in Appendix A. The\namount of noise per component is controlled by a second trainable weight matrix Wnoise.\nG(x) = Softmax(KeepTopK(H(x), k))\n(3)\nH(x)i = (x \u00b7 Wg)i + StandardNormal() \u00b7 Softplus((x \u00b7 Wnoise)i)\n(4)\nKeepTopK(v, k)i =\n\ufffdvi\nif vi is in the top k elements of v.\n\u2212\u221e\notherwise.\n(5)\nTraining the Gating Network\nWe train the gating network by simple back-propagation, along\nwith the rest of the model. If we choose k > 1, the gate values for the top k experts have nonzero\nderivatives with respect to the weights of the gating network. This type of occasionally-sensitive\nbehavior is described in (Bengio et al., 2013) with respect to noisy recti\ufb01ers. Gradients also back-\npropagate through the gating network to its inputs. Our method differs here from (Bengio et al.,\n2015) who use boolean gates and a REINFORCE-style approach to train the gating network.\n3\nADDRESSING PERFORMANCE CHALLENGES\n3.1\nTHE SHRINKING BATCH PROBLEM\nOn modern CPUs and GPUs, large batch sizes are necessary for computational ef\ufb01ciency, so as\nto amortize the overhead of parameter loads and updates. If the gating network chooses k out of\nn experts for each example, then for a batch of b examples, each expert receives a much smaller\nbatch of approximately kb\nn \u226a b examples. This causes a naive MoE implementation to become\nvery inef\ufb01cient as the number of experts increases. The solution to this shrinking batch problem is\nto make the original batch size as large as possible. However, batch size tends to be limited by the\nmemory necessary to store activations between the forwards and backwards passes. We propose the\nfollowing techniques for increasing the batch size:\nMixing Data Parallelism and Model Parallelism:\nIn a conventional distributed training setting,\nmultiple copies of the model on different devices asynchronously process distinct batches of data,\nand parameters are synchronized through a set of parameter servers. In our technique, these different\nbatches run synchronously so that they can be combined for the MoE layer. We distribute the\nstandard layers of the model and the gating network according to conventional data-parallel schemes,\nbut keep only one shared copy of each expert. Each expert in the MoE layer receives a combined\nbatch consisting of the relevant examples from all of the data-parallel input batches. The same set\nof devices function as data-parallel replicas (for the standard layers and the gating networks) and\nas model-parallel shards (each hosting a subset of the experts). If the model is distributed over d\ndevices, and each device processes a batch of size b, each expert receives a batch of approximately\nkbd\nn examples. Thus, we achieve a factor of d improvement in expert batch size.\nIn the case of a hierarchical MoE (Section B), the primary gating network employs data parallelism,\nand the secondary MoEs employ model parallelism. Each secondary MoE resides on one device.\n4\n\n\nUnder review as a conference paper at ICLR 2017\nThis technique allows us to increase the number of experts (and hence the number of parameters) by\nproportionally increasing the number of devices in the training cluster. The total batch size increases,\nkeeping the batch size per expert constant. The memory and bandwidth requirements per device also\nremain constant, as do the step times, as does the amount of time necessary to process a number of\ntraining examples equal to the number of parameters in the model. It is our goal to train a trillion-\nparameter model on a trillion-word corpus. We have not scaled our systems this far as of the writing\nof this paper, but it should be possible by adding more hardware.\nTaking Advantage of Convolutionality:\nIn our language models, we apply the same MoE to each\ntime step of the previous layer. If we wait for the previous layer to \ufb01nish, we can apply the MoE\nto all the time steps together as one big batch. Doing so increases the size of the input batch to the\nMoE layer by a factor of the number of unrolled time steps.\nIncreasing Batch Size for a Recurrent MoE:\nWe suspect that even more powerful models may\ninvolve applying a MoE recurrently. For example, the weight matrices of a LSTM or other RNN\ncould be replaced by a MoE. Sadly, such models break the convolutional trick from the last para-\ngraph, since the input to the MoE at one timestep depends on the output of the MoE at the previous\ntimestep. Gruslys et al. (2016) describe a technique for drastically reducing the number of stored\nactivations in an unrolled RNN, at the cost of recomputing forward activations. This would allow\nfor a large increase in batch size.\n3.2\nNETWORK BANDWIDTH\nAnother major performance concern in distributed computing is network bandwidth. Since the ex-\nperts are stationary (see above) and the number of gating parameters is small, most of the communi-\ncation involves sending the inputs and outputs of the experts across the network. To maintain com-\nputational ef\ufb01ciency, the ratio of an expert\u2019s computation to the size of its input and output must ex-\nceed the ratio of computational to network capacity of the computing device. For GPUs, this may be\nthousands to one. In our experiments, we use experts with one hidden layer containing thousands of\nRELU-activated units. Since the weight matrices in the expert have sizes input_size\u00d7hidden_size\nand hidden_size \u00d7 output_size, the ratio of computation to input and output is equal to the size of\nthe hidden layer. Conveniently, we can increase computational ef\ufb01ciency simply by using a larger\nhidden layer, or more hidden layers.\n4\nBALANCING EXPERT UTILIZATION\nWe have observed that the gating network tends to converge to a state where it always produces\nlarge weights for the same few experts. This imbalance is self-reinforcing, as the favored experts\nare trained more rapidly and thus are selected even more by the gating network. Eigen et al. (2013)\ndescribe the same phenomenon, and use a hard constraint at the beginning of training to avoid this\nlocal minimum. Bengio et al. (2015) include a soft constraint on the batch-wise average of each\ngate.1\nWe take a soft constraint approach. We de\ufb01ne the importance of an expert relative to a batch of\ntraining examples to be the batchwise sum of the gate values for that expert. We de\ufb01ne an additional\nloss Limportance, which is added to the overall loss function for the model. This loss is equal to\nthe square of the coef\ufb01cient of variation of the set of importance values, multiplied by a hand-tuned\nscaling factor wimportance. This additional loss encourages all experts to have equal importance.\nImportance(X) =\n\ufffd\nx\u2208X\nG(x)\n(6)\nLimportance(X) = wimportance \u00b7 CV (Importance(X))2\n(7)\n1Bengio et al. (2015) also include two additional losses. One controls per-example sparsity, which we do\nnot need since it is enforced by the \ufb01xed value of k. A third loss encourages diversity of gate values. In our\nexperiments, we \ufb01nd that the gate values naturally diversify as the experts specialize (in a virtuous cycle), and\nwe do not need to enforce diversity of gate values.\n5\n\n\nUnder review as a conference paper at ICLR 2017\nWhile this loss function can ensure equal importance, experts may still receive very different num-\nbers of examples. For example, one expert may receive a few examples with large weights, and\nanother may receive many examples with small weights. This can cause memory and performance\nproblems on distributed hardware. To solve this problem, we introduce a second loss function,\nLload , which ensures balanced loads. Appendix A contains the de\ufb01nition of this function, along\nwith experimental results.",
        "page_start": 3,
        "page_end": 6,
        "chunk_ids": []
      },
      {
        "id": "section_3",
        "title": "5\nEXPERIMENTS",
        "content": "5\nEXPERIMENTS\n5.1\n1 BILLION WORD LANGUAGE MODELING BENCHMARK\nDataset:\nThis dataset, introduced by (Chelba et al., 2013) consists of shuf\ufb02ed unique sentences\nfrom news articles, totaling approximately 829 million words, with a vocabulary of 793,471 words.\nPrevious State-of-the-Art:\nThe best previously published results (Jozefowicz et al., 2016) use\nmodels consisting of one or more stacked Long Short-Term Memory (LSTM) layers (Hochreiter\n& Schmidhuber, 1997; Gers et al., 2000). The number of parameters in the LSTM layers of these\nmodels vary from 2 million to 151 million. Quality increases greatly with parameter count, as do\ncomputational costs. Results for these models form the top line of Figure 2-right.\nMoE Models:\nOur models consist of two stacked LSTM layers with a MoE layer between them\n(see Figure 1). We vary the sizes of the layers and the number of experts. For full details on model\narchitecture, training regimen, additional baselines and results, see Appendix C.\nLow Computation, Varied Capacity:\nTo investigate the effects of adding capacity, we trained\na series of MoE models all with roughly equal computational costs: about 8 million multiply-and-\nadds per training example per timestep in the forwards pass, excluding the softmax layer. We call\nthis metric (ops/timestep). We trained models with \ufb02at MoEs containing 4, 32, and 256 experts, and\nmodels with hierarchical MoEs containing 256, 1024, and 4096 experts. Each expert had about 1\nmillion parameters. For all the MoE layers, 4 experts were active per input.\nThe results of these models are shown in Figure 2-left. The model with 4 always-active experts\nperformed (unsurprisingly) similarly to the computationally-matched baseline models, while the\nlargest of the models (4096 experts) achieved an impressive 24% lower perplexity on the test set.\nFigure 2: Model comparison on 1-Billion-Word Language-Modeling Benchmark. On the left, we\nplot test perplexity as a function of model capacity for models with similar computational budgets\nof approximately 8-million-ops-per-timestep. On the right, we plot test perplexity as a function of\ncomputational budget. The top line represents the LSTM models from (Jozefowicz et al., 2016).\nThe bottom line represents 4-billion parameter MoE models with different computational budgets.\nVaried Computation, High Capacity:\nIn addition to the largest model from the previous section,\nwe trained two more MoE models with similarly high capacity (4 billion parameters), but higher\ncomputation budgets. These models had larger LSTMs, and fewer but larger and experts. Details\n6\n\n\nUnder review as a conference paper at ICLR 2017\nTable 1: Summary of high-capacity MoE-augmented models with varying computational budgets,\nvs. best previously published results (Jozefowicz et al., 2016). Details in Appendix C.\nTest\nTest\n#Parameters\nops/timestep\nTraining\nTFLOPS\nPerplexity\nPerplexity\nexcluding embedding\nTime\n/GPU\n10 epochs 100 epochs\nand softmax layers\n10 epochs\nBest Published Results\n34.7\n30.6\n151 million\n151 million\n59 hours, 32 k40s\n1.09\nLow-Budget MoE Model\n34.1\n4303 million\n8.9 million\n15 hours, 16 k40s\n0.74\nMedium-Budget MoE Model\n31.3\n4313 million\n33.8 million\n17 hours, 32 k40s\n1.22\nHigh-Budget MoE Model\n28.0\n4371 million\n142.7 million 47 hours, 32 k40s\n1.56\ncan be found in Appendix C.2. Results of these three models form the bottom line of Figure 2-right.\nTable 1 compares the results of these models to the best previously-published result on this dataset .\nEven the fastest of these models beats the best published result (when controlling for the number of\ntraining epochs), despite requiring only 6% of the computation.\nComputational Ef\ufb01ciency:\nWe trained our models using TensorFlow (Abadi et al., 2016) on clus-\nters containing 16-32 Tesla K40 GPUs. For each of our models, we determine computational ef\ufb01-\nciency in TFLOPS/GPU by dividing the number of \ufb02oating point operations required to process\none training batch by the observed step time and the number of GPUs in the cluster. The operation\ncounts used here are higher than the ones we report in our ops/timestep numbers in that we include\nthe backwards pass, we include the importance-sampling-based training of the softmax layer, and\nwe count a multiply-and-add as two separate operations. For all of our MoE models, the \ufb02oating\npoint operations involved in the experts represent between 37% and 46% of the total.\nFor our baseline models wtih no MoE, observed computational ef\ufb01ciency ranged from 1.07-1.29\nTFLOPS/GPU. For our low-computation MoE models, computation ef\ufb01ciency ranged from 0.74-\n0.90 TFLOPS/GPU, except for the 4-expert model which did not make full use of the available\nparallelism. Our highest-computation MoE model was more ef\ufb01cient at 1.56 TFLOPS/GPU, likely\ndue to the larger matrices. These numbers represent a signi\ufb01cant fraction of the theoretical maximum\nof 4.29 TFLOPS/GPU claimed by NVIDIA. Detailed results are in Appendix C, Table 7.\n5.2\n100 BILLION WORD GOOGLE NEWS CORPUS\nFigure 3: Language modeling on a 100 billion word corpus. Models have similar computational\nbudgets (8 million ops/timestep).\nOn the 1-billion-word corpus, adding additional capacity seems to produce diminishing returns as\nthe number of parameters in the MoE layer exceeds 1 billion, as can be seen in Figure 2-left. We\nhypothesized that for a larger training set, even higher capacities would produce signi\ufb01cant quality\nimprovements.\nWe constructed a similar training set consisting of shuf\ufb02ed unique sentences from Google\u2019s internal\nnews corpus, totalling roughly 100 billion words. Similarly to the previous section, we tested a\nseries of models with similar computational costs of about 8 million ops/timestep. In addition to a\nbaseline LSTM model, we trained models augmented with MoE layers containing 32, 256, 1024,\n7\n\n\nUnder review as a conference paper at ICLR 2017\n4096, 16384, 65536, and 131072 experts. This corresponds to up to 137 billion parameters in the\nMoE layer. Details on architecture, training, and results are given in Appendix D.",
        "page_start": 6,
        "page_end": 8,
        "chunk_ids": []
      },
      {
        "id": "section_4",
        "title": "Results",
        "content": "Results:\nFigure 3 shows test perplexity as a function of capacity after training on 10 billion words\n(top line) and 100 billion words (bottom line). When training over the full 100 billion words, test\nperplexity improves signi\ufb01cantly up to 65536 experts (68 billion parameters), dropping 39% lower\nthan the computationally matched baseline, but degrades at 131072 experts, possibly a result of too\nmuch sparsity. The widening gap between the two lines demonstrates (unsurprisingly) that increased\nmodel capacity helps more on larger training sets.\nEven at 65536 experts (99.994% layer sparsity), computational ef\ufb01ciency for the model stays at a\nrespectable 0.72 TFLOPS/GPU.\n5.3\nMACHINE TRANSLATION (SINGLE LANGUAGE PAIR)\nModel Architecture:\nOur model was a modi\ufb01ed version of the GNMT model described in (Wu\net al., 2016). To reduce computation, we decreased the number of LSTM layers in the encoder\nand decoder from 9 and 8 to 3 and 2 respectively. We inserted MoE layers in both the encoder\n(between layers 2 and 3) and the decoder (between layers 1 and 2). Each MoE layer contained up\nto 2048 experts each with about two million parameters, adding a total of about 8 billion parameters\nto the models. Further details on model architecture, testing procedure and results can be found in\nAppendix E.\nDatasets:\nWe benchmarked our method on the WMT\u201914 En\u2192Fr and En\u2192De corpora, whose\ntraining sets have 36M sentence pairs and 5M sentence pairs, respectively. The experimental proto-\ncols were also similar to those in (Wu et al., 2016): newstest2014 was used as the test set to compare\nagainst previous work (Luong et al., 2015a; Zhou et al., 2016; Wu et al., 2016), while the combina-\ntion of newstest2012 and newstest2013 was used as the development set. We also tested the same\nmodel on a Google\u2019s Production English to French data.\nTable 2: Results on WMT\u201914 En\u2192 Fr newstest2014 (bold values represent best results).\nModel\nTest\nTest\nops/timenstep\nTotal\nTraining\nPerplexity BLEU\n#Parameters\nTime\nMoE with 2048 Experts\n2.69\n40.35\n85M\n8.7B\n3 days/64 k40s\nMoE with 2048 Experts (longer training)\n2.63\n40.56\n85M\n8.7B\n6 days/64 k40s\nGNMT (Wu et al., 2016)\n2.79\n39.22\n214M\n278M\n6 days/96 k80s\nGNMT+RL (Wu et al., 2016)\n2.96\n39.92\n214M\n278M\n6 days/96 k80s\nPBMT (Durrani et al., 2014)\n37.0\nLSTM (6-layer) (Luong et al., 2015b)\n31.5\nLSTM (6-layer+PosUnk) (Luong et al., 2015b)\n33.1\nDeepAtt (Zhou et al., 2016)\n37.7\nDeepAtt+PosUnk (Zhou et al., 2016)\n39.2\nTable 3: Results on WMT\u201914 En \u2192 De newstest2014 (bold values represent best results).\nModel\nTest\nTest\nops/timestep\nTotal\nTraining\nPerplexity\nBLEU\n#Parameters\nTime\nMoE with 2048 Experts\n4.64\n26.03\n85M\n8.7B\n1 day/64 k40s\nGNMT (Wu et al., 2016)\n5.25\n24.91\n214M\n278M\n1 day/96 k80s\nGNMT +RL (Wu et al., 2016)\n8.08\n24.66\n214M\n278M\n1 day/96 k80s\nPBMT (Durrani et al., 2014)\n20.7\nDeepAtt (Zhou et al., 2016)\n20.6\nTable 4: Results on the Google Production En\u2192 Fr dataset (bold values represent best results).\nModel\nEval\nEval\nTest\nTest\nops/timestep\nTotal\nTraining\nPerplexity\nBLEU\nPerplexity\nBLEU\n#Parameters\nTime\nMoE with 2048 Experts\n2.60\n37.27\n2.69\n36.57\n85M\n8.7B\n1 day/64 k40s\nGNMT (Wu et al., 2016)\n2.78\n35.80\n2.87\n35.56\n214M\n278M\n6 days/96 k80s\n8\n\n\nUnder review as a conference paper at ICLR 2017",
        "page_start": 8,
        "page_end": 9,
        "chunk_ids": []
      },
      {
        "id": "section_5",
        "title": "Results",
        "content": "Results:\nTables 2, 3, and 4 show the results of our largest models, compared with published",
        "page_start": 9,
        "page_end": 9,
        "chunk_ids": []
      },
      {
        "id": "section_6",
        "title": "results",
        "content": "results. Our approach achieved BLEU scores of 40.56 and 26.03 on the WMT\u201914 En\u2192Fr and\nEn\u2192De benchmarks. As our models did not use RL re\ufb01nement, these results constitute signi\ufb01cant\ngains of 1.34 and 1.12 BLEU score on top of the strong baselines in (Wu et al., 2016). The perplexity\nscores are also better.2 On the Google Production dataset, our model achieved 1.01 higher test BLEU\nscore even after training for only one sixth of the time.\n5.4\nMULTILINGUAL MACHINE TRANSLATION\nDataset:\n(Johnson et al., 2016) train a single GNMT (Wu et al., 2016) model on a very large com-\nbined dataset of twelve language pairs. Results are somewhat worse than those for 12 separately\ntrained single-pair GNMT models. This is not surprising, given that the twelve models have 12\ntimes the capacity and twelve times the aggregate training of the one model. We repeat this ex-\nperiment with a single MoE-augmented model. See Appendix E for details on model architecture.\nWe train our model on the same dataset as (Johnson et al., 2016) and process the same number of\ntraining examples (about 3 billion sentence pairs). Our training time was shorter due to the lower\ncomputational budget of our model.",
        "page_start": 9,
        "page_end": 9,
        "chunk_ids": []
      },
      {
        "id": "section_7",
        "title": "Results",
        "content": "Results:",
        "page_start": 9,
        "page_end": 9,
        "chunk_ids": []
      },
      {
        "id": "section_8",
        "title": "Results",
        "content": "Results for the single-pair GNMT models, the multilingual GNMT model and the mul-\ntilingual MoE model are given in Table 5. The MoE model achieves 19% lower perplexity on the\ndev set than the multilingual GNMT model. On BLEU score, the MoE model signi\ufb01cantly beats\nthe multilingual GNMT model on 11 of the 12 language pairs (by as much as 5.84 points), and even\nbeats the monolingual GNMT models on 8 of 12 language pairs. The poor performance on English\n\u2192 Korean seems to be a result of severe overtraining, as for the rarer language pairs a small number\nof real examples were highly oversampled in the training corpus.\nTable 5: Multilingual Machine Translation (bold values represent best results).\nGNMT-Mono\nGNMT-Multi\nMoE-Multi\nMoE-Multi vs.\nGNMT-Multi\nParameters 278M / model\n278M\n8.7B\nops/timestep\n212M\n212M\n102M\ntraining time, hardware\nvarious\n21 days, 96 k20s 12 days, 64 k40s\nPerplexity (dev)\n4.14\n3.35\n-19%\nFrench \u2192 English Test BLEU\n36.47\n34.40\n37.46\n+3.06\nGerman \u2192 English Test BLEU\n31.77\n31.17\n34.80\n+3.63\nJapanese \u2192 English Test BLEU\n23.41\n21.62\n25.91\n+4.29\nKorean \u2192 English Test BLEU\n25.42\n22.87\n28.71\n+5.84\nPortuguese \u2192 English Test BLEU\n44.40\n42.53\n46.13\n+3.60\nSpanish \u2192 English Test BLEU\n38.00\n36.04\n39.39\n+3.35\nEnglish \u2192 French Test BLEU\n35.37\n34.00\n36.59\n+2.59\nEnglish \u2192 German Test BLEU\n26.43\n23.15\n24.53\n+1.38\nEnglish \u2192 Japanese Test BLEU\n23.66\n21.10\n22.78\n+1.68\nEnglish \u2192 Korean Test BLEU\n19.75\n18.41\n16.62\n-1.79\nEnglish \u2192 Portuguese Test BLEU\n38.40\n37.35\n37.90\n+0.55\nEnglish \u2192 Spanish Test BLEU\n34.50\n34.25\n36.21\n+1.96",
        "page_start": 9,
        "page_end": 9,
        "chunk_ids": []
      },
      {
        "id": "section_9",
        "title": "6\nCONCLUSION",
        "content": "6\nCONCLUSION\nThis work is the \ufb01rst to demonstrate major wins from conditional computation in deep networks.\nWe carefully identi\ufb01ed the design considerations and challenges of conditional computing and ad-\ndressed them with a combination of algorithmic and engineering solutions. While we focused on\ntext, conditional computation may help in other domains as well, provided suf\ufb01ciently large train-\ning sets. We look forward to seeing many novel implementations and applications of conditional\ncomputation in the years to come.\nACKNOWLEDGMENTS\nWe would like to thank all of the members of the Google Brain and Google Translate teams who\nhelped us with this project, in particular Zhifeng Chen, Yonghui Wu, and Melvin Johnson. Thanks\nalso to our anonymous ICLR reviewers for the helpful suggestions on making this paper better.\n2Reported perplexities relative to the tokenization used by both our models and GNMT.\n9\n\n\nUnder review as a conference paper at ICLR 2017\nREFERENCES\nMart\u00edn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Gre-\ngory S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian J. Good-\nfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal J\u00f3zefowicz, Lukasz\nKaiser, Manjunath Kudlur, Josh Levenberg, Dan Man\u00e9, Rajat Monga, Sherry Moore, Derek Gor-\ndon Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal\nTalwar, Paul A. Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda B. Vi\u00e9gas, Oriol Vinyals,\nPete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensor\ufb02ow:\nLarge-scale machine learning on heterogeneous distributed systems.\nCoRR, abs/1603.04467,\n2016. URL http://arxiv.org/abs/1603.04467.\nRahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars. Expert gate: Lifelong learning with a\nnetwork of experts. CoRR, abs/1611.06194, 2016. URL http://arxiv.org/abs/1611.\n06194.\nA. Almahairi, N. Ballas, T. Cooijmans, Y. Zheng, H. Larochelle, and A. Courville. Dynamic Capac-\nity Networks. ArXiv e-prints, November 2015.\nDario Amodei, Rishita Anubhai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jing-\ndong Chen, Mike Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse Engel, Linxi\nFan, Christopher Fougner, Tony Han, Awni Y. Hannun, Billy Jun, Patrick LeGresley, Libby Lin,\nSharan Narang, Andrew Y. Ng, Sherjil Ozair, Ryan Prenger, Jonathan Raiman, Sanjeev Satheesh,\nDavid Seetapun, Shubho Sengupta, Yi Wang, Zhiqian Wang, Chong Wang, Bo Xiao, Dani Yo-\ngatama, Jun Zhan, and Zhenyao Zhu. Deep speech 2: End-to-end speech recognition in english\nand mandarin. arXiv preprint arXiv:1512.02595, 2015.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau, and Doina Precup. Conditional computation\nin neural networks for faster models. arXiv preprint arXiv:1511.06297, 2015.\nYoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville.\nEstimating or propagating gradients\nthrough stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony\nRobinson. One billion word benchmark for measuring progress in statistical language modeling.\narXiv preprint arXiv:1312.3005, 2013.\nK. Cho and Y. Bengio. Exponentially Increasing the Capacity-to-Computation Ratio for Conditional\nComputation in Deep Learning. ArXiv e-prints, June 2014.\nRonan Collobert, Samy Bengio, and Yoshua Bengio. A parallel mixture of SVMs for very large\nscale problems. Neural Computing, 2002.\nAndrew Davis and Itamar Arel. Low-rank approximations for conditional feedforward computation\nin deep neural networks. arXiv preprint arXiv:1312.4461, 2013.\nMarc Peter Deisenroth and Jun Wei Ng. Distributed Gaussian processes. In ICML, 2015.\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and\nstochastic optimization, 2010.\nNadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Hea\ufb01eld. Edinburgh\u2019s phrase-based\nmachine translation systems for wmt-14. In Proceedings of the Ninth Workshop on Statistical\nMachine Translation, 2014.\nDavid Eigen, Marc\u2019Aurelio Ranzato, and Ilya Sutskever. Learning factored representations in a deep\nmixture of experts. arXiv preprint arXiv:1312.4314, 2013.\nEkaterina Garmash and Christof Monz. Ensemble learning for multi-source neural machine transla-\ntion. In staff.science.uva.nl/c.monz, 2016.\n10\n\n\nUnder review as a conference paper at ICLR 2017\nFelix A. Gers, J\u00fcrgen A. Schmidhuber, and Fred A. Cummins. Learning to forget: Continual pre-\ndiction with lstm. Neural Computation, 2000.\nAudrunas Gruslys, R\u00e9mi Munos, Ivo Danihelka, Marc Lanctot, and Alex Graves. Memory-ef\ufb01cient\nbackpropagation through time. CoRR, abs/1606.03401, 2016. URL http://arxiv.org/\nabs/1606.03401.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. IEEE Conference on Computer Vision and Pattern Recognition, 2015.\nGeoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,\nAndrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, et al. Deep neural networks\nfor acoustic modeling in speech recognition: The shared views of four research groups. IEEE\nSignal Processing Magazine, 2012.\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 1997.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\nRobert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures\nof local experts. Neural Computing, 1991.\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil\nThorat, Fernanda B. Vi\u00e9gas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey\nDean. Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation.\nCoRR, abs/1611.04558, 2016. URL http://arxiv.org/abs/1611.04558.\nMichael I. Jordan and Robert A. Jacobs. Hierarchical mixtures of experts and the EM algorithm.\nNeural Computing, 1994.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the\nlimits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\nReinhard Kneser and Hermann. Ney. Improved backingoff for m-gram language modeling., 1995.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classi\ufb01cation with deep convo-\nlutional neural networks. In NIPS, 2012.\nQuoc V. Le, Marc\u2019Aurelio Ranzato, Rajat Monga, Matthieu Devin, Kai Chen, Greg S. Corrado,\nJeffrey Dean, and Andrew Y. Ng. Building high-level features using large scale unsupervised\nlearning. In ICML, 2012.\nPatrick Gallinari Ludovic Denoyer.\nDeep sequential neural network.\narXiv preprint\narXiv:1410.0510, 2014.\nMinh-Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-\nbased neural machine translation. EMNLP, 2015a.\nMinh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. Addressing\nthe rare word problem in neural machine translation. ACL, 2015b.\nCarl Edward Rasmussen and Zoubin Ghahramani. In\ufb01nite mixtures of Gaussian process experts.\nNIPS, 2002.\nHasim Sak, Andrew W Senior, and Fran\u00e7oise Beaufays. Long short-term memory recurrent neural\nnetwork architectures for large scale acoustic modeling. In INTERSPEECH, pp. 338\u2013342, 2014.\nMike Schuster and Kaisuke Nakajima. Japanese and Korean voice search. ICASSP, 2012.\nBabak Shahbaba and Radford Neal. Nonlinear models using dirichlet process mixtures. JMLR,\n2009.\n11\n\n\nUnder review as a conference paper at ICLR 2017\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks.\nIn NIPS, 2014.\nLucas Theis and Matthias Bethge. Generative image modeling using spatial LSTMs. In NIPS, 2015.\nVolker Tresp. Mixtures of Gaussian Processes. In NIPS, 2001.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, \u0141ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa,\nKeith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa,\nAlex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google\u2019s neural\nmachine translation system: Bridging the gap between human and machine translation. arXiv\npreprint arXiv:1609.08144, 2016.\nBangpeng Yao, Dirk Walther, Diane Beck, and Li Fei-fei. Hierarchical mixture of classi\ufb01cation\nexperts uncovers interactions between brain regions. In NIPS. 2009.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329, 2014.\nJie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with fast-forward\nconnections for neural machine translation. arXiv preprint arXiv:1606.04199, 2016.\n12\n\n\nUnder review as a conference paper at ICLR 2017\nAPPENDICES\nA\nLOAD-BALANCING LOSS\nAs discussed in section 4, for load-balancing purposes, we want to de\ufb01ne an additional loss function\nto encourage experts to receive roughly equal numbers of training examples. Unfortunately, the\nnumber of examples received by an expert is a discrete quantity, so it can not be used in back-\npropagation. Instead, we de\ufb01ne a smooth estimator Load(X) of the number of examples assigned to\neach expert for a batch X of inputs. The smoothness allows us to back-propagate gradients through\nthe estimator. This is the purpose of the noise term in the gating function. We de\ufb01ne P(x, i) as the\nprobability that G(x)i is nonzero, given a new random choice of noise on element i, but keeping\nthe already-sampled choices of noise on the other elements. To compute P(x, i), we note that the\nG(x)i is nonzero if and only if H(x)i is greater than the kth-greatest element of H(x) excluding\nitself. The probability works out to be:\nP(x, i) = Pr\n\ufffd\n(x \u00b7 Wg)i + StandardNormal() \u00b7 Softplus((x \u00b7 Wnoise)i)\n> kth_excluding(H(x), k, i)\n\ufffd\n(8)\nWhere kth_excluding(v, k, i) means the kth highest component of v, excluding component i. Sim-\nplifying, we get:\nP(x, i) = \u03a6\n\ufffd(x \u00b7 Wg)i \u2212 kth_excluding(H(x), k, i)\nSoftplus((x \u00b7 Wnoise)i)\n\ufffd\n(9)\nWhere \u03a6 is the CDF of the standard normal distribution.\nLoad(X)i =\n\ufffd\nx\u2208X\nP(x, i)\n(10)\nWe can now de\ufb01ne the load loss to be the square of the coef\ufb01cient of variation of the load vector,\nmultiplied by a hand-tuned scaling factor wload.\nLload(X) = wload \u00b7 CV (Load(X))2\n(11)\nInitial Load Imbalance:\nTo avoid out-of-memory errors, we need to initialize the network in a\nstate of approximately equal expert load (since the soft constraints need some time to work). To\naccomplish this, we initialize the matrices Wg and Wnoise to all zeros, which yields no signal and\nsome noise.\nExperiments:\nWe trained a set of models with identical architecture (the MoE-256 model de-\nscribed in Appendix C), using different values of wimportance and wload. We trained each model for\n10 epochs, then measured perplexity on the test set. We also measured the coef\ufb01cients of variation\nin Importance and Load, as well as ratio of the load on the most overloaded expert to the average\nload. This last value is signi\ufb01cant for load balancing purposes on distributed hardware. All of these\nmetrics were averaged over several training batches.\nTable 6: Experiments with different combinations of losses.\nwimportance wload Test Perplexity CV (Importance(X)) CV (Load(X))\nmax(Load(X))\nmean(Load(X))\n0.0\n0.0\n39.8\n3.04\n3.01\n17.80\n0.2\n0.0\n35.6\n0.06\n0.17\n1.47\n0.0\n0.2\n35.7\n0.22\n0.04\n1.15\n0.1\n0.1\n35.6\n0.06\n0.05\n1.14\n0.01\n0.01\n35.7\n0.48\n0.11\n1.37\n1.0\n1.0\n35.7\n0.03\n0.02\n1.07\n13\n\n\nUnder review as a conference paper at ICLR 2017",
        "page_start": 9,
        "page_end": 14,
        "chunk_ids": []
      },
      {
        "id": "section_10",
        "title": "Results",
        "content": "Results:",
        "page_start": 14,
        "page_end": 14,
        "chunk_ids": []
      },
      {
        "id": "section_11",
        "title": "Results",
        "content": "Results are reported in Table 6. All the combinations containing at least one the two\nlosses led to very similar model quality, where having no loss was much worse. Models with higher\nvalues of wload had lower loads on the most overloaded expert.\nB\nHIERACHICAL MIXTURE OF EXPERTS\nIf the number of experts is very large, we can reduce the branching factor by using a two-level\nhierarchical MoE. In a hierarchical MoE, a primary gating network chooses a sparse weighted com-\nbination of \u201cexperts\", each of which is itself a secondary mixture-of-experts with its own gating\nnetwork.3 If the hierarchical MoE consists of a groups of b experts each, we denote the primary gat-\ning network by Gprimary, the secondary gating networks by (G1, G2..Ga), and the expert networks\nby (E0,0, E0,1..Ea,b). The output of the MoE is given by:\nyH =\na\n\ufffd\ni=1\nb\n\ufffd\nj=1\nGprimary(x)i \u00b7 Gi(x)j \u00b7 Ei,j(x)\n(12)\nOur metrics of expert utilization change to the following:\nImportanceH(X)i,j =\n\ufffd\nx\u2208X\nGprimary(x)i \u00b7 Gi(x)j\n(13)\nLoadH(X)i,j = Loadprimary(X)i \u00b7 Loadi(X(i))j\n|X(i)|\n(14)\nLoadprimary and Loadi deonte the Load functions for the primary gating network and ith sec-\nondary gating network respectively. X(i) denotes the subset of X for which Gprimary(x)i > 0.\nIt would seem simpler to let LoadH(X)i,j = Loadi(Xi)j , but this would not have a gradient with\nrespect to the primary gating network, so we use the formulation above.\nC\n1 BILLION WORD LANGUAGE MODELING BENCHMARK - EXPERIMENTAL DETAILS\nC.1\n8-MILLION-OPERATIONS-PER-TIMESTEP MODELS\nModel Architecture:\nOur model consists of \ufb01ve layers: a word embedding layer, a recurrent\nLong Short-Term Memory (LSTM) layer (Hochreiter & Schmidhuber, 1997; Gers et al., 2000), a\nMoE layer, a second LSTM layer, and a softmax layer. The dimensionality of the embedding layer,\nthe number of units in each LSTM layer, and the input and output dimensionality of the MoE layer\nare all equal to 512. For every layer other than the softmax, we apply drouput (Zaremba et al.,\n2014) to the layer output, dropping each activation with probability DropProb, otherwise dividing\nby (1 \u2212 DropProb). After dropout, the output of the previous layer is added to the layer output.\nThis residual connection encourages gradient \ufb02ow (He et al., 2015).\nMoE Layer Architecture:\nEach expert in the MoE layer is a feed forward network with one\nReLU-activated hidden layer of size 1024 and an output layer of size 512. Thus, each expert contains\n[512 \u2217 1024] + [1024 \u2217 512] = 1M parameters. The output of the MoE layer is passed through a\nsigmoid function before dropout. We varied the number of experts between models, using ordinary\nMoE layers with 4, 32 and 256 experts and hierarchical MoE layers with 256, 1024 and 4096 experts.\nWe call the resulting models MoE-4, MoE-32, MoE-256, MoE-256-h, MoE-1024-h and MoE-4096-\nh. For the hierarchical MoE layers, the \ufb01rst level branching factor was 16, corresponding to the\nnumber of GPUs in our cluster. We use Noisy-Top-K Gating (see Section 2.1) with k = 4 for the\nordinary MoE layers and k = 2 at each level of the hierarchical MoE layers. Thus, each example is\nprocessed by exactly 4 experts for a total of 4M ops/timestep. The two LSTM layers contribute 2M\nops/timestep each for the desired total of 8M.\n3 We have not found the need for deeper hierarchies.\n14\n\n\nUnder review as a conference paper at ICLR 2017\nComputationally-Matched Baselines:\nThe MoE-4 model does not employ sparsity, since all 4\nexperts are always used. In addition, we trained four more computationally-matched baseline models\nwith no sparsity:\n\u2022 MoE-1-Wide: The MoE layer consists of a single \"expert\" containing one ReLU-activated\nhidden layer of size 4096.\n\u2022 MoE-1-Deep: The MoE layer consists of a single \"expert\" containing four ReLU-activated\nhidden layers, each with size 1024.\n\u2022 4xLSTM-512: We replace the MoE layer with two additional 512-unit LSTM layers.\n\u2022 LSTM-2048-512: The model contains one 2048-unit LSTM layer (and no MoE). The out-\nput of the LSTM is projected down to 512 dimensions (Sak et al., 2014). The next timestep\nof the LSTM receives the projected output. This is identical to one of the models published\nin (Jozefowicz et al., 2016). We re-ran it to account for differences in training regimen, and\nobtained results very similar to the published ones.\nTraining:\nThe models were trained on a cluster of 16 K40 GPUs using the synchronous method\ndescribed in Section 3. Each batch consisted of a set of sentences totaling roughly 300,000 words. In\nthe interest of time, we limited training to 10 epochs, (27,000 steps). Training took 12-16 hours for\nall models, except for MoE-4, which took 18 hours (since all the expert computation was performed\non only 4 of 16 GPUs). We used the Adam optimizer (Kingma & Ba, 2015). The base learning\nrate was increased linearly for the \ufb01rst 1000 training steps, and decreased after that so as to be\nproportional to the inverse square root of the step number. The Softmax output layer was trained\nef\ufb01ciently using importance sampling similarly to the models in (Jozefowicz et al., 2016). For each\nmodel, we performed a hyper-parmeter search to \ufb01nd the best dropout probability, in increments of\n0.1.\nTo ensure balanced expert utilization we set wimportance = 0.1 and wload = 0.1, as described in\nSection 4 and Appendix A.",
        "page_start": 14,
        "page_end": 15,
        "chunk_ids": []
      },
      {
        "id": "section_12",
        "title": "Results",
        "content": "Results:\nWe evaluate our model using perplexity on the holdout dataset, used by (Chelba et al.,\n2013; Jozefowicz et al., 2016). We follow the standard procedure and sum over all the words in-\ncluding the end of sentence symbol. Results are reported in Table 7. For each model, we report\nthe test perplexity, the computational budget, the parameter counts, the value of DropProb, and the\ncomputational ef\ufb01ciency.\nTable 7: Model comparison on 1 Billion Word Language Modeling Benchmark. Models marked\nwith * are from (Jozefowicz et al., 2016).\nModel\nTest\nTest\nops/timestep #Params excluding\nTotal\nDrop-\nTFLOPS\nPerplexity Perplexity\n(millions)\nembed. & softmax\n#Params\nProb\nper GPU\n10 epochs\n(\ufb01nal)\n(millions)\n(billions)\n(observed)\nKneser-Ney 5-gram*\n67.6\n0.00001\n1.8\nLSTM-512-512*\n54.1\n2.4\n2.4\n0.8\n0.1\nLSTM-1024-512*\n48.2\n4.7\n4.7\n0.8\n0.1\nLSTM-2048-512*\n45.0\n43.7\n9.4\n9.4\n0.8\n0.1\n0.61\nLSTM-2048-512\n44.7\n9.4\n9.4\n0.8\n0.1\n1.21\n4xLSTM-512\n46.0\n8.4\n8.4\n0.8\n0.1\n1.07\nMoE-1-Wide\n46.1\n8.4\n8.4\n0.8\n0.1\n1.29\nMoE-1-Deep\n45.7\n8.4\n8.4\n0.8\n0.1\n1.29\nMoE-4\n45.0\n8.4\n8.4\n0.8\n0.1\n0.52\nMoE-32\n39.7\n8.4\n37.8\n0.9\n0.1\n0.87\nMoE-256\n35.7\n8.6\n272.9\n1.1\n0.1\n0.81\nMoE-256-h\n36.0\n8.4\n272.9\n1.1\n0.1\n0.89\nMoE-1024-h\n34.6\n8.5\n1079.0\n1.9\n0.2\n0.90\nMoE-4096-h\n34.1\n8.9\n4303.4\n5.1\n0.2\n0.74\n2xLSTM-8192-1024*\n34.7\n30.6\n151.0\n151.0\n1.8\n0.25\n1.09\nMoE-34M\n31.3\n33.8\n4313.9\n6.0\n0.3\n1.22\nMoE-143M\n28.0\n142.7\n4371.1\n6.0\n0.4\n1.56\n15\n\n\nUnder review as a conference paper at ICLR 2017\nC.2\nMORE EXPENSIVE MODELS\nWe ran two additional models (MoE-34M and MoE-143M) to investigate the effects of adding more\ncomputation in the presence of a large MoE layer. These models have computation budgets of 34M\nand 143M ops/timestep. Similar to the models above, these models use a MoE layer between two\nLSTM layers. The dimensionality of the embedding layer, and the input and output dimensionality\nof the MoE layer are set to 1024 instead of 512. For MoE-34M, the LSTM layers have 1024 units.\nFor MoE-143M, the LSTM layers have 4096 units and an output projection of size 1024 (Sak et al.,\n2014). MoE-34M uses a hierarchical MoE layer with 1024 experts, each with a hidden layer of size\n2048. MoE-143M uses a hierarchical MoE layer with 256 experts, each with a hidden layer of size\n8192. Both models have 4B parameters in the MoE layers. We searched for the best DropProb for\neach model, and trained each model for 10 epochs.\nThe two models achieved test perplexity of 31.3 and 28.0 respectively, showing that even in the\npresence of a large MoE, more computation is still useful. Results are reported at the bottom of\nTable 7. The larger of the two models has a similar computational budget to the best published\nmodel from the literature, and training times are similar. Comparing after 10 epochs, our model has\na lower test perplexity by 18%.\nD\n100 BILLION WORD GOOGLE NEWS CORPUS - EXPERIMENTAL DETAILS\nModel Architecture:\nThe models are similar in structure to the 8-million-operations-per-timestep\nmodels described in the previous section. We vary the number of experts between models, using\nan ordinary MoE layer with 32 experts and hierarchical MoE layers with 256, 1024, 4096, 16384,\n65536 and 131072 experts. For the hierarchical MoE layers, the \ufb01rst level branching factors are 32,\n32, 64, 128, 256 and 256, respectively.\nTraining:\nModels are trained on a cluster of 32 Tesla K40 GPUs, except for the last two models,\nwhich are trained on clusters of 64 and 128 GPUs so as to have enough memory for all the param-\neters. For all models, training batch sizes are approximately 2.5 million words. Models are trained\nonce-through over about 100 billion words.\nWe implement several memory optimizations in order to \ufb01t up to 1 billion parameters per GPU.\nFirst, we do not store the activations of the hidden layers of the experts, but instead recompute them\non the backwards pass. Secondly, we modify the optimizer on the expert parameters to require less\nauxiliary storage:\nThe Adam optimizer (Kingma & Ba, 2015) keeps \ufb01rst and second moment estimates of the per-\nparameter gradients. This triples the required memory. To avoid keeping a \ufb01rst-moment estimator,\nwe set \u03b21 = 0. To reduce the size of the second moment estimator, we replace it with a factored\napproximation. For a matrix of parameters, instead of maintaining a full matrix of second-moment\nestimators, we maintain vectors of row-wise and column-wise averages of that matrix. At each step,\nthe matrix of estimators is taken to be the outer product of those two vectors divided by the mean of\neither one. This technique could similarly be applied to Adagrad (Duchi et al., 2010).\nTable 8: Model comparison on 100 Billion Word Google News Dataset\nModel\nTest\nTest\nops/timestep #Params excluding\nTotal\nTFLOPS\nPerplexity Perplexity\n(millions)\nembed. & softmax\n#Params\nper GPU\n.1 epochs\n1 epoch\n(millions)\n(billions) (observed)\nKneser-Ney 5-gram\n67.1\n45.3\n0.00001\n76.0\n4xLSTM-512\n54.5\n47.0\n8.4\n8.4\n0.1\n1.23\nMoE-32\n48.5\n40.4\n8.4\n37.8\n0.1\n0.83\nMoE-256-h\n42.8\n35.3\n8.4\n272.9\n0.4\n1.11\nMoE-1024-h\n40.3\n32.7\n8.5\n1079.0\n1.2\n1.14\nMoE-4096-h\n38.9\n30.9\n8.6\n4303.4\n4.4\n1.07\nMoE-16384-h\n38.2\n29.7\n8.8\n17201.0\n17.3\n0.96\nMoE-65536-h\n38.2\n28.9\n9.2\n68791.0\n68.9\n0.72\nMoE-131072-h\n39.8\n29.2\n9.7\n137577.6\n137.7\n0.30",
        "page_start": 15,
        "page_end": 16,
        "chunk_ids": []
      },
      {
        "id": "section_13",
        "title": "Results",
        "content": "Results:\nWe evaluate our model using perplexity on a holdout dataset. Results are reported in\nTable 8. Perplexity after 100 billion training words is 39% lower for the 68-billion-parameter MoE\n16\n\n\nUnder review as a conference paper at ICLR 2017\nmodel than for the baseline model. It is notable that the measured computational ef\ufb01ciency of\nthe largest model (0.30 TFLOPS/GPU) is very low compared to the other models. This is likely\na result of the fact that, for purposes of comparison to the other models, we did not increase the\ntraining batch size proportionally to the number of GPUs. For comparison, we include results for\na computationally matched baseline model consisting of 4 LSTMs, and for an unpruned 5-gram\nmodel with Kneser-Ney smoothing (Kneser & Ney, 1995).4\nE\nMACHINE TRANSLATION - EXPERIMENTAL DETAILS\nModel Architecture for Single Language Pair MoE Models:\nOur model is a modi\ufb01ed version\nof the GNMT model described in (Wu et al., 2016). To reduce computation, we decrease the number\nof LSTM layers in the encoder and decoder from 9 and 8 to 3 and 2 respectively. We insert MoE\nlayers in both the encoder (between layers 2 and 3) and the decoder (between layers 1 and 2). We use\nan attention mechanism between the encoder and decoder, with the \ufb01rst decoder LSTM receiving\noutput from and providing input for the attention 5. All of the layers in our model have input and\noutput dimensionality of 512. Our LSTM layers have 2048 hidden units, with a 512-dimensional\noutput projection. We add residual connections around all LSTM and MoE layers to encourage\ngradient \ufb02ow (He et al., 2015). Similar to GNMT, to effectively deal with rare words, we used sub-\nword units (also known as \u201cwordpieces\") (Schuster & Nakajima, 2012) for inputs and outputs in our\nsystem.\nWe use a shared source and target vocabulary of 32K wordpieces. We also used the same beam\nsearch technique as proposed in (Wu et al., 2016).\nWe train models with different numbers of experts in the MoE layers. In addition to a baseline\nmodel with no MoE layers, we train models with \ufb02at MoE layers containing 32 experts, and models\nwith hierarchical MoE layers containing 512 and 2048 experts. The \ufb02at MoE layers use k = 4 and\nthe hierarchical MoE models use k = 2 at each level of the gating network. Thus, each input is\nprocessed by exactly 4 experts in each MoE layer. Each expert in the MoE layer is a feed forward\nnetwork with one hidden layer of size 2048 and ReLU activation. Thus, each expert contains [512 \u2217\n2048] + [2048 \u2217 512] = 2M parameters. The output of the MoE layer is passed through a sigmoid\nfunction. We use the strictly-balanced gating function described in Appendix F.\nModel Architecture for Multilingual MoE Model:\nWe used the same model architecture as\nfor the single-language-pair models, with the following exceptions: We used noisy-top-k gating as\ndescribed in Section 2.1, not the scheme from Appendix F. The MoE layers in the encoder and\ndecoder are non-hierarchical MoEs with n = 512 experts, and k = 2. Each expert has a larger\nhidden layer of size 8192. This doubles the amount of computation in the MoE layers, raising the\ncomputational budget of the entire model from 85M to 102M ops/timestep.\nTraining:\nWe trained our networks using the Adam optimizer (Kingma & Ba, 2015). The base\nlearning rate was increased linearly for the \ufb01rst 2000 training steps, held constant for an additional\n8000 steps, and decreased after that so as to be proportional to the inverse square root of the step\nnumber. For the single-language-pair models, similarly to (Wu et al., 2016), we applied dropout\n(Zaremba et al., 2014) to the output of all embedding, LSTM and MoE layers, using DropProb =\n0.4. Training was done synchronously on a cluster of up to 64 GPUs as described in section 3. Each\ntraining batch consisted of a set of sentence pairs containing roughly 16000 words per GPU.\nTo ensure balanced expert utilization we set wimportance = 0.01 and wload = 0.01, as described in\nSection 4 and Appendix A.\nMetrics:\nWe evaluated our models using the perplexity and the standard BLEU score metric. We\nreported tokenized BLEU score as computed by the multi-bleu.pl script, downloaded from the public\nimplementation of Moses (on Github), which was also used in (Luong et al., 2015a).\n4While the original size of the corpus was 130 billion words, the neural models were trained for a maximum\nof 100 billion words. The reported Kneser-Ney 5-gram models were trained over 13 billion and 130 billion\nwords respectively, giving them a slight advantage over the other reported results.\n5For performance reasons, we use a slightly different attention function from the one described in (Wu et al.,\n2016) - See Appendix G\n17\n\n\nUnder review as a conference paper at ICLR 2017",
        "page_start": 16,
        "page_end": 18,
        "chunk_ids": []
      },
      {
        "id": "section_14",
        "title": "Results",
        "content": "Results:\nTables 2, 3 and 4 in Section 5.3 show comparisons of our results to other published",
        "page_start": 18,
        "page_end": 18,
        "chunk_ids": []
      },
      {
        "id": "section_15",
        "title": "methods",
        "content": "methods. Figure 4 shows test perplexity as a function of number of words in the (training data\u2019s)\nsource sentences processed for models with different numbers of experts. As can be seen from the\nFigure, as we increased the number of experts to approach 2048, the test perplexity of our model\ncontinued to improve.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNumber of source words processed\n1e9\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\nPerplexity\n#Experts=0\n#Experts=32\n#Experts=512\n#Experts=2048\n0.0\n0.5\n1.0\n1.5\n2.0\nNumber of source words processed\n1e10\n2\n3\n4\n5\n6\n7\n8\nPerplexity\n#Experts=0\n#Experts=32\n#Experts=512\n#Experts=2048\nFigure 4: Perplexity on WMT\u201914 En\u2192 Fr (left) and Google Production En\u2192 Fr (right) datasets as\na function of number of words processed. The large differences between models at the beginning\nof training are due to different batch sizes. All models incur the same computational budget (85M\nops/timestep) except the one with no experts.\nWe found that the experts indeed become highly specialized by syntax and/or semantics, as can be\nseen in Table 9. For example, one expert is used when the inde\ufb01nite article \u201ca\" introduces the direct\nobject in a verb phrase indicating importance or leadership.\nTable 9: Contexts corresponding to a few of the 2048 experts in the MoE layer in the encoder portion\nof the WMT\u201914 En\u2192 Fr translation model. For each expert i, we sort the inputs in a training batch\nin decreasing order of G(x)i, and show the words surrounding the corresponding positions in the\ninput sentences.\nExpert 381\nExpert 752\nExpert 2004\n... with researchers , ...\n... plays a core ...\n... with rapidly growing ...\n... to innovation .\n... plays a critical ...\n... under static conditions ...\n... tics researchers .\n... provides a legislative ...\n... to swift ly ...\n... the generation of ...\n... play a leading ...\n... to dras tically ...\n... technology innovations is ...\n... assume a leadership ...\n... the rapid and ...\n... technological innovations , ...\n... plays a central ...\n... the fast est ...\n... support innovation throughout ...\n... taken a leading ...\n... the Quick Method ...\n... role innovation will ...\n... established a reconciliation ...\n... rec urrent ) ...\n... research scienti st ...\n... played a vital ...\n... provides quick access ...\n... promoting innovation where ...\n... have a central ...\n... of volatile organic ...\n...\n...\n...\nF\nSTRICTLY BALANCED GATING\nDue to some peculiarities in our infrastructure which have since been \ufb01xed, at the time we ran some\nof the machine translation experiments, our models ran faster if every expert received exactly the\nsame batch size. To accommodate this, we used a different gating function which we describe below.\nRecall that we de\ufb01ne the softmax gating function to be:\nG\u03c3(x) = Softmax(x \u00b7 Wg)\n(15)\nSparse Gating (alternate formulation):\nTo obtain a sparse gating vector, we multiply G\u03c3(x)\ncomponent-wise with a sparse mask M(G\u03c3(x)) and normalize the output. The mask itself is a\nfunction of G\u03c3(x) and speci\ufb01es which experts are assigned to each input example:\n18\n\n\nUnder review as a conference paper at ICLR 2017\nG(x)i =\nG\u03c3(x)iM(G\u03c3(x))i\n\ufffdn\nj=1 G\u03c3(x)jM(G\u03c3(x))j\n(16)\nTop-K Mask:\nTo implement top-k gating in this formulation, we would let M(v) = TopK(v, k),\nwhere:\nTopK(v, k)i =\n\ufffd1\nif vi is in the top k elements of v.\n0\notherwise.\n(17)\nBatchwise Mask:\nTo force each expert to receive the exact same number of examples, we intro-\nduce an alternative mask function, Mbatchwise(X, m), which operates over batches of input vectors.\nInstead of keeping the top k values per example, we keep the top m values per expert across the\ntraining batch, where m = k|X|\nn , so that each example is sent to an average of k experts.\nMbatchwise(X, m)j,i =\n\ufffd1\nif Xj,i is in the top m values for to expert i\n0\notherwise\n(18)\nAs our experiments suggest and also observed in (Ioffe & Szegedy, 2015), using a batchwise func-\ntion during training (such as Mbatchwise) requires modi\ufb01cations to the inference when we may not\nhave a large batch of examples. Our solution to this is to train a vector T of per-expert threshold\nvalues to approximate the effects of the batchwise mask. We use the following mask at inference\ntime:\nMthreshold(x, T)i =\n\ufffd1\nif xi > Ti\n0\notherwise\n(19)\nTo learn the threshold values, we apply an additional loss at training time which is minimized when\nthe batchwise mask and the threshold mask are identical.\nLbatchwise(X, T, m) =\n|X|\n\ufffd\nj=1\nn\n\ufffd\ni=1\n(Mthreshold(x, T)i \u2212 Mbatchwise(X, m)j,i)(Xj,i \u2212 Ti)\n(20)\nG\nATTENTION FUNCTION\nThe attention mechanism described in GNMT (Wu et al., 2016) involves a learned \u201cAttention Func-\ntion\" A(xi, yj) which takes a \u201csource vector\" xi and a \u201ctarget vector\" yj, and must be computed for\nevery source time step i and target time step j. In GNMT, the attention function is implemented as\na feed forward neural network with a hidden layer of size n. It can be expressed as:\nAGNMT (xi, yj) =\nn\n\ufffd\nd=1\nVdtanh((xiU)d + (yjW)d)\n(21)\nWhere U and W are trainable weight matrices and V is a trainable weight vector.\nFor performance reasons, in our models, we used a slightly different attention function:\nA(xi, yj) =\nn\n\ufffd\nd=1\nVdtanh((xiU)d)tanh((yjW)d)\n(22)\nWith our attention function, we can simultaneously compute the attention function on multiple\nsource time steps and multiple target time steps using optimized matrix multiplications. We found\nlittle difference in quality between the two functions.\n19",
        "page_start": 18,
        "page_end": 19,
        "chunk_ids": []
      }
    ],
    "total_pages": 19,
    "processed_at": "2025-11-17T19:41:29.106123",
    "created_at": "2025-11-17T19:41:29.106123"
  }
}